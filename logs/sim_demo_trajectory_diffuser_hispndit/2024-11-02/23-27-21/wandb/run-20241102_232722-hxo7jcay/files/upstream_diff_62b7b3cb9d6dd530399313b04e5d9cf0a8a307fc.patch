diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
new file mode 100644
index 0000000..f38c83a
--- /dev/null
+++ b/.pre-commit-config.yaml
@@ -0,0 +1,32 @@
+# See https://pre-commit.com for more information
+# See https://pre-commit.com/hooks.html for more hooks
+repos:
+-   repo: https://github.com/pre-commit/pre-commit-hooks
+    rev: v4.4.0
+    hooks:
+    -   id: trailing-whitespace
+        exclude: "tests/testdata/"
+    -   id: end-of-file-fixer
+        exclude: "tests/testdata/"
+    -   id: check-yaml
+    -   id: check-added-large-files
+        args: ['--maxkb=5000']
+-   repo: https://github.com/kynan/nbstripout
+    rev: 0.6.0
+    hooks:
+    -   id: nbstripout
+-   repo: https://github.com/myint/autoflake
+    rev: v2.1.1
+    hooks:
+    -   id: autoflake
+        args:
+        -   --in-place
+        -   --remove-all-unused-imports
+-   repo: https://github.com/timothycrosley/isort
+    rev: 5.12.0
+    hooks:
+        - id: isort
+-   repo: https://github.com/psf/black
+    rev: 23.3.0
+    hooks:
+    -   id: black
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
new file mode 100644
index 0000000..3c942fb
--- /dev/null
+++ b/CONTRIBUTING.md
@@ -0,0 +1,21 @@
+# Contributing
+
+## Install
+```
+pip install -e ".[develop]"
+
+pre-commit install
+```
+
+## Run CI locally
+To run the CI locally:
+
+Setup (make sure docker is installed):
+```
+brew install act
+```
+
+Run act
+```
+act -j develop
+```
diff --git a/README.md b/README.md
index d0e590e..1454427 100644
--- a/README.md
+++ b/README.md
@@ -1,4 +1,151 @@
 # FlowBotHD
-Code for FlowBotHD: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation
 
-Coming soon...
+[FlowBotHD](https://flowbothd.github.io/) is a history-aware diffusion handling ambiguities (multi-modality and occlusion) in articulated objects.
+
+![Alt text](imgs/teaser.jpg)
+
+
+## Installation
+
+```{bash}
+conda create -n flowbothd python=3.9
+
+conda activate flowbothd
+
+pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
+
+pip install -r requirements.txt
+```
+
+> Note: Make sure you install pytorch with regard to your machine's CUDA version. If you want to change the cuda version, you also need to change the cuda version specified in requirements.txt
+
+
+Then we have to also install the flowbothd package itself. [Make sure you are directly under the flowbothd directory, where you can see pyproject.toml]
+
+```{bash}
+pip install .
+```
+
+## Demo
+
+In our `demo.ipynb`, we have prediction and simulation demo given a pretrained model.
+
+TODO
+
+## Train
+
+
+### STEP 0 : Set the config files
+
+0) configs/_logging.yaml:
+
+    Set the `wandb.entity` and `wandb.project` for wandb logging.
+
+1) configs/train.yaml: 
+
+    Choose model
+    - `model`: diffuser_hispndit (FlowBotHD) / artflownet (FlowBot3D) / other structures we've tried whose names are listed in the configs/model/.
+
+2) configs/dataset/trajectory.yaml
+
+    `dataset_type`: full-dataset for training and evaluating on full partnet-mobility, doors-only for training and evaluating only on the most ambiguous category: doors.
+
+3) configs/training/trajectory_{model.name}.yaml
+
+    Change the corresponding detailed configs for the training process: `learning_rate`, `batch_size`, `warmup_steps`.
+
+4) configs/model/{model.name}.yaml
+
+    Change the detailed configs for the model (Only needed for diffusion)
+
+    - `num_train_timesteps`: diffusion timestep
+
+### STEP 1 : Run training script
+```
+python scripts/train.py
+```
+
+## Eval and Simulation
+
+If you want to evaluate (calculate the basic cosine, mag, rmse metrics across dataset) and evaluate the simulation (calculate the success rate, normalized distance) of a trained model:
+
+### STEP 0 : Set the config files and specify the ckpt
+
+1. Specify the checkpoint / run you are evaluating:
+
+    Set `checkpoint.run_id` and `wandb.group` with the run_id and the group name of the run to be evaluated.
+
+2. Set the same `model`, `dataset_type` etc as the training config files above.
+
+3. In the evaluation / simulation script you want to run, specify the ckpt_file = "PATH_FILE"
+
+### STEP 1 : Run evaluation / simulation
+
+For evaluating diffuser-based methods with history (that needs winner-take-all metric, meaning multiple samples and take the best metric):
+
+```{bash} 
+python scripts/eval_history_diffuser_wta.py  # For basic evaluation
+python scripts/eval_sim_diffuser_history.py  # For simulation
+python scripts/eval_sim_diffuser.py          # For simulation with model trained without history
+```
+
+For evaluating regression-based method (flowbot3d):
+```{bash}
+python scripts/eval.py                       # For basic evaluation
+python scripts/eval_sim.py                   # For simulation
+```
+
+
+## Run Specified Subset Experiment
+
+Need to change scripts/train.py for training and scripts/eval(_sim).py for evaluation:
+
+When creating dataset, specify the arguments `special_req` and `toy_dataset`.
+
+1) special_req: 
+
+- "half-half-01"(Part of data fully closed, part of data randomly opened, with 0 or 1-step history)
+- "fully-closed"(All of data fully closed)
+- "randomly-open"(All of the data randomly open)
+
+2) toy_dataset: a dict to specify a small dataset
+- id: the name for the toy dataset
+- train-train: the ids for the training set
+- train-test: the ids for the validation set
+- test: the ids for the test set
+
+An Example:
+```
+# Create FlowBot dataset
+datamodule = data_module_class[cfg.dataset.name](
+    root=cfg.dataset.data_dir,
+    batch_size=cfg.training.batch_size,
+    num_workers=cfg.resources.num_workers,
+    n_proc=cfg.resources.n_proc_per_worker,
+    seed=cfg.seed,
+    trajectory_len=trajectory_len, 
+    special_req="half-half-01"
+    toy_dataset = {
+        "id": "door-1",
+        "train-train": ["8994", "9035"],
+        "train-test": ["8994", "9035"],
+        "test": ["8867"],
+    }
+)
+```
+
+Then run train and eval exactly like before.
+
+## Cite
+
+If you find this codebase useful in your research, please consider citing:
+
+```
+@inproceedings{liflowbothd,
+  title={FlowBotHD: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation},
+  author={Li, Yishu and Leng, Wen Hui and Fang, Yiming and Eisner, Ben and Held, David},
+  booktitle={8th Annual Conference on Robot Learning}
+}
+```
+
+
diff --git a/configs/_logging.yaml b/configs/_logging.yaml
new file mode 100644
index 0000000..18c4bc2
--- /dev/null
+++ b/configs/_logging.yaml
@@ -0,0 +1,32 @@
+# Where logs go, i.e. the top folder.
+log_dir: ${hydra:runtime.cwd}/logs
+
+output_dir: ${hydra:runtime.output_dir}
+
+# This has to come from above.
+job_type: ???
+
+hydra:
+  run:
+    dir: ${log_dir}/${hydra.job.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
+  sweep:
+    dir: ${log_dir}/${hydra.job.name}/sweep/${now:%Y-%m-%d}/${now:%H-%M-%S}
+    subdir: ${hydra.job.num}
+  job:
+    chdir: True
+    name: ${job_type}
+
+lightning:
+  checkpoint_dir: ${output_dir}/checkpoints
+
+wandb:
+  entity: leisure-thu-cv # ???  # The wandb entity
+  project: flowbothd # ???  # The wandb project name
+
+  # Group is for grouping runs together (i.e. a train run and an eval run).
+  group: ???
+
+  # Where to dump wandb logs, etc.
+  save_dir: ${output_dir}
+  # Put artifacts at the toplevel so that we don't have to re-download each time...
+  artifact_dir: ${hydra:runtime.cwd}/wandb_artifacts
diff --git a/configs/dataset/trajectory.yaml b/configs/dataset/trajectory.yaml
new file mode 100644
index 0000000..06caea5
--- /dev/null
+++ b/configs/dataset/trajectory.yaml
@@ -0,0 +1,12 @@
+name: trajectory
+data_dir: ${oc.env:HOME}/datasets/partnet-mobility
+
+dataset_type: "doors-only"  # "full-dataset"
+special_req: "half-half-01"  #"fully-closed", "randomly-open" (no special request)
+lr: 1e-3
+mask_input_channel: True
+randomize_camera: True
+randomize_size: False
+augmentation: False   # Turn this on with doors-only
+seed: 42
+n_points: 1200
diff --git a/configs/eval.yaml b/configs/eval.yaml
new file mode 100644
index 0000000..d5f8292
--- /dev/null
+++ b/configs/eval.yaml
@@ -0,0 +1,42 @@
+mode: eval
+
+# This is somewhat arbitrary.
+job_type: ${mode}_${dataset.name}_${model.name} #_wta
+
+defaults:
+  # Each of these have their own configuration parameters.
+  - dataset: trajectory
+  - model: diffuser_hispndit
+
+  # A set of inference settings for the model. Note that these may be different
+  # from / or a subset of the training settings. This is that we don't have to
+  # provide, like, a learning rater or something to eval.
+  - inference: ${dataset}_${model}
+
+  # Simple shared imports.
+  - _logging
+
+  # Override.
+  - _self_
+
+seed: 42
+
+# This is the checkpoint that we're evaluating. You can change this to whatever you need,
+# like if you want multiple checkpoints simultaneously, etc.
+checkpoint:
+  # If we want to load a model for a specific run, we can change that here.
+  run_id: ???
+  reference: ${wandb.entity}/${wandb.project}/model-${checkpoint.run_id}:best
+
+resources:
+  num_workers: 30
+  n_proc_per_worker: 2
+  gpus:
+    - 0
+
+wandb:
+  # The group ***should*** be the same as the training group (so it can be bundled)
+  # nicely in the UI. But you might have a one-off eval or something.
+  group: ???
+
+metric_output_dir: './logs'
diff --git a/configs/eval_history.yaml b/configs/eval_history.yaml
new file mode 100644
index 0000000..aa150a1
--- /dev/null
+++ b/configs/eval_history.yaml
@@ -0,0 +1,43 @@
+mode: eval_history
+
+# This is somewhat arbitrary.
+job_type: ${mode}_${dataset.name}_${model.name} #_wta
+
+defaults:
+  # Each of these have their own configuration parameters.
+  - dataset: trajectory
+  - model: diffuser_hispndit
+
+  # A set of inference settings for the model. Note that these may be different
+  # from / or a subset of the training settings. This is that we don't have to
+  # provide, like, a learning rater or something to eval.
+  - inference: ${dataset}_${model}
+
+  # Simple shared imports.
+  - _logging
+
+  # Override.
+  - _self_
+
+seed: 42
+
+# This is the checkpoint that we're evaluating. You can change this to whatever you need,
+# like if you want multiple checkpoints simultaneously, etc.
+checkpoint:
+  # If we want to load a model for a specific run, we can change that here.
+  run_id: ???
+
+  reference: ${wandb.entity}/${wandb.project}/model-${checkpoint.run_id}:best
+
+resources:
+  num_workers: 30
+  n_proc_per_worker: 2
+  gpus:
+    - 0
+
+wandb:
+  # The group ***should*** be the same as the training group (so it can be bundled)
+  # nicely in the UI. But you might have a one-off eval or something.
+  group: ???
+
+metric_output_dir: './logs'
diff --git a/configs/eval_sim.yaml b/configs/eval_sim.yaml
new file mode 100644
index 0000000..c639281
--- /dev/null
+++ b/configs/eval_sim.yaml
@@ -0,0 +1,52 @@
+mode: sim_demo
+
+# This is somewhat arbitrary.
+job_type: ${mode}_${dataset.name}_${model.name}
+
+defaults:
+  # Each of these have their own configuration parameters.
+  - dataset: trajectory
+  - model: diffuser_hispndit
+
+  # A set of inference settings for the model. Note that these may be different
+  # from / or a subset of the training settings. This is that we don't have to
+  # provide, like, a learning rater or something to eval.
+  - inference: ${dataset}_${model}
+
+  # Simple shared imports.
+  - _logging
+
+  # Override.
+  - _self_
+
+seed: 42
+gui: False
+website: True
+website_port: 9001  # 9001, 9002, 9003
+
+sgp: False  # Use sgp?
+consistency_check: True # True
+history_filter: True # True
+
+
+# This is the checkpoint that we're evaluating. You can change this to whatever you need,
+# like if you want multiple checkpoints simultaneously, etc.
+checkpoint:
+  # If we want to load a model for a specific run, we can change that here.
+  run_id: hn61knsq #???
+
+  reference: ${wandb.entity}/${wandb.project}/model-${checkpoint.run_id}:best
+
+resources:
+  num_workers: 30
+  n_proc_per_worker: 2
+  gpus:
+    - 0
+
+wandb:
+  # The group ***should*** be the same as the training group (so it can be bundled)
+  # nicely in the UI. But you might have a one-off eval or something.
+  group: flowbothd # ???
+
+
+metric_output_dir: './logs'
diff --git a/configs/inference/flowbot_pn++.yaml b/configs/inference/flowbot_pn++.yaml
new file mode 100644
index 0000000..01fff46
--- /dev/null
+++ b/configs/inference/flowbot_pn++.yaml
@@ -0,0 +1,3 @@
+name: flowbot_pn++
+batch_size: 64
+mask_input_channel: False
diff --git a/configs/inference/trajectory_diffuser_dgdit.yaml b/configs/inference/trajectory_diffuser_dgdit.yaml
new file mode 100644
index 0000000..d1216f0
--- /dev/null
+++ b/configs/inference/trajectory_diffuser_dgdit.yaml
@@ -0,0 +1,3 @@
+name: trajectory_diffuser_dgdit
+batch_size: 64
+trajectory_len: 1
diff --git a/configs/inference/trajectory_diffuser_dit.yaml b/configs/inference/trajectory_diffuser_dit.yaml
new file mode 100644
index 0000000..8dfb175
--- /dev/null
+++ b/configs/inference/trajectory_diffuser_dit.yaml
@@ -0,0 +1,3 @@
+name: trajectory_diffuser_dit
+batch_size: 64
+trajectory_len: 1
diff --git a/configs/inference/trajectory_diffuser_hisdit.yaml b/configs/inference/trajectory_diffuser_hisdit.yaml
new file mode 100644
index 0000000..594cbed
--- /dev/null
+++ b/configs/inference/trajectory_diffuser_hisdit.yaml
@@ -0,0 +1,3 @@
+name: trajectory_diffuser_hisdit
+batch_size: 64
+trajectory_len: 1
diff --git a/configs/inference/trajectory_diffuser_hispndit.yaml b/configs/inference/trajectory_diffuser_hispndit.yaml
new file mode 100644
index 0000000..228dd13
--- /dev/null
+++ b/configs/inference/trajectory_diffuser_hispndit.yaml
@@ -0,0 +1,3 @@
+name: trajectory_diffuser_hispndit
+batch_size: 64
+trajectory_len: 1
diff --git a/configs/inference/trajectory_diffuser_pn++.yaml b/configs/inference/trajectory_diffuser_pn++.yaml
new file mode 100644
index 0000000..8421c0b
--- /dev/null
+++ b/configs/inference/trajectory_diffuser_pn++.yaml
@@ -0,0 +1,3 @@
+name: trajectory_diffuser_pn++
+batch_size: 64
+trajectory_len: 1
diff --git a/configs/inference/trajectory_diffuser_pndit.yaml b/configs/inference/trajectory_diffuser_pndit.yaml
new file mode 100644
index 0000000..b68e6a9
--- /dev/null
+++ b/configs/inference/trajectory_diffuser_pndit.yaml
@@ -0,0 +1,3 @@
+name: trajectory_diffuser_pndit
+batch_size: 64
+trajectory_len: 1
diff --git a/configs/inference/trajectory_pn++.yaml b/configs/inference/trajectory_pn++.yaml
new file mode 100644
index 0000000..bace490
--- /dev/null
+++ b/configs/inference/trajectory_pn++.yaml
@@ -0,0 +1,5 @@
+name: trajectory_pn++
+batch_size: 64
+trajectory_len: 1
+# mask_input_channel: True
+mask_input_channel: False
diff --git a/configs/model/diffuser_dgdit.yaml b/configs/model/diffuser_dgdit.yaml
new file mode 100644
index 0000000..57cf56c
--- /dev/null
+++ b/configs/model/diffuser_dgdit.yaml
@@ -0,0 +1,8 @@
+# Diffuser params
+name: diffuser_dgdit
+time_proj_dim: 64
+time_embed_dim: 64
+freq_shift: 0
+flip_sin_to_cos: True
+num_train_timesteps: 100
+num_inference_timesteps: 100
diff --git a/configs/model/diffuser_dit.yaml b/configs/model/diffuser_dit.yaml
new file mode 100644
index 0000000..e3d3016
--- /dev/null
+++ b/configs/model/diffuser_dit.yaml
@@ -0,0 +1,8 @@
+# Diffuser params
+name: diffuser_dit
+time_proj_dim: 64
+time_embed_dim: 64
+freq_shift: 0
+flip_sin_to_cos: True
+num_train_timesteps: 100
+num_inference_timesteps: 100
diff --git a/configs/model/diffuser_hisdit.yaml b/configs/model/diffuser_hisdit.yaml
new file mode 100644
index 0000000..de76189
--- /dev/null
+++ b/configs/model/diffuser_hisdit.yaml
@@ -0,0 +1,12 @@
+# Diffuser params
+name: diffuser_hisdit
+history_model: encoder # encoder  # translator
+time_proj_dim: 64
+time_embed_dim: 64
+freq_shift: 0
+flip_sin_to_cos: True
+num_train_timesteps: 100
+num_inference_timesteps: 100
+history_len: 1
+history_dim: 128
+batch_norm: False
diff --git a/configs/model/diffuser_hispndit.yaml b/configs/model/diffuser_hispndit.yaml
new file mode 100644
index 0000000..e9b2c2d
--- /dev/null
+++ b/configs/model/diffuser_hispndit.yaml
@@ -0,0 +1,12 @@
+# Diffuser params
+name: diffuser_hispndit
+history_model: encoder # encoder  # translator
+time_proj_dim: 64
+time_embed_dim: 64
+freq_shift: 0
+flip_sin_to_cos: True
+num_train_timesteps: 100
+num_inference_timesteps: 100
+history_len: 1
+history_dim: 128
+batch_norm: True
diff --git a/configs/model/diffuser_pn++.yaml b/configs/model/diffuser_pn++.yaml
new file mode 100644
index 0000000..2bf8a1c
--- /dev/null
+++ b/configs/model/diffuser_pn++.yaml
@@ -0,0 +1,8 @@
+# Diffuser params
+name: diffuser_pn++
+time_proj_dim: 64
+time_embed_dim: 64
+freq_shift: 0
+flip_sin_to_cos: True
+num_train_timesteps: 100
+num_inference_timesteps: 100
diff --git a/configs/model/diffuser_pndit.yaml b/configs/model/diffuser_pndit.yaml
new file mode 100644
index 0000000..c1b13ac
--- /dev/null
+++ b/configs/model/diffuser_pndit.yaml
@@ -0,0 +1,8 @@
+# Diffuser params
+name: diffuser_pndit
+time_proj_dim: 64
+time_embed_dim: 64
+freq_shift: 0
+flip_sin_to_cos: True
+num_train_timesteps: 100
+num_inference_timesteps: 100
diff --git a/configs/model/pn++.yaml b/configs/model/pn++.yaml
new file mode 100644
index 0000000..5fb9fe5
--- /dev/null
+++ b/configs/model/pn++.yaml
@@ -0,0 +1 @@
+name: pn++
diff --git a/configs/train.yaml b/configs/train.yaml
new file mode 100644
index 0000000..6244c92
--- /dev/null
+++ b/configs/train.yaml
@@ -0,0 +1,30 @@
+mode: train
+
+# This is somewhat arbitrary.
+job_type: ${mode}_${dataset.name}_${model.name}
+
+defaults:
+  # Each of these have their own configuration parameters.
+  - dataset: trajectory
+  - model: diffuser_hispndit   # Or choose any other model that exists in the configs/model directory
+
+  # We assume a different training config for each dataset/model pair.
+  - training: ${dataset}_${model}
+
+  # Simple shared imports.
+  - _logging
+
+  # Override.
+  - _self_
+
+seed: 42
+
+resources:
+  num_workers: 30
+  n_proc_per_worker: 2
+  gpus:
+    - 0
+
+wandb:
+  # Assume no group provided, we will create a default one.
+  group: Null
diff --git a/configs/training/flowbot_pn++.yaml b/configs/training/flowbot_pn++.yaml
new file mode 100644
index 0000000..16adc05
--- /dev/null
+++ b/configs/training/flowbot_pn++.yaml
@@ -0,0 +1,8 @@
+name: flowbot_pn++
+lr: 1e-3
+batch_size: 64
+epochs: 100
+check_val_every_n_epoch: 1
+trajectory_len: 1
+mode: "delta"
+mask_input_channel: False
diff --git a/configs/training/trajectory_diffuser_dgdit.yaml b/configs/training/trajectory_diffuser_dgdit.yaml
new file mode 100644
index 0000000..81a51f3
--- /dev/null
+++ b/configs/training/trajectory_diffuser_dgdit.yaml
@@ -0,0 +1,16 @@
+name: trajectory_diffuser_dgdit
+lr: 1e-4
+lr_warmup_steps: 100
+batch_size: 32
+epochs: 1000
+train_sample_number : None  # To be filled in train.py
+check_val_every_n_epoch: 10
+trajectory_len: 1
+mode: delta
+wta: True
+wta_trial_times: 20
+
+# lr_warmup_steps: 5
+# batch_size: 1
+# epochs: 5000  # For toy
+# train_sample_number : 2  # For toy
diff --git a/configs/training/trajectory_diffuser_dit.yaml b/configs/training/trajectory_diffuser_dit.yaml
new file mode 100644
index 0000000..e8f979b
--- /dev/null
+++ b/configs/training/trajectory_diffuser_dit.yaml
@@ -0,0 +1,17 @@
+name: trajectory_diffuser_dit
+lr: 1e-4
+lr_warmup_steps: 100
+# batch_size: 128
+batch_size: 32
+epochs: 1000
+train_sample_number : None # To be filled in train.py
+check_val_every_n_epoch: 10
+trajectory_len: 1
+mode: delta
+wta: True
+wta_trial_times: 20
+
+# lr_warmup_steps: 5
+# batch_size: 1
+# epochs: 5000  # For toy
+# train_sample_number : 2  # For toy
diff --git a/configs/training/trajectory_diffuser_hisdit.yaml b/configs/training/trajectory_diffuser_hisdit.yaml
new file mode 100644
index 0000000..e3b3259
--- /dev/null
+++ b/configs/training/trajectory_diffuser_hisdit.yaml
@@ -0,0 +1,17 @@
+name: trajectory_diffuser_hisdit
+lr: 1e-4
+lr_warmup_steps: 100
+batch_size: 128
+epochs: 1000
+train_sample_number : None # To be filled in train.py
+check_val_every_n_epoch: 10
+trajectory_len: 1
+# history_len: 1
+mode: delta
+wta: True
+wta_trial_times: 20
+
+# lr_warmup_steps: 5
+# batch_size: 1
+# epochs: 5000  # For toy
+# train_sample_number : 2  # For toy
diff --git a/configs/training/trajectory_diffuser_hispndit.yaml b/configs/training/trajectory_diffuser_hispndit.yaml
new file mode 100644
index 0000000..b94d313
--- /dev/null
+++ b/configs/training/trajectory_diffuser_hispndit.yaml
@@ -0,0 +1,11 @@
+name: trajectory_diffuser_hispndit
+lr: 1e-4
+lr_warmup_steps: 100
+batch_size: 128
+epochs: 1000
+train_sample_number : None # To be filled in train.py
+check_val_every_n_epoch: 5 # 20
+trajectory_len: 1
+mode: delta
+wta: True
+wta_trial_times: 20
diff --git a/configs/training/trajectory_diffuser_pn++.yaml b/configs/training/trajectory_diffuser_pn++.yaml
new file mode 100644
index 0000000..2afca8a
--- /dev/null
+++ b/configs/training/trajectory_diffuser_pn++.yaml
@@ -0,0 +1,16 @@
+name: trajectory_diffuser_pn++
+lr: 1e-4
+lr_warmup_steps: 100
+batch_size: 16
+epochs: 1000
+train_sample_number : None  # To be filled in train.py
+check_val_every_n_epoch: 5
+trajectory_len: 1
+mode: delta
+wta: True
+wta_trial_times: 20
+
+# lr_warmup_steps: 5
+# batch_size: 1
+# epochs: 5000  # For toy
+# train_sample_number : 2  # For toy
diff --git a/configs/training/trajectory_diffuser_pndit.yaml b/configs/training/trajectory_diffuser_pndit.yaml
new file mode 100644
index 0000000..9fe7de1
--- /dev/null
+++ b/configs/training/trajectory_diffuser_pndit.yaml
@@ -0,0 +1,16 @@
+name: trajectory_diffuser_pndit
+lr: 1e-4
+lr_warmup_steps: 100
+batch_size: 32
+epochs: 1000
+train_sample_number : None  # To be filled in train.py
+check_val_every_n_epoch: 10
+trajectory_len: 1
+mode: delta
+wta: True
+wta_trial_times: 20
+
+# lr_warmup_steps: 5
+# batch_size: 1
+# epochs: 5000  # For toy
+# train_sample_number : 2  # For toy
diff --git a/configs/training/trajectory_pn++.yaml b/configs/training/trajectory_pn++.yaml
new file mode 100644
index 0000000..d578202
--- /dev/null
+++ b/configs/training/trajectory_pn++.yaml
@@ -0,0 +1,20 @@
+name: trajectory_pn++
+lr: 1e-3
+batch_size: 64
+epochs: 100
+check_val_every_n_epoch: 1
+trajectory_len: 1
+mode: "delta"
+# mask_input_channel: True
+mask_input_channel: False
+wta: False
+wta_trial_times: 20
+
+# batch_size: 32
+
+
+# Toy dataset
+# batch_size: 1
+# mask_input_channel: False
+# trajectory_len: 1
+# epochs: 500
diff --git a/demo.ipynb b/demo.ipynb
new file mode 100644
index 0000000..fe976cb
--- /dev/null
+++ b/demo.ipynb
@@ -0,0 +1,848 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Load the model"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import rpad.pyg.nets.pointnet2 as pnp_orig\n",
+    "from flowbothd.models.modules.dit_models import DGDiT, DiT, PN2DiT, PN2HisDiT\n",
+    "from flowbothd.models.modules.history_encoder import HistoryEncoder\n",
+    "from flowbothd.models.flow_trajectory_predictor import FlowTrajectoryInferenceModule\n",
+    "from flowbothd.models.flow_diffuser_dit import FlowTrajectoryDiffuserInferenceModule_DiT\n",
+    "from flowbothd.models.flow_diffuser_dgdit import FlowTrajectoryDiffuserInferenceModule_DGDiT\n",
+    "from flowbothd.models.flow_diffuser_hispndit import FlowTrajectoryDiffuserInferenceModule_HisPNDiT\n",
+    "# from flowbothd.models.flow_diffuser_pndit import FlowTrajectoryDiffuserInferenceModule_PNDiT\n",
+    "inference_module_class = {\n",
+    "    \"dit\": FlowTrajectoryDiffuserInferenceModule_DiT,\n",
+    "    \"dgdit\": FlowTrajectoryDiffuserInferenceModule_DGDiT,\n",
+    "    \"hispndit\": FlowTrajectoryDiffuserInferenceModule_HisPNDiT,\n",
+    "    \"flowbot\": FlowTrajectoryInferenceModule\n",
+    "}\n",
+    "networks = {\n",
+    "    \"flowbot\": pnp_orig.PN2Dense(\n",
+    "        in_channels=0,\n",
+    "        out_channels=3,\n",
+    "        p=pnp_orig.PN2DenseParams(),\n",
+    "    ),\n",
+    "    \"dit\": DiT(in_channels=6, depth=5, hidden_size=128, num_heads=4, learn_sigma=True).cuda(),\n",
+    "    \"dgdit\": DGDiT(in_channels=3, depth=5, hidden_size=128, patch_size=1, num_heads=4, n_points=1200).cuda(),\n",
+    "    \"hispndit\": {\n",
+    "        \"DiT\": PN2HisDiT(\n",
+    "            history_embed_dim=128,\n",
+    "            in_channels=3,\n",
+    "            depth=5,\n",
+    "            hidden_size=128,\n",
+    "            num_heads=4,\n",
+    "            learn_sigma=True,\n",
+    "        ).cuda(),\n",
+    "        \"History\": HistoryEncoder(\n",
+    "            history_dim=128,\n",
+    "            history_len=1,\n",
+    "            batch_norm=True,\n",
+    "            transformer=False,\n",
+    "            repeat_dim=False,\n",
+    "        ).cuda(),\n",
+    "    }\n",
+    "    # \"pndit\": PN2DiT(in_channels=3, depth=5, hidden_size=128, patch_size=1, num_heads=4, n_points=1200),  \n",
+    "}"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "class InferenceConfig:\n",
+    "    def __init__(self):\n",
+    "        self.batch_size = 1\n",
+    "        self.trajectory_len = 1\n",
+    "        self.mask_input_channel = False\n",
+    "\n",
+    "inference_config = InferenceConfig()\n",
+    "\n",
+    "class ModelConfig:\n",
+    "    def __init__(self):\n",
+    "        self.num_train_timesteps = 100\n",
+    "\n",
+    "model_config = ModelConfig()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import os\n",
+    "ckpt_dir = './pretrained'\n",
+    "train_type = 'fullset_half_half'   # door_half_half, fullset_half_half - what dataset the model is trained on \n",
+    "model_type = 'hispndit'   # dit, dgdit, hispndit - model structure\n",
+    "ckpt_path = os.path.join(ckpt_dir, f'{train_type}_{model_type}.ckpt')#_aug500.ckpt')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model = inference_module_class[model_type](\n",
+    "    networks[model_type], inference_cfg=inference_config, model_cfg=model_config\n",
+    ")\n",
+    "model.load_from_ckpt(ckpt_path)\n",
+    "model.eval()\n",
+    "model.cuda()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Make a prediction"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Read the point cloud"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import numpy as np\n",
+    "pcd_dir = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu'\n",
+    "pcd_paths = [os.path.join(pcd_dir, pcd_name) for pcd_name in os.listdir(pcd_dir)]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "id = 2\n",
+    "# path = pcd_paths[id]\n",
+    "# path = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu/incorrect_toilet.npy'\n",
+    "# path = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu/fridge_L_open_45.npy'\n",
+    "\n",
+    "# Multi-modal door\n",
+    "# path = '/home/yishu/flowbot_panda/failure_case_with_additional_crop_world.npy'\n",
+    "# path = '/home/yishu/flowbot_panda/door1.npy'\n",
+    "# path = '/home/yishu/flowbot_panda/newDoor.npy'\n",
+    "# path = '/home/yishu/flowbothd/real_world_pcds/slightly_open_xyz.npy'\n",
+    "# path = '/home/yishu/flowbothd/real_world_pcds/slightly_open_xyz.npy'\n",
+    "path = '/home/yishu/flowbothd/real_world_pcds/oven_without_top.npy'\n",
+    "pcd = np.load(path)\n",
+    "print(path)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Rotate the point cloud"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import numpy as np\n",
+    "\n",
+    "def rotate_pcd(pcd):\n",
+    "    rot = np.array([[0, -1, 0], [1,  0, 0], [0,  0, 1]])\n",
+    "    mean_x = pcd[:, 0].mean()\n",
+    "    mean_y = pcd[:, 1].mean()\n",
+    "    rot_pcd = pcd.copy()\n",
+    "    rot_pcd[:, 0] -= mean_x\n",
+    "    rot_pcd[:, 1] -= mean_y\n",
+    "    # rot_pcd[:, 2] += 1\n",
+    "    rot_pcd = rot_pcd@rot.T\n",
+    "    return rot_pcd\n",
+    "\n",
+    "pcd = pcd * 3\n",
+    "rot_pcd = rotate_pcd(pcd)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Sample it to 1200 points"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Could use pytorch3d for this but it has some cuda conflict with my current env and I don't wnana change lol\n",
+    "import numpy as np\n",
+    "\n",
+    "def farthest_point_sampling(world_points, points, k):\n",
+    "    num_points = points.shape[0]\n",
+    "    chosen_indices = np.zeros(k, dtype=int)\n",
+    "    chosen_indices[0] = np.random.randint(num_points)\n",
+    "    distances = np.full(num_points, np.inf)\n",
+    "    \n",
+    "    for i in range(1, k):\n",
+    "        dist = np.linalg.norm(points - points[chosen_indices[i-1]], axis=1)\n",
+    "        distances = np.minimum(distances, dist)\n",
+    "        chosen_indices[i] = np.argmax(distances)\n",
+    "        \n",
+    "    return points[chosen_indices], world_points[chosen_indices]\n",
+    "\n",
+    "# Example usage\n",
+    "# sampled_points = farthest_point_sampling(pcd, 1200)\n",
+    "sampled_points, sampled_points_world = farthest_point_sampling(pcd, rot_pcd, 1200)\n",
+    "\n",
+    "print(sampled_points.shape)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "- If don't use history"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Random sampling\n",
+    "# import torch\n",
+    "sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
+    "pred_flow = model.predict(sampled_points)[:, 0, :]"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "- If use history"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "history_pcd = np.zeros_like(sampled_points)\n",
+    "history_flow = np.zeros_like(sampled_points)\n",
+    "pred_flow = model.predict(sampled_points, history_pcd=history_flow, history_flow=history_pcd)[:, 0, :]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# import tqdm\n",
+    "# from flowbothd.metrics.trajectory import flow_metrics\n",
+    "# gt_flow = torch.zeros(1200, 3)\n",
+    "# gt_flow[:, 2] = 1 \n",
+    "# cosines = []\n",
+    "# for i in tqdm.tqdm(range(20)):\n",
+    "#     sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
+    "#     pred_flow = model.predict(sampled_points)[:, 0, :]\n",
+    "#     rmse, cos_dist, mag_error = flow_metrics(\n",
+    "#         pred_flow, gt_flow, reduce=True\n",
+    "#     )\n",
+    "#     cosines.append(cos_dist)\n",
+    "\n",
+    "# print(max(cosines))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Visualize the prediction"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Get the history"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "prev_path = '/home/yishu/flowbothd/real_world_pcds/xyz.npy'\n",
+    "history_pcd = np.load(prev_path)\n",
+    "# prev_rot_pcd = rotate_pcd(prev_pcd)\n",
+    "\n",
+    "prev_flow_path = '/home/yishu/flowbothd/real_world_pcds/flow.npy'\n",
+    "history_flow = np.load(prev_flow_path)\n",
+    "\n",
+    "import torch\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "## Point cloud only\n",
+    "animation = FlowNetAnimation()\n",
+    "\n",
+    "animation.add_trace(\n",
+    "    torch.as_tensor(history_pcd),\n",
+    "    # torch.as_tensor([pcd[mask]]),\n",
+    "    # torch.as_tensor([flow[mask]]),\n",
+    "    torch.as_tensor([history_pcd]),\n",
+    "    torch.as_tensor([history_flow]),\n",
+    "    # torch.as_tensor([pred_flow.cpu().numpy()]* 10),\n",
+    "    \"red\",\n",
+    ")\n",
+    "\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "import numpy as np\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "animation = FlowNetAnimation()\n",
+    "\n",
+    "# sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
+    "gt_action = torch.tensor([-1, 0, 0])\n",
+    "cosines = []\n",
+    "predictions = []\n",
+    "for i in range(50):\n",
+    "    # sampled_points = pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
+    "    sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
+    "    # No history\n",
+    "    pred_flow = model.predict(sampled_points)[:, 0, :]\n",
+    "    ## History\n",
+    "    # pred_flow = model.predict(sampled_points, history_pcd=history_pcd, history_flow=history_flow)[:, 0, :]\n",
+    "    idx = torch.topk(pred_flow, k=1)[1][0]\n",
+    "    action = pred_flow[idx]\n",
+    "    cosine = torch.cosine_similarity(action, gt_action)\n",
+    "    cosines.append(cosine)\n",
+    "    predictions.append(pred_flow)\n",
+    "\n",
+    "    animation.add_trace(\n",
+    "        torch.as_tensor(sampled_points),\n",
+    "        # torch.as_tensor([pcd[mask]]),\n",
+    "        # torch.as_tensor([flow[mask]]),\n",
+    "        torch.as_tensor([sampled_points]),\n",
+    "        torch.as_tensor([pred_flow.cpu().numpy()]),\n",
+    "        # torch.as_tensor([pred_flow.cpu().numpy()]* 10),\n",
+    "        \"red\",\n",
+    "    )\n",
+    "\n",
+    "\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "count = 0\n",
+    "for i in range(len(cosines)):\n",
+    "    if cosines[i] < 0:\n",
+    "        count += 1\n",
+    "\n",
+    "print(count)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "cosines"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "## Point cloud only\n",
+    "animation = FlowNetAnimation()\n",
+    "\n",
+    "# sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
+    "gt_action = torch.tensor([-1, 0, 0])\n",
+    "cosines = []\n",
+    "predictions = []\n",
+    "animation.add_trace(\n",
+    "    torch.as_tensor(sampled_points),\n",
+    "    # torch.as_tensor([pcd[mask]]),\n",
+    "    # torch.as_tensor([flow[mask]]),\n",
+    "    torch.as_tensor([sampled_points]),\n",
+    "    torch.as_tensor([np.zeros((1200, 3)) * 2]),\n",
+    "    # torch.as_tensor([pred_flow.cpu().numpy()]* 10),\n",
+    "    \"red\",\n",
+    ")\n",
+    "\n",
+    "\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### The goal point and orientation"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from scipy.spatial.transform import Rotation \n",
+    "def get_contact_point_and_flow_vector(flow, xyz):\n",
+    "    magnitude = torch.norm(flow, dim=1)\n",
+    "    idx_of_max_flow = torch.argmax(magnitude.unsqueeze(1))\n",
+    "    contact_point = torch.from_numpy(xyz[idx_of_max_flow])\n",
+    "    flow_vector = flow[idx_of_max_flow] \n",
+    "    flow_vector_normalized = (flow_vector / flow_vector.norm(dim=-1)).float()\n",
+    "    return contact_point, flow_vector_normalized\n",
+    "\n",
+    "\n",
+    "def get_goal_point_and_orientation(contact_point, flow_vector):\n",
+    "    goal_point = contact_point + 0.2 * flow_vector\n",
+    "    e_z_init = torch.tensor([0, 0, 1.0]).float().cuda()\n",
+    "    e_y = -flow_vector\n",
+    "    e_x = torch.linalg.cross(e_y, e_z_init)\n",
+    "    e_x = e_x / e_x.norm(dim=-1)\n",
+    "    e_z = torch.linalg.cross(e_x, e_y)\n",
+    "    R_goal = torch.stack([e_x, e_y, e_z], dim=1).cuda()\n",
+    "    R_gripper = torch.as_tensor(\n",
+    "        [\n",
+    "            [1, 0, 0],\n",
+    "            [0, 0, 1.0],\n",
+    "            [0, -1.0, 0],\n",
+    "        ]\n",
+    "    ).cuda()\n",
+    "\n",
+    "    goal_orientation_mat = (R_goal @ R_gripper).cpu()\n",
+    "    goal_orientation_quat = Rotation.from_matrix((R_goal @ R_gripper).cpu()).as_quat()\n",
+    "    return goal_point, goal_orientation_quat, goal_orientation_mat\n",
+    "\n",
+    "\n",
+    "def transform_flow_contact_point_goal_point_and_orientation_to_world(contact_point, goal_point, goal_orientation, mean_x, mean_y, flow_vector):\n",
+    "    # Formatting goal_point and goal_orientation to be in the same frame as the point cloud so that it can be visualized\n",
+    "    R = torch.tensor([[0, 1, 0], [-1, 0, 0], [0, 0, 1]]).float().cuda()\n",
+    "\n",
+    "    goal_point = goal_point @ R.T\n",
+    "    contact_point = contact_point @ R.T\n",
+    "\n",
+    "    # Add the mean in x and y\n",
+    "    goal_point[0] += mean_x\n",
+    "    goal_point[1] += mean_y\n",
+    "    contact_point[0] += mean_x\n",
+    "    contact_point[1] += mean_y\n",
+    "    \n",
+    "    goal_point = goal_point.cpu().numpy()\n",
+    "    goal_point = np.reshape(goal_point, (1, 3))\n",
+    "    contact_point = contact_point.cpu().numpy()\n",
+    "    contact_point = np.reshape(contact_point, (1, 3))\n",
+    "\n",
+    "    goal_orientation = Rotation.from_quat(goal_orientation)\n",
+    "    goal_orientation = torch.from_numpy(goal_orientation.as_matrix()).float().cuda()\n",
+    "    goal_orientation = R @ goal_orientation\n",
+    "    goal_orientation = Rotation.from_matrix(goal_orientation.cpu()).as_quat()\n",
+    "\n",
+    "    flow_vector = flow_vector @ R.T\n",
+    "\n",
+    "    return contact_point, goal_point, goal_orientation, flow_vector"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "import numpy as np\n",
+    "contact_point, flow_vector_normalized = get_contact_point_and_flow_vector(pred_flow, sampled_points)\n",
+    "contact_point = contact_point.float().cuda()\n",
+    "flow_vector_normalized = flow_vector_normalized.float().cuda()\n",
+    "goal_point, goal_orientation_quat, goal_orientation_mat = get_goal_point_and_orientation(contact_point, flow_vector_normalized)\n",
+    "# contact_point_world, goal_point_world, goal_orientation_world,flow_vector = transform_flow_contact_point_goal_point_and_orientation_to_world(contact_point, goal_point, goal_orientation_quat, mean_x, mean_y, flow_vector_normalized)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "flow_vector_normalized"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "print(contact_point)\n",
+    "print(goal_point)\n",
+    "print(goal_orientation_quat)\n",
+    "print(goal_orientation_mat)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# import copy\n",
+    "# import open3d as o3d\n",
+    "# goal_orientation_frame = (o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=goal_point[0]))\n",
+    "# goal_orientation_frame_r = copy.deepcopy(goal_orientation_frame)\n",
+    "# rot = goal_orientation_frame.get_rotation_matrix_from_quaternion(goal_orientation_quat)\n",
+    "# goal_orientation_frame_r.rotate(rot, center=goal_point[0])\n",
+    "# mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.6, origin=[0, 0, 0])\n",
+    "\n",
+    "# pc = o3d.geometry.PointCloud()\n",
+    "# pc.points = o3d.utility.Vector3dVector(sampled_points)  # Random points\n",
+    "\n",
+    "# # Create a visualizer object and add the point cloud\n",
+    "# vis = o3d.visualization.Visualizer()\n",
+    "# vis.create_window()\n",
+    "# vis.add_geometry(goal_orientation_frame_r)\n",
+    "# vis.add_geometry(pc)\n",
+    "# vis.run()\n",
+    "# vis.destroy_window()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "#### Flowbot frame"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "import numpy as np\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "\n",
+    "magnitude = torch.norm(pred_flow, dim=1)\n",
+    "idx_of_max_flow = torch.argmax(magnitude)\n",
+    "# print(idx_of_max_flow, pred_flow[idx_of_max_flow])\n",
+    "masked_pred_flow = pred_flow.clone() * 0.1\n",
+    "masked_pred_flow[idx_of_max_flow, :] *= 20\n",
+    "print(idx_of_max_flow, masked_pred_flow[idx_of_max_flow], goal_orientation_mat)\n",
+    "\n",
+    "animation = FlowNetAnimation()\n",
+    "animation.add_trace(\n",
+    "    torch.as_tensor(sampled_points),\n",
+    "    # torch.as_tensor([pcd[mask]]),\n",
+    "    # torch.as_tensor([flow[mask]]),\n",
+    "    torch.as_tensor([sampled_points]),\n",
+    "    torch.as_tensor([masked_pred_flow.numpy()]),\n",
+    "    \"red\",\n",
+    ")\n",
+    "animation.add_trace(\n",
+    "    torch.as_tensor(goal_point.unsqueeze(0).repeat(3, 1).cpu().numpy()),\n",
+    "    # torch.as_tensor([pcd[mask]]),\n",
+    "    # torch.as_tensor([flow[mask]]),\n",
+    "    torch.as_tensor([goal_point.unsqueeze(0).repeat(3, 1).cpu().numpy()]),\n",
+    "    torch.as_tensor([goal_orientation_mat.T.cpu().numpy()]),\n",
+    "    \"yellow\",\n",
+    ")\n",
+    "\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "print(contact_point)\n",
+    "print(goal_point)\n",
+    "print(goal_orientation_mat)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "#### World frame"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "contact_point_world, goal_point_world, goal_orientation_world, flow_vector = transform_flow_contact_point_goal_point_and_orientation_to_world(contact_point, goal_point, goal_orientation_quat, mean_x, mean_y, flow_vector_normalized)\n",
+    "goal_orientation_mat_world = Rotation.from_quat(goal_orientation_world).as_matrix()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "print(contact_point_world)\n",
+    "print(goal_point_world)\n",
+    "print(goal_orientation_mat_world)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "import numpy as np\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "\n",
+    "# magnitude = torch.norm(pred_flow, dim=1)\n",
+    "# idx_of_max_flow = torch.argmax(magnitude)\n",
+    "masked_pred_flow = pred_flow.clone() * 0.1\n",
+    "masked_pred_flow[idx_of_max_flow, :] *= 20\n",
+    "\n",
+    "R = torch.tensor([[0, 1, 0], [-1, 0, 0], [0, 0, 1]]).float()\n",
+    "\n",
+    "masked_pred_flow_world = masked_pred_flow @ R.T\n",
+    "print(idx_of_max_flow, masked_pred_flow_world[idx_of_max_flow], goal_orientation_mat_world)\n",
+    "\n",
+    "animation = FlowNetAnimation()\n",
+    "animation.add_trace(\n",
+    "    torch.as_tensor(sampled_points_world),\n",
+    "    # torch.as_tensor([pcd[mask]]),\n",
+    "    # torch.as_tensor([flow[mask]]),\n",
+    "    torch.as_tensor([sampled_points_world]),\n",
+    "    torch.as_tensor([masked_pred_flow_world.numpy()]),\n",
+    "    \"red\",\n",
+    ")\n",
+    "animation.add_trace(\n",
+    "    torch.as_tensor(torch.from_numpy(goal_point_world).repeat(3, 1).cpu().numpy()),\n",
+    "    # torch.as_tensor([pcd[mask]]),\n",
+    "    # torch.as_tensor([flow[mask]]),\n",
+    "    torch.as_tensor([torch.from_numpy(goal_point_world).repeat(3, 1).cpu().numpy()]),\n",
+    "    torch.as_tensor([goal_orientation_mat_world.T]),\n",
+    "    \"yellow\",\n",
+    ")\n",
+    "\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import plotly.graph_objects as go\n",
+    "\n",
+    "# Define vectors\n",
+    "vectors = [\n",
+    "    dict(x=[0, 3], y=[0, 2], z=[0, 1], name='Vector 1'),\n",
+    "    dict(x=[0, 1], y=[0, 2], z=[0, 3], name='Vector 2'),\n",
+    "    dict(x=[0, 1], y=[0, 0], z=[0, 1], name='Vector 3')\n",
+    "]\n",
+    "\n",
+    "# Create the figure\n",
+    "fig = go.Figure()\n",
+    "\n",
+    "# Add vectors to plot\n",
+    "for v in vectors:\n",
+    "    fig.add_trace(go.Scatter3d(x=v['x'], y=v['y'], z=v['z'], mode='lines+markers+text', name=v['name']))\n",
+    "\n",
+    "# Update layout for a nice aspect ratio\n",
+    "fig.update_layout(scene=dict(\n",
+    "    xaxis=dict(nticks=4, range=[-5,5]),\n",
+    "    yaxis=dict(nticks=4, range=[-5,5]),\n",
+    "    zaxis=dict(nticks=4, range=[-5,5])\n",
+    "), width=700)\n",
+    "\n",
+    "# Show the plot\n",
+    "fig.show()\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "print(contact_point)\n",
+    "print(goal_point, goal_orientation)\n",
+    "print(contact_point, goal_point, goal_orientation)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "print(flow_vector_normalized)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "import numpy as np\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "animation = FlowNetAnimation()\n",
+    "animation.add_trace(\n",
+    "    torch.as_tensor(pcd),\n",
+    "    # torch.as_tensor([pcd[mask]]),\n",
+    "    # torch.as_tensor([flow[mask]]),\n",
+    "    torch.as_tensor([pcd]),\n",
+    "    torch.as_tensor([np.zeros_like(pcd)]),\n",
+    "    \"red\",\n",
+    ")\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Visualize the predictions (With history)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## About the policy"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "- Switch grasp point"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Pseudo codes\n",
+    "def switch_grasp_point(last_gripper_pos, current_gripper_pos, flow_prediction, current_pcd):\n",
+    "    # 1 - find the point in current_pcd closest to current_grasp_point\n",
+    "    grasp_point_id = 0  # current_pcd's closest point id\n",
+    "    grasp_flow = flow_prediction[grasp_point_id]\n",
+    "    # 2 - Compare the grasp point flow with the max prediction flow\n",
+    "    leverage_increase = flow_prediction.norm(dim=-1).max() - grasp_flow.norm()\n",
+    "    if last_gripper_pos - current_gripper_pos < 0.01 or leverage_increase > 0.2:  # move threshold\n",
+    "        return True\n",
+    "    return False"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "- Use / not use history"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Basically we want to have a signal that tells us whether the last step makes positive progress"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Policy: Maintain a correct direction stack\n",
+    "\n",
+    "For each step, record the movement of the gripper (delta gripper)\n",
+    "\n",
+    "1) When the stack is empty, only push the delta gripper in the stack when the |delta gripper| > threshold (means that the gripper successfully moves the object)\n",
+    "\n",
+    "2) Always compare the current predicted direction with the direction from the stack top: \n",
+    "\n",
+    "- if different by over 90 degree, don't push the current action in and also don't execute this step\n",
+    "- Execute the step and push the delta gripper of this step in the stack"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "openany",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.17"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/notebooks/README.md b/notebooks/README.md
new file mode 100644
index 0000000..1da912a
--- /dev/null
+++ b/notebooks/README.md
@@ -0,0 +1,3 @@
+# Notebooks.
+
+This is where you can plop all your favorite notebooks.
diff --git a/notebooks/diffuser_inference.ipynb b/notebooks/diffuser_inference.ipynb
new file mode 100644
index 0000000..6b2c2c4
--- /dev/null
+++ b/notebooks/diffuser_inference.ipynb
@@ -0,0 +1,388 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Inference"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "ckpt_path = \"/home/yishu/flowbothd/logs/train_trajectory/2023-10-18/08-45-11/checkpoints/epoch=74-step=15000-val_loss=0.00-weights-only.ckpt\""
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from flowbothd.models.flow_trajectory_diffuser import (\n",
+    "    FlowTrajectoryDiffusionModule,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from hydra import compose, initialize\n",
+    "from omegaconf import OmegaConf\n",
+    "initialize(config_path=\"../configs\", version_base=\"1.3\")\n",
+    "cfg = compose(config_name=\"train_synthetic\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "cfg"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import rpad.pyg.nets.pointnet2 as pnp\n",
+    "network = pnp.PN2Dense(\n",
+    "    in_channels=67,\n",
+    "    out_channels=3,\n",
+    "    p=pnp.PN2DenseParams(),\n",
+    ")\n",
+    "\n",
+    "model = FlowTrajectoryDiffusionModule(network, cfg.training, cfg.model)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "ckpt = torch.load(ckpt_path)\n",
+    "model.load_state_dict(ckpt[\"state_dict\"])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch_geometric.loader as tgl\n",
+    "from flowbothd.datasets.flow_trajectory_dataset_pyg import FlowTrajectoryPyGDataset\n",
+    "from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule\n",
+    "datamodule = FlowTrajectoryDataModule(\n",
+    "    root=\"/home/yishu/datasets/partnet-mobility\",\n",
+    "    batch_size=1,\n",
+    "    num_workers=30,\n",
+    "    n_proc=2,\n",
+    "    seed=42,\n",
+    "    trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model\n",
+    "    toy_dataset = {\n",
+    "        \"id\": \"door-1\",\n",
+    "        \"train-train\": [\"8994\", \"9035\"],\n",
+    "        \"train-test\": [\"8994\", \"9035\"],\n",
+    "        \"test\": [\"8867\"],\n",
+    "    }\n",
+    ")\n",
+    "\n",
+    "train_dataloader = datamodule.train_dataloader()\n",
+    "val_dataloader = datamodule.train_val_dataloader()\n",
+    "unseen_dataloader = datamodule.unseen_dataloader()\n",
+    "\n",
+    "# datamodule = FlowTrajectoryPyGDataset(\n",
+    "#     root=\"/home/yishu/datasets/partnet-mobility/raw\",\n",
+    "#     split=\"umpnet-train-test\",\n",
+    "#     randomize_joints=True,\n",
+    "#     randomize_camera=True,\n",
+    "#     # batch_size=1,\n",
+    "#     # num_workers=30,\n",
+    "#     # n_proc=2,\n",
+    "#     seed=42,\n",
+    "#     trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model\n",
+    "# )\n",
+    "# unseen_dataloader = tgl.DataLoader(datamodule, 1, shuffle=False, num_workers=0)\n",
+    "\n",
+    "samples = list(enumerate(val_dataloader))\n",
+    "# # breakpoint()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from flowbothd.metrics.trajectory import artflownet_loss, flow_metrics, normalize_trajectory\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "import numpy as np\n",
+    "\n",
+    "@torch.no_grad()\n",
+    "def diffuse_visual(initial_noise, batch, model):  # 1 sample batch\n",
+    "    model.eval()\n",
+    "    \n",
+    "    animation = FlowNetAnimation()\n",
+    "    pcd = batch.pos.cpu().numpy()\n",
+    "    mask = batch.mask.cpu().long().numpy()\n",
+    "\n",
+    "\n",
+    "    bs = batch.delta.shape[0] // 1200\n",
+    "    # batch.traj_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
+    "    batch.traj_noise = initial_noise\n",
+    "    # batch.traj_noise = normalize_trajectory(batch.traj_noise)\n",
+    "    # breakpoint()\n",
+    "\n",
+    "    # import time\n",
+    "    # batch_time = 0\n",
+    "    # model_time = 0\n",
+    "    # noise_scheduler_time = 0\n",
+    "    # self.noise_scheduler_inference.set_timesteps(self.num_inference_timesteps)\n",
+    "    # print(self.noise_scheduler_inference.timesteps)\n",
+    "    # for t in self.noise_scheduler_inference.timesteps:\n",
+    "    for t in model.noise_scheduler.timesteps:\n",
+    "        \n",
+    "        # tm = time.time()\n",
+    "        batch.timesteps = torch.zeros(bs, device=model.device) + t  # Uniform t steps\n",
+    "        batch.timesteps = batch.timesteps.long()\n",
+    "        # batch_time += time.time() - tm\n",
+    "\n",
+    "        # tm = time.time()\n",
+    "        model_output = model(batch)          # bs * 1200, traj_len * 3\n",
+    "        model_output = model_output.reshape(model_output.shape[0], -1, 3)  # bs * 1200, traj_len, 3\n",
+    "\n",
+    "        print(model_output)\n",
+    "        \n",
+    "        batch.traj_noise = model.noise_scheduler.step(\n",
+    "            # batch.traj_noise = self.noise_scheduler_inference.step(\n",
+    "            model_output.reshape(\n",
+    "                -1, model.sample_size, model_output.shape[1], model_output.shape[2]\n",
+    "            ),\n",
+    "            t,\n",
+    "            batch.traj_noise.reshape(\n",
+    "                -1, model.sample_size, model_output.shape[1], model_output.shape[2]\n",
+    "            ),\n",
+    "        ).prev_sample\n",
+    "        batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)\n",
+    "\n",
+    "        # print(batch.traj_noise)\n",
+    "        if t % 1 == 0:\n",
+    "            flow = batch.traj_noise.squeeze().cpu().numpy()\n",
+    "            # print(flow[mask])\n",
+    "            # segmented_flow = np.zeros_like(flow, dtype=np.float32)\n",
+    "            # segmented_flow[mask] = flow[mask]\n",
+    "            # print(\"seg\", segmented_flow, \"flow\", flow)\n",
+    "            animation.add_trace(\n",
+    "                torch.as_tensor(pcd),\n",
+    "                # torch.as_tensor([pcd[mask]]),\n",
+    "                # torch.as_tensor([flow[mask]]),\n",
+    "                torch.as_tensor([pcd]),\n",
+    "                torch.as_tensor([flow]),\n",
+    "                \"red\",\n",
+    "            )\n",
+    "            # animation.append_gif_frame(f)\n",
+    "\n",
+    "    f_pred = batch.traj_noise\n",
+    "    f_pred = normalize_trajectory(f_pred)\n",
+    "    # largest_mag: float = torch.linalg.norm(\n",
+    "    #     f_pred, ord=2, dim=-1\n",
+    "    # ).max()\n",
+    "    # f_pred = f_pred / (largest_mag + 1e-6)\n",
+    "\n",
+    "    # Compute the loss.\n",
+    "    n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(\"cuda\")  # type: ignore\n",
+    "    f_ix = batch.mask.bool()\n",
+    "    f_target = batch.delta\n",
+    "    f_target = normalize_trajectory(f_target)\n",
+    "\n",
+    "    f_target = f_target.float()\n",
+    "    # loss = artflownet_loss(f_pred, f_target, n_nodes)\n",
+    "\n",
+    "    # Compute some metrics on flow-only regions.\n",
+    "    rmse, cos_dist, mag_error = flow_metrics(\n",
+    "        f_pred[f_ix], batch.delta[f_ix]\n",
+    "    )\n",
+    "\n",
+    "    return cos_dist, animation"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "samples[0][1].pos"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "len(samples)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sample = samples[1][1].cuda()\n",
+    "batch = sample\n",
+    "model = model.cuda()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "initial_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
+    "cos_dist, animation = diffuse_visual(initial_noise, batch, model)\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tqdm\n",
+    "for i in tqdm.tqdm(range(100)):\n",
+    "    initial_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
+    "    cos_dist, animation = diffuse_visual(initial_noise, batch, model)\n",
+    "    if cos_dist < -0.7:\n",
+    "        break"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "cos_dist"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tqdm\n",
+    "for i in tqdm.tqdm(range(100)):\n",
+    "    initial_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
+    "    cos_dist, animation = diffuse_visual(initial_noise, batch, model)\n",
+    "    if cos_dist > 0.5:\n",
+    "        break"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "cos_dist"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Find multimodal cases"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tqdm\n",
+    "repeat_times = 10\n",
+    "stop = False\n",
+    "for sample in tqdm.tqdm(samples):\n",
+    "    sample_id = sample[0]\n",
+    "    sample = sample[1]\n",
+    "    if stop:\n",
+    "        break\n",
+    "    batch = sample.cuda()\n",
+    "    has_correct = False\n",
+    "    has_incorrect = False\n",
+    "    for _ in range(repeat_times):\n",
+    "        cos_dist, animation = diffuse_visual(batch, model)\n",
+    "        if cos_dist > 0.7:\n",
+    "            has_correct = True\n",
+    "            correct_animation = animation\n",
+    "        elif cos_dist < 0: \n",
+    "            has_incorrect = True\n",
+    "            incorrect_animation = animation\n",
+    "    if has_correct and has_incorrect:\n",
+    "        print(sample_id, sample_id)\n",
+    "        stop = True\n",
+    "        break"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "openany",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.17"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/notebooks/flowbot_inference.ipynb b/notebooks/flowbot_inference.ipynb
new file mode 100644
index 0000000..5f54b10
--- /dev/null
+++ b/notebooks/flowbot_inference.ipynb
@@ -0,0 +1,345 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Flowbot inference"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "ckpt_path = \"/home/yishu/flowbothd/logs/train_trajectory/2023-11-15/23-59-12/checkpoints/epoch=199-step=9200.ckpt\""
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from flowbothd.models.flow_trajectory_predictor import (\n",
+    "    FlowTrajectoryTrainingModule\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from hydra import compose, initialize\n",
+    "from omegaconf import OmegaConf\n",
+    "initialize(config_path=\"../configs\", version_base=\"1.3\")\n",
+    "cfg = compose(config_name=\"train\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "cfg"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import rpad.pyg.nets.pointnet2 as pnp\n",
+    "network = pnp.PN2Dense(\n",
+    "    in_channels=0,\n",
+    "    out_channels=3,\n",
+    "    p=pnp.PN2DenseParams(),\n",
+    ")\n",
+    "\n",
+    "model = FlowTrajectoryTrainingModule(network, cfg.training)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "ckpt = torch.load(ckpt_path)\n",
+    "model.load_state_dict(ckpt[\"state_dict\"])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch_geometric.loader as tgl\n",
+    "from flowbothd.datasets.flow_trajectory_dataset_pyg import FlowTrajectoryPyGDataset\n",
+    "from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule\n",
+    "datamodule = FlowTrajectoryDataModule(\n",
+    "    root=\"/home/yishu/datasets/partnet-mobility\",\n",
+    "    batch_size=1,\n",
+    "    num_workers=30,\n",
+    "    n_proc=2,\n",
+    "    seed=42,\n",
+    "    trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model\n",
+    "    # toy_dataset = {\n",
+    "    #     \"id\": \"door-1\",\n",
+    "    #     \"train-train\": [\"8994\", \"9035\"],\n",
+    "    #     \"train-test\": [\"8994\", \"9035\"],\n",
+    "    #     \"test\": [\"8867\"],\n",
+    "    # }\n",
+    "    toy_dataset = {\n",
+    "        \"id\": \"door-full-new\",\n",
+    "        \"train-train\": [\"8877\", \"8893\", \"8897\", \"8903\", \"8919\", \"8930\", \"8961\", \"8997\", \"9016\", \"9032\", \"9035\", \"9041\", \"9065\", \"9070\", \"9107\", \"9117\", \"9127\", \"9128\", \"9148\", \"9164\", \"9168\", \"9277\", \"9280\", \"9281\", \"9288\", \"9386\", \"9388\", \"9410\"],\n",
+    "        \"train-test\": [\"8867\", \"8983\", \"8994\", \"9003\", \"9263\", \"9393\"],\n",
+    "        \"test\": [\"8867\", \"8983\", \"8994\", \"9003\", \"9263\", \"9393\"],\n",
+    "    }\n",
+    ")\n",
+    "\n",
+    "train_val_dataloader = datamodule.train_val_dataloader()\n",
+    "val_dataloader = datamodule.val_dataloader()\n",
+    "# unseen_dataloader = datamodule.unseen_dataloader()\n",
+    "\n",
+    "# datamodule = FlowTrajectoryPyGDataset(\n",
+    "#     root=\"/home/yishu/datasets/partnet-mobility/raw\",\n",
+    "#     split=\"umpnet-train-test\",\n",
+    "#     randomize_joints=True,\n",
+    "#     randomize_camera=True,\n",
+    "#     # batch_size=1,\n",
+    "#     # num_workers=30,\n",
+    "#     # n_proc=2,\n",
+    "#     seed=42,\n",
+    "#     trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model\n",
+    "# )\n",
+    "# unseen_dataloader = tgl.DataLoader(datamodule, 1, shuffle=False, num_workers=0)\n",
+    "\n",
+    "samples = list(enumerate(train_val_dataloader))\n",
+    "# # breakpoint()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from flowbothd.metrics.trajectory import artflownet_loss, flow_metrics, normalize_trajectory\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "import numpy as np\n",
+    "\n",
+    "@torch.no_grad()\n",
+    "def flowbot_visual(batch, model):  # 1 sample batch\n",
+    "    model.eval()\n",
+    "    \n",
+    "    animation = FlowNetAnimation()\n",
+    "    pcd = batch.pos.cpu().numpy()\n",
+    "    f_pred = model(batch)\n",
+    "    f_pred = normalize_trajectory(f_pred[:, None, :])\n",
+    "\n",
+    "    animation.add_trace(\n",
+    "        torch.as_tensor(pcd),\n",
+    "        # torch.as_tensor([pcd[mask]]),\n",
+    "        # torch.as_tensor([flow[mask]]),\n",
+    "        torch.as_tensor([pcd]),\n",
+    "        torch.as_tensor([f_pred.squeeze().cpu().numpy()]),\n",
+    "        \"red\",\n",
+    "    )\n",
+    "\n",
+    "    # largest_mag: float = torch.linalg.norm(\n",
+    "    #     f_pred, ord=2, dim=-1\n",
+    "    # ).max()\n",
+    "    # f_pred = f_pred / (largest_mag + 1e-6)\n",
+    "\n",
+    "    # Compute the loss.\n",
+    "    n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(\"cuda\")  # type: ignore\n",
+    "    f_ix = batch.mask.bool()\n",
+    "    f_target = batch.delta\n",
+    "    f_target = normalize_trajectory(f_target)\n",
+    "\n",
+    "    f_target = f_target.float()\n",
+    "    loss = artflownet_loss(f_pred, f_target, n_nodes)\n",
+    "\n",
+    "    # Compute some metrics on flow-only regions.\n",
+    "    rmse, cos_dist, mag_error = flow_metrics(\n",
+    "        f_pred[f_ix], batch.delta[f_ix]\n",
+    "    )\n",
+    "\n",
+    "    return rmse, cos_dist, mag_error, loss, animation"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "all_rmse = 0\n",
+    "all_cos_dist = 0\n",
+    "all_mag_error = 0\n",
+    "all_loss = 0\n",
+    "model = model.cuda()\n",
+    "for i in range(len(samples)):\n",
+    "    sample = samples[i][1].cuda()\n",
+    "    batch = sample\n",
+    "    rmse, cos_dist, mag_error, loss, animation = flowbot_visual(sample, model)\n",
+    "    all_rmse += rmse.item()\n",
+    "    all_cos_dist += cos_dist.item()\n",
+    "    all_loss += loss.item()\n",
+    "    all_mag_error += mag_error.item()\n",
+    "\n",
+    "all_rmse /= len(samples)\n",
+    "all_cos_dist /= len(samples)\n",
+    "all_mag_error /= len(samples)\n",
+    "all_loss /= len(samples)\n",
+    "print(f\"rmse:{all_rmse:.4f}, cos:{all_cos_dist:.4f}, mag:{all_mag_error:.4f}, flowloss:{all_loss:.4f}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Example 1"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "len(samples)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sample = samples[15][1].cuda()\n",
+    "batch = sample\n",
+    "model = model.cuda()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "rmse, cos_dist, mag_error, loss, animation = flowbot_visual(sample, model)\n",
+    "print(f\"rmse:{rmse:.4f}, cos:{cos_dist:.4f}, mag:{mag_error:.4f}, flowloss:{loss:.4f}\")\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "print(rmse, cos_dist, mag_error, loss)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "###  Example 2"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sample = samples[1][1].cuda()\n",
+    "batch = sample\n",
+    "model = model.cuda()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "rmse, cos_dist, mag_error, loss, animation = flowbot_visual(sample, model)\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "print(rmse, cos_dist, mag_error, loss)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Test 1"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sample = samples[0][1].cuda()\n",
+    "batch = sample\n",
+    "model = model.cuda()\n",
+    "\n",
+    "rmse, cos_dist, mag_error, loss, animation = flowbot_visual(sample, model)\n",
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "print(rmse, cos_dist, mag_error, loss)"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "openany",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.17"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/notebooks/flowbot_simulation.ipynb b/notebooks/flowbot_simulation.ipynb
new file mode 100644
index 0000000..f588c90
--- /dev/null
+++ b/notebooks/flowbot_simulation.ipynb
@@ -0,0 +1,253 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# The evaluation script that runs a rollout for each object in the eval-ed dataset and calculates:\n",
+    "# - success : 90% open\n",
+    "# - distance to open\n",
+    "import json\n",
+    "import os\n",
+    "\n",
+    "import hydra\n",
+    "import lightning as L\n",
+    "import numpy as np\n",
+    "import omegaconf\n",
+    "import pandas as pd\n",
+    "import rpad.pyg.nets.pointnet2 as pnp\n",
+    "import torch\n",
+    "import tqdm\n",
+    "import wandb\n",
+    "from rpad.visualize_3d import html\n",
+    "\n",
+    "from flowbothd.datasets.flowbot import FlowBotDataModule\n",
+    "from flowbothd.simulations.simulation import trial_with_prediction\n",
+    "from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn\n",
+    "\n",
+    "\n",
+    "def load_obj_id_to_category():\n",
+    "    # Extract existing classes.\n",
+    "    with open(\"../scripts/umpnet_data_split.json\", \"r\") as f:\n",
+    "        data = json.load(f)\n",
+    "\n",
+    "    id_to_cat = {}\n",
+    "    for _, category_dict in data.items():\n",
+    "        for category, split_dict in category_dict.items():\n",
+    "            for _, id_list in split_dict.items():\n",
+    "                for id in id_list:\n",
+    "                    id_to_cat[id] = category\n",
+    "    return id_to_cat\n",
+    "\n",
+    "\n",
+    "def load_obj_and_link():\n",
+    "    with open(\"../scripts/umpnet_object_list.json\", \"r\") as f:\n",
+    "        object_link_json = json.load(f)\n",
+    "    return object_link_json\n",
+    "\n",
+    "\n",
+    "id_to_cat = load_obj_id_to_category()\n",
+    "object_to_link = load_obj_and_link()\n",
+    "\n",
+    "object_ids = [   # Door\n",
+    "    \"8877\",\n",
+    "    \"8893\",\n",
+    "    \"8897\",\n",
+    "    \"8903\",\n",
+    "    \"8919\",\n",
+    "    \"8930\",\n",
+    "    \"8961\",\n",
+    "    \"8997\",\n",
+    "    \"9016\",\n",
+    "    \"9032\",\n",
+    "    \"9035\",\n",
+    "    \"9041\",\n",
+    "    \"9065\",\n",
+    "    \"9070\",\n",
+    "    \"9107\",\n",
+    "    \"9117\",\n",
+    "    \"9127\",\n",
+    "    \"9128\",\n",
+    "    \"9148\",\n",
+    "    \"9164\",\n",
+    "    \"9168\",\n",
+    "    \"9277\",\n",
+    "    \"9280\",\n",
+    "    \"9281\",\n",
+    "    \"9288\",\n",
+    "    \"9386\",\n",
+    "    \"9388\",\n",
+    "    \"9410\",\n",
+    "    \"8867\",\n",
+    "    \"8983\",\n",
+    "    \"8994\",\n",
+    "    \"9003\",\n",
+    "    \"9263\",\n",
+    "    \"9393\",\n",
+    "]\n",
+    "\n",
+    "\n",
+    "suc_results = []\n",
+    "suc_figs = []\n",
+    "fail_results = []\n",
+    "fail_figs = []\n",
+    "\n",
+    "from hydra import compose, initialize\n",
+    "from omegaconf import OmegaConf\n",
+    "initialize(config_path=\"../configs\", version_base=\"1.3\")\n",
+    "cfg = compose(config_name=\"eval_sim\")\n",
+    "\n",
+    "\n",
+    "######################################################################\n",
+    "# Torch settings.\n",
+    "######################################################################\n",
+    "\n",
+    "# Make deterministic + reproducible.\n",
+    "torch.backends.cudnn.deterministic = True\n",
+    "torch.backends.cudnn.benchmark = False\n",
+    "\n",
+    "# Since most of us are training on 3090s+, we can use mixed precision.\n",
+    "torch.set_float32_matmul_precision(\"medium\")\n",
+    "\n",
+    "# Global seed for reproducibility.\n",
+    "L.seed_everything(42)\n",
+    "\n",
+    "\n",
+    "mask_channel = 1 if cfg.inference.mask_input_channel else 0\n",
+    "network = pnp.PN2Dense(\n",
+    "    in_channels=mask_channel,\n",
+    "    out_channels=3 * cfg.inference.trajectory_len,\n",
+    "    p=pnp.PN2DenseParams(),\n",
+    ")\n",
+    "\n",
+    "ckpt_file = \"/home/yishu/flowbothd/logs/train_trajectory/2023-12-05/21-52-28/checkpoints/epoch=199-step=157200.ckpt\"\n",
+    "\n",
+    "# Load the network weights.\n",
+    "ckpt = torch.load(ckpt_file)\n",
+    "network.load_state_dict(\n",
+    "    {k.partition(\".\")[2]: v for k, v, in ckpt[\"state_dict\"].items()}\n",
+    ")\n",
+    "\n",
+    "# Simulation and results.\n",
+    "print(\"Simulating\")\n",
+    "\n",
+    "obj_cats = list(set(id_to_cat.values()))\n",
+    "metric_df = pd.DataFrame(\n",
+    "    np.zeros((len(set(id_to_cat.values())), 3)),\n",
+    "    index=obj_cats,\n",
+    "    columns=[\"count\", \"success_rate\", \"norm_dist\"],\n",
+    ")\n",
+    "category_counts = {}\n",
+    "# for obj_id, obj_cat in tqdm.tqdm(list(id_to_cat.items())):\n",
+    "for obj_id, available_links in tqdm.tqdm(list(object_to_link.items())):\n",
+    "    if obj_id not in object_ids:  # For Door dataset\n",
+    "        continue\n",
+    "\n",
+    "    obj_cat = id_to_cat[obj_id]\n",
+    "    if not os.path.exists(f\"/home/yishu/datasets/partnet-mobility/raw/{obj_id}\"):\n",
+    "        continue\n",
+    "    print(f\"OBJ {obj_id} of {obj_cat}\")\n",
+    "    trial_figs, trial_results = trial_with_prediction(\n",
+    "        obj_id=obj_id,\n",
+    "        network=network,\n",
+    "        n_step=30,\n",
+    "        gui=cfg.gui,\n",
+    "        all_joint=True,\n",
+    "        # available_joints=available_links,   # Don't use this for doors\n",
+    "        website=cfg.website,\n",
+    "    )\n",
+    "\n",
+    "    if len(trial_results) == 0:  # If nothing succeeds\n",
+    "        continue\n",
+    "\n",
+    "    # Wandb table\n",
+    "    if obj_cat not in category_counts.keys():\n",
+    "        category_counts[obj_cat] = 0\n",
+    "    # category_counts[obj_cat] += len(trial_results)\n",
+    "    for (fig, result) in zip(trial_figs, trial_results):\n",
+    "        if result.contact == False:\n",
+    "            continue\n",
+    "        category_counts[obj_cat] += 1\n",
+    "        metric_df.loc[obj_cat][\"success_rate\"] += result.success\n",
+    "        # print(fig, trial_figs)\n",
+    "        if result.success:\n",
+    "            suc_results.append(result.metric)\n",
+    "            # print(fig, trial_figs)\n",
+    "            suc_figs.append(trial_figs[fig])\n",
+    "\n",
+    "        if result.metric < 0.1:\n",
+    "            fail_results.append(result.metric)\n",
+    "            # print(fig, trial_figs)\n",
+    "            fail_figs.append(trial_figs[fig])\n",
+    "\n",
+    "        metric_df.loc[obj_cat][\"norm_dist\"] += result.metric\n",
+    "\n",
+    "    wandb_df = metric_df.copy(deep=True)\n",
+    "    for obj_cat in category_counts.keys():\n",
+    "        wandb_df.loc[obj_cat][\"success_rate\"] /= category_counts[obj_cat]\n",
+    "        wandb_df.loc[obj_cat][\"norm_dist\"] /= category_counts[obj_cat]\n",
+    "        wandb_df.loc[obj_cat][\"count\"] = category_counts[obj_cat]\n",
+    "\n",
+    "    table = wandb.Table(dataframe=wandb_df.reset_index())\n",
+    "\n",
+    "# for obj_cat in category_counts.keys():\n",
+    "#     metric_df.loc[obj_cat][\"success_rate\"] /= category_counts[obj_cat]\n",
+    "#     metric_df.loc[obj_cat][\"norm_dist\"] /= category_counts[obj_cat]\n",
+    "#     metric_df.loc[obj_cat][\"category\"] = obj_cat\n",
+    "\n",
+    "# table = wandb.Table(dataframe=metric_df)\n",
+    "# run.log({f\"simulation_metric_table\": table})\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "fail_results"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "wandb_df"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "fail_figs[3].show()"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "openany",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.17"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/notebooks/multimodal.ipynb b/notebooks/multimodal.ipynb
new file mode 100644
index 0000000..c7ec7d5
--- /dev/null
+++ b/notebooks/multimodal.ipynb
@@ -0,0 +1,315 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import rpad.partnet_mobility_utils.dataset as rpd\n",
+    "all_objs = (\n",
+    "    rpd.UMPNET_TEST_OBJS\n",
+    ")\n",
+    "id_to_obj_class = {obj_id: obj_class for obj_id, obj_class in all_objs}"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "set(id_to_obj_class.values())"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from hydra import compose, initialize\n",
+    "from omegaconf import OmegaConf\n",
+    "initialize(config_path=\"../configs\", version_base=\"1.3\")\n",
+    "cfg = compose(config_name=\"train\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch_geometric.loader as tgl\n",
+    "from flowbothd.datasets.flow_trajectory_dataset_pyg import FlowTrajectoryPyGDataset\n",
+    "datamodule = FlowTrajectoryPyGDataset(\n",
+    "    root=\"/home/yishu/datasets/partnet-mobility/raw\",\n",
+    "    split=\"umpnet-test\",\n",
+    "    randomize_joints=True,\n",
+    "    randomize_camera=True,\n",
+    "    # batch_size=1,\n",
+    "    # num_workers=30,\n",
+    "    # n_proc=2,\n",
+    "    seed=42,\n",
+    "    trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model\n",
+    ")\n",
+    "val_dataloader = tgl.DataLoader(datamodule, 1, shuffle=False, num_workers=0)\n",
+    "\n",
+    "samples = list(enumerate(val_dataloader))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tqdm\n",
+    "door_cnt = 0\n",
+    "door_samples = []\n",
+    "for sample in tqdm.tqdm(samples):\n",
+    "    sample_id = sample[1].id[0]\n",
+    "    sample_class = id_to_obj_class[sample_id]\n",
+    "    if sample_class==\"Door\":\n",
+    "        door_cnt += 1\n",
+    "        door_samples.append(sample[1])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "door_cnt"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Diffuser visual"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
+    "from flowbothd.metrics.trajectory import artflownet_loss, flow_metrics, normalize_trajectory\n",
+    "\n",
+    "@torch.no_grad()\n",
+    "def diffuse_visual(batch, model):  # 1 sample batch\n",
+    "    model.eval()\n",
+    "    \n",
+    "    animation = FlowNetAnimation()\n",
+    "    pcd = batch.pos.cpu().numpy()\n",
+    "    mask = batch.mask.cpu().long().numpy()\n",
+    "\n",
+    "    fix_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
+    "\n",
+    "    bs = batch.delta.shape[0] // 1200\n",
+    "    # batch.traj_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
+    "    batch.traj_noise = fix_noise\n",
+    "    # batch.traj_noise = normalize_trajectory(batch.traj_noise)\n",
+    "    # breakpoint()\n",
+    "\n",
+    "    # import time\n",
+    "    # batch_time = 0\n",
+    "    # model_time = 0\n",
+    "    # noise_scheduler_time = 0\n",
+    "    # self.noise_scheduler_inference.set_timesteps(self.num_inference_timesteps)\n",
+    "    # print(self.noise_scheduler_inference.timesteps)\n",
+    "    # for t in self.noise_scheduler_inference.timesteps:\n",
+    "    for t in model.noise_scheduler.timesteps:\n",
+    "        \n",
+    "        # tm = time.time()\n",
+    "        batch.timesteps = torch.zeros(bs, device=model.device) + t  # Uniform t steps\n",
+    "        batch.timesteps = batch.timesteps.long()\n",
+    "        # batch_time += time.time() - tm\n",
+    "\n",
+    "        # tm = time.time()\n",
+    "        model_output = model(batch)          # bs * 1200, traj_len * 3\n",
+    "        model_output = model_output.reshape(model_output.shape[0], -1, 3)  # bs * 1200, traj_len, 3\n",
+    "        \n",
+    "        batch.traj_noise = model.noise_scheduler.step(\n",
+    "            # batch.traj_noise = self.noise_scheduler_inference.step(\n",
+    "            model_output.reshape(\n",
+    "                -1, model.sample_size, model_output.shape[1], model_output.shape[2]\n",
+    "            ),\n",
+    "            t,\n",
+    "            batch.traj_noise.reshape(\n",
+    "                -1, model.sample_size, model_output.shape[1], model_output.shape[2]\n",
+    "            ),\n",
+    "        ).prev_sample\n",
+    "        batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)\n",
+    "\n",
+    "        # print(batch.traj_noise)\n",
+    "        if t % 50 == 0:\n",
+    "            flow = batch.traj_noise.squeeze().cpu().numpy()\n",
+    "            # print(flow[mask])\n",
+    "            # segmented_flow = np.zeros_like(flow, dtype=np.float32)\n",
+    "            # segmented_flow[mask] = flow[mask]\n",
+    "            # print(\"seg\", segmented_flow, \"flow\", flow)\n",
+    "            animation.add_trace(\n",
+    "                torch.as_tensor(pcd),\n",
+    "                # torch.as_tensor([pcd[mask]]),\n",
+    "                # torch.as_tensor([flow[mask].detach().cpu().numpy()]),\n",
+    "                torch.as_tensor([pcd]),\n",
+    "                torch.as_tensor([normalize_trajectory(batch.traj_noise).squeeze().cpu().numpy()]),\n",
+    "                \"red\",\n",
+    "            )\n",
+    "\n",
+    "    f_pred = batch.traj_noise\n",
+    "    f_pred = normalize_trajectory(f_pred)\n",
+    "    # largest_mag: float = torch.linalg.norm(\n",
+    "    #     f_pred, ord=2, dim=-1\n",
+    "    # ).max()\n",
+    "    # f_pred = f_pred / (largest_mag + 1e-6)\n",
+    "\n",
+    "    # Compute the loss.\n",
+    "    n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(\"cuda\")  # type: ignore\n",
+    "    f_ix = batch.mask.bool()\n",
+    "    f_target = batch.delta\n",
+    "    f_target = normalize_trajectory(f_target)\n",
+    "\n",
+    "    f_target = f_target.float()\n",
+    "    # loss = artflownet_loss(f_pred, f_target, n_nodes)\n",
+    "\n",
+    "    # Compute some metrics on flow-only regions.\n",
+    "    rmse, cos_dist, mag_error = flow_metrics(\n",
+    "        f_pred[f_ix], batch.delta[f_ix]\n",
+    "    )\n",
+    "\n",
+    "    return cos_dist, animation"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Model"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import rpad.pyg.nets.pointnet2 as pnp\n",
+    "from flowbothd.models.flow_trajectory_diffuser import (\n",
+    "    FlowTrajectoryDiffusionModule,\n",
+    ")\n",
+    "ckpt_path = \"/home/yishu/flowbothd/logs/train_trajectory/2023-08-31/16-13-10/checkpoints/epoch=394-step=310470-val_loss=0.00-weights-only.ckpt\"\n",
+    "network = pnp.PN2Dense(\n",
+    "    in_channels=67,\n",
+    "    out_channels=3,\n",
+    "    p=pnp.PN2DenseParams(),\n",
+    ")\n",
+    "\n",
+    "model = FlowTrajectoryDiffusionModule(network, cfg.training, cfg.model)\n",
+    "ckpt = torch.load(ckpt_path)\n",
+    "model.load_state_dict(ckpt[\"state_dict\"])\n",
+    "model = model.cuda()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tqdm\n",
+    "import math\n",
+    "best_animations = []\n",
+    "best_cos_dists = []\n",
+    "worst_animations = []\n",
+    "worst_cos_dists = []\n",
+    "mean_cos_dist = 0\n",
+    "for sample in tqdm.tqdm(door_samples[1:2]):\n",
+    "    best_cos = -1\n",
+    "    best_cos_reverse = 1\n",
+    "    for repeat in range(10):\n",
+    "        cos_dist, animation = diffuse_visual(sample.cuda(), model)\n",
+    "        if cos_dist > best_cos:\n",
+    "            best_animation = animation\n",
+    "        if cos_dist < best_cos_reverse:\n",
+    "            worst_animation = animation\n",
+    "        \n",
+    "        best_cos = max(best_cos, cos_dist)\n",
+    "        best_cos_reverse = min(best_cos_reverse, cos_dist)\n",
+    "    \n",
+    "    best_animations.append(best_animation)\n",
+    "    best_cos_dists.append(best_cos)\n",
+    "    worst_animations.append(worst_animation)\n",
+    "    worst_cos_dists.append(best_cos_reverse)\n",
+    "    mean_cos_dist += best_cos\n",
+    "mean_cos_dist /= 27"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "mean_cos_dist"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "for i in range(27):\n",
+    "    print(best_cos_dists[i], worst_cos_dists[i])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "fig = animation[0].animate()\n",
+    "fig.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "fig = animation.animate()\n",
+    "fig.show()"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "openany",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.17"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/notebooks/well_formed.txt b/notebooks/well_formed.txt
new file mode 100644
index 0000000..23d8dea
--- /dev/null
+++ b/notebooks/well_formed.txt
@@ -0,0 +1,2218 @@
+100013,Remote
+100015,KitchenPot
+100017,KitchenPot
+100021,KitchenPot
+100023,KitchenPot
+100025,KitchenPot
+100028,KitchenPot
+100031,Kettle
+100032,KitchenPot
+100033,KitchenPot
+100038,KitchenPot
+100040,KitchenPot
+100045,KitchenPot
+100047,KitchenPot
+100051,KitchenPot
+100054,KitchenPot
+100055,KitchenPot
+100056,KitchenPot
+100057,KitchenPot
+100058,KitchenPot
+100060,KitchenPot
+100061,USB
+100064,USB
+100065,USB
+100068,USB
+100071,USB
+100072,USB
+100073,USB
+100075,Cart
+100078,USB
+100079,USB
+100082,USB
+100084,Cart
+100085,USB
+100086,USB
+100087,USB
+100092,USB
+100095,USB
+100103,USB
+100106,USB
+100108,USB
+100109,USB
+100113,USB
+100116,USB
+100123,USB
+100128,USB
+100133,USB
+100141,Box
+100142,Pliers
+100144,Pliers
+100146,Pliers
+100150,Pliers
+100154,Box
+100162,Box
+100172,Pliers
+100174,Box
+100178,Pliers
+100179,Pliers
+100180,Pliers
+100182,Pliers
+100188,Pliers
+100189,Box
+100191,Box
+100194,Box
+100197,Box
+100202,Box
+100214,Box
+100221,Box
+100224,Box
+100234,Box
+100243,Box
+100247,Box
+100248,Suitcase
+100249,Suitcase
+100269,Remote
+100270,Remote
+100279,Printer
+100282,WashingMachine
+100283,WashingMachine
+100285,Lighter
+100289,Lighter
+100292,Lighter
+100293,Lighter
+100294,Lighter
+100295,Lighter
+100309,Lighter
+100310,Lighter
+100311,Lighter
+100313,Lighter
+100317,Lighter
+100318,Lighter
+100319,Lighter
+100320,Lighter
+100321,Lighter
+100330,Lighter
+100334,Lighter
+100335,Lighter
+100340,Lighter
+100343,Lighter
+100348,Lighter
+100350,Lighter
+100354,Lighter
+100355,Lighter
+10036,Refrigerator
+100366,Switch
+100367,Switch
+100368,Switch
+100385,Remote
+100392,Remote
+100394,Remote
+10040,Laptop
+100405,Remote
+100408,Remote
+100412,Remote
+100426,Box
+100431,Bucket
+100432,Bucket
+100435,Bucket
+100438,Bucket
+100439,Bucket
+100441,Bucket
+100442,Bucket
+100443,Bucket
+100444,Bucket
+100446,Bucket
+100448,Bucket
+100452,Bucket
+100454,Bucket
+100460,Bucket
+100461,Bucket
+100462,Bucket
+100464,Bucket
+100465,Bucket
+100466,Bucket
+100468,Bucket
+100469,Bucket
+100470,Bucket
+100472,Bucket
+100473,Bucket
+100477,Bucket
+100481,Bucket
+100482,Bucket
+100484,Bucket
+100487,Cart
+100488,Cart
+100490,Cart
+100491,Cart
+100494,Cart
+100496,Cart
+100498,Cart
+100500,Cart
+100501,Cart
+100502,Cart
+100504,Cart
+100507,Cart
+100508,Cart
+100509,Cart
+100511,USB
+100513,USB
+100520,FoldingChair
+100521,FoldingChair
+100523,FoldingChair
+100526,FoldingChair
+100531,FoldingChair
+100532,FoldingChair
+100550,Suitcase
+100557,FoldingChair
+100561,FoldingChair
+100562,FoldingChair
+100568,FoldingChair
+100579,FoldingChair
+100586,FoldingChair
+100587,FoldingChair
+100590,FoldingChair
+100598,FoldingChair
+100599,FoldingChair
+100600,FoldingChair
+100608,FoldingChair
+100609,FoldingChair
+100611,FoldingChair
+100613,KitchenPot
+100616,FoldingChair
+100619,KitchenPot
+100623,KitchenPot
+100658,Box
+100664,Box
+100671,Box
+100676,Box
+10068,Refrigerator
+100685,Box
+100693,KitchenPot
+100705,Pliers
+100706,Remote
+100712,Remote
+100720,Globe
+100731,TrashCan
+100732,TrashCan
+100733,Globe
+100736,Globe
+100739,Globe
+100740,Globe
+100741,Globe
+100743,Globe
+100744,Globe
+100745,Globe
+100746,Globe
+100747,Globe
+100748,Globe
+100749,Globe
+100750,Globe
+100751,Globe
+100753,Globe
+100754,Globe
+100755,Globe
+100756,Globe
+100757,Globe
+100758,Globe
+100759,Globe
+100760,Globe
+100761,Globe
+100762,Globe
+100763,Globe
+100764,Globe
+100765,Globe
+100767,Suitcase
+100768,Globe
+100770,Globe
+100776,Suitcase
+100778,Globe
+100779,Globe
+100781,Globe
+100782,Globe
+100783,Globe
+100784,Globe
+100785,Globe
+100786,Globe
+100787,Globe
+100788,Globe
+100789,Globe
+100790,Globe
+100791,Globe
+100792,Globe
+100793,Globe
+100794,Globe
+100795,Globe
+100796,Globe
+100798,Globe
+100799,Globe
+100800,Globe
+100801,Globe
+100803,Globe
+100809,Remote
+100811,Remote
+100814,Remote
+100816,Remote
+100819,Remote
+100825,Suitcase
+100828,Remote
+100836,Suitcase
+100837,Suitcase
+100838,Suitcase
+100839,Suitcase
+100840,Suitcase
+100842,Suitcase
+100845,Switch
+100846,Switch
+100847,Switch
+100848,Switch
+100849,Switch
+100850,Switch
+100852,Cart
+100853,Cart
+100854,Cart
+100856,Cart
+100858,Cart
+100860,Cart
+100861,Cart
+100866,Switch
+100870,Switch
+100871,Switch
+100880,Switch
+100882,Switch
+100883,Switch
+100885,Switch
+100888,Switch
+100889,Switch
+100900,Switch
+100901,Switch
+100902,Switch
+100904,Switch
+100905,Switch
+100906,Switch
+100907,Switch
+100908,Switch
+100910,Switch
+100911,Switch
+100914,Switch
+100915,Switch
+100919,Switch
+100920,Switch
+100924,Switch
+100925,Switch
+100928,Switch
+100930,Switch
+100933,Switch
+100934,Switch
+100935,Switch
+100937,Switch
+100938,Cart
+100939,Cart
+100948,Switch
+100952,Switch
+100953,Switch
+100954,Switch
+100955,Switch
+100957,Switch
+100959,Switch
+100963,Switch
+100965,Switch
+100966,Switch
+100968,Switch
+100970,Switch
+100971,Switch
+100974,Switch
+100976,Switch
+100978,Switch
+100979,Switch
+10098,Laptop
+100980,Switch
+100981,Switch
+100982,Window
+100991,Remote
+100993,Remote
+100997,Remote
+100999,Remote
+101002,Remote
+101004,Remote
+101007,Remote
+10101,Laptop
+101010,Remote
+101011,Remote
+101014,Remote
+101015,Remote
+101016,Remote
+101023,Remote
+101028,Remote
+101034,Remote
+101048,Suitcase
+101049,Suitcase
+101050,Suitcase
+101051,Suitcase
+101052,Knife
+101053,Cart
+101054,Knife
+101055,Cart
+101057,Knife
+101059,Knife
+101060,Cart
+101062,Knife
+101064,Cart
+101065,Cart
+101066,Cart
+101068,Knife
+101072,Cart
+101073,Cart
+101075,Cart
+101077,Cart
+101079,Knife
+101080,Knife
+101081,Cart
+101083,Cart
+101085,Knife
+101086,Cart
+101090,Cart
+101091,Cart
+101092,Cart
+101093,Cart
+101095,Knife
+101097,Cart
+101099,Cart
+1011,Faucet
+101102,Cart
+101104,Remote
+101106,Knife
+101107,Knife
+101108,Knife
+101112,Knife
+101114,Knife
+101115,Knife
+101117,Remote
+101118,Remote
+101121,Remote
+101131,Remote
+101133,Remote
+101139,Remote
+101142,Remote
+101176,Cart
+101178,Cart
+101182,Cart
+101183,Cart
+101217,Knife
+101220,Fan
+101236,Knife
+101245,Knife
+101253,Knife
+101260,Knife
+101284,Eyeglasses
+101285,Eyeglasses
+101287,Eyeglasses
+101288,Eyeglasses
+101291,Eyeglasses
+101293,Eyeglasses
+101297,Eyeglasses
+101300,Eyeglasses
+101303,Eyeglasses
+101305,Kettle
+101311,Kettle
+101313,Kettle
+101315,Kettle
+101319,Toilet
+101320,Toilet
+101323,Toilet
+101326,Eyeglasses
+101328,Eyeglasses
+101332,Eyeglasses
+101335,Eyeglasses
+101336,Dispenser
+101352,Camera
+101362,Camera
+101363,Safe
+101365,Fan
+101366,Fan
+101369,Fan
+101371,Fan
+101372,Fan
+101373,Fan
+101374,Fan
+101375,Fan
+101377,TrashCan
+101378,TrashCan
+101380,TrashCan
+101382,Fan
+101383,Fan
+101384,TrashCan
+101386,Fan
+101387,Fan
+101388,Fan
+101389,Fan
+101396,Fan
+101397,Fan
+101399,Mouse
+101401,Fan
+101402,Fan
+101403,Mouse
+101405,Fan
+101407,Fan
+101408,Mouse
+101416,Mouse
+101417,Dispenser
+101419,Fan
+101420,Fan
+101421,Fan
+101422,Fan
+101423,Fan
+101425,Fan
+101428,Fan
+101429,Fan
+10143,Refrigerator
+101432,Fan
+101433,Fan
+101435,Fan
+101436,Fan
+101437,Fan
+101439,Fan
+10144,Refrigerator
+101440,Fan
+101441,Fan
+101444,Fan
+101445,Fan
+101446,Fan
+101448,Fan
+101449,Fan
+101450,Fan
+101456,Fan
+101457,Fan
+101460,Fan
+101461,Fan
+101463,Dispenser
+101465,Fan
+101467,Fan
+101468,Fan
+101469,Fan
+101470,Fan
+101472,Fan
+101475,Fan
+101476,Fan
+101481,Fan
+101483,Fan
+101489,Dispenser
+101490,Dispenser
+101493,Fan
+101494,Fan
+101496,Fan
+101501,Dispenser
+101504,Fan
+101505,Fan
+101507,Dispenser
+101510,Fan
+101511,Fan
+101517,Dispenser
+101523,Fan
+101524,Fan
+101528,Dispenser
+101533,Dispenser
+101539,Dispenser
+101540,Dispenser
+101541,Dispenser
+101542,Dispenser
+101546,Dispenser
+101548,Fan
+101557,Dispenser
+101560,Dispenser
+101561,Dispenser
+101563,Dispenser
+101564,Safe
+101565,Dispenser
+101566,Dispenser
+101579,Safe
+101583,Safe
+101584,Safe
+101591,Safe
+101593,Safe
+101594,Safe
+101599,Safe
+101603,Safe
+101604,Safe
+101605,Safe
+101611,Safe
+101612,Safe
+101613,Safe
+101619,Safe
+101623,Safe
+101659,Knife
+101660,Knife
+101662,Knife
+101668,Suitcase
+101673,Suitcase
+101681,Suitcase
+101685,Pen
+101698,Pen
+101703,Pen
+101712,Pen
+101713,Pen
+101714,Pen
+101722,Pen
+101727,Pen
+101732,Pen
+101735,Pen
+101736,Pen
+101741,Pen
+101748,Pen
+101770,Pen
+101773,Oven
+101786,Pen
+101787,Pen
+101793,Pen
+101796,Pen
+101808,Oven
+101831,Eyeglasses
+101833,Eyeglasses
+101836,Eyeglasses
+101838,Eyeglasses
+101839,Eyeglasses
+101840,Eyeglasses
+101842,Eyeglasses
+101843,Eyeglasses
+101844,Eyeglasses
+101845,Eyeglasses
+101848,Eyeglasses
+101859,Eyeglasses
+101860,Eyeglasses
+101861,Eyeglasses
+101863,Eyeglasses
+101864,Eyeglasses
+101866,Eyeglasses
+101868,Eyeglasses
+101869,Eyeglasses
+101870,Eyeglasses
+101871,Eyeglasses
+101874,Eyeglasses
+101886,USB
+101892,Cart
+101908,Oven
+101909,Oven
+101917,Oven
+101921,Oven
+101924,Oven
+101930,Oven
+101931,Oven
+101940,Oven
+101943,Oven
+101946,Oven
+101947,Oven
+101948,USB
+101950,USB
+101952,USB
+101960,USB
+101971,Oven
+101982,USB
+101983,USB
+101994,USB
+101999,USB
+102001,Oven
+102004,USB
+102008,USB
+102009,USB
+102016,USB
+102018,Oven
+102019,Oven
+102021,USB
+102024,USB
+102025,USB
+102033,USB
+102037,USB
+102042,USB
+102044,Oven
+102052,USB
+102053,USB
+102055,Oven
+102060,Oven
+102062,USB
+102063,USB
+102065,USB
+102068,USB
+102070,Fan
+102073,Pliers
+102074,Pliers
+102075,Pliers
+102080,KitchenPot
+102090,Fan
+102092,Fan
+102093,Fan
+102095,Fan
+102099,Fan
+102100,Fan
+10213,Laptop
+102130,Remote
+102145,CoffeeMachine
+102153,TrashCan
+102154,TrashCan
+102155,TrashCan
+102156,TrashCan
+102158,TrashCan
+102160,TrashCan
+102163,TrashCan
+102165,TrashCan
+102171,TrashCan
+102177,TrashCan
+102181,TrashCan
+102182,TrashCan
+102186,TrashCan
+102187,TrashCan
+102189,TrashCan
+102192,TrashCan
+102193,TrashCan
+102194,TrashCan
+102200,TrashCan
+102201,TrashCan
+102202,TrashCan
+102209,TrashCan
+102218,TrashCan
+102219,TrashCan
+102221,Pliers
+102227,TrashCan
+102229,TrashCan
+102234,TrashCan
+102242,Pliers
+102244,TrashCan
+102252,TrashCan
+102254,TrashCan
+102255,FoldingChair
+102256,TrashCan
+102257,TrashCan
+102258,Pliers
+102259,TrashCan
+102260,Pliers
+102263,FoldingChair
+102269,FoldingChair
+102272,Mouse
+102273,Mouse
+102276,Mouse
+102285,Pliers
+102288,Pliers
+102290,Mouse
+102292,Pliers
+102301,Safe
+102309,Safe
+102311,Safe
+102314,FoldingChair
+102316,Safe
+102318,Safe
+102333,FoldingChair
+102347,Cart
+102352,Bucket
+102358,Bucket
+102359,Bucket
+102363,Bucket
+102365,Bucket
+102367,Bucket
+102369,Bucket
+102373,Box
+102377,Box
+102379,Box
+10238,Laptop
+102380,Safe
+102381,Safe
+102384,Safe
+102387,Safe
+102389,Safe
+102394,Camera
+102398,Camera
+102400,Knife
+102401,Knife
+102402,Knife
+102403,Camera
+102407,Camera
+102408,Camera
+102411,Camera
+102414,Camera
+102417,Camera
+102418,Safe
+102423,Safe
+10243,Laptop
+102431,Camera
+102432,Camera
+102434,Camera
+102442,Camera
+102456,Box
+102472,Camera
+10248,Laptop
+102505,Camera
+102506,Camera
+102520,Camera
+102523,Camera
+102527,Camera
+102528,Camera
+102532,Camera
+102536,Camera
+102539,Camera
+102542,Camera
+102546,Cart
+102551,Cart
+102552,Cart
+102555,Cart
+102556,Cart
+102557,Cart
+102561,Cart
+102562,Cart
+102567,Eyeglasses
+102568,Eyeglasses
+102569,Eyeglasses
+102570,Eyeglasses
+102571,Eyeglasses
+102572,Eyeglasses
+102573,Eyeglasses
+102578,Eyeglasses
+102586,Eyeglasses
+102587,Eyeglasses
+102588,Eyeglasses
+102589,Eyeglasses
+102590,Eyeglasses
+102591,Eyeglasses
+102596,Eyeglasses
+102599,Eyeglasses
+102601,Eyeglasses
+102603,Eyeglasses
+102608,Eyeglasses
+102611,Eyeglasses
+102612,Eyeglasses
+102617,Eyeglasses
+102619,Toilet
+102620,Toilet
+102621,Toilet
+102622,Toilet
+102625,Toilet
+102628,Toilet
+102629,Toilet
+102630,Toilet
+102631,Toilet
+102632,Toilet
+102634,Toilet
+102636,Toilet
+102639,Toilet
+102641,Toilet
+102643,Toilet
+102645,Toilet
+102646,Toilet
+102647,Toilet
+102648,Toilet
+102649,Toilet
+102650,Toilet
+102651,Toilet
+102652,Toilet
+102654,Toilet
+102655,Toilet
+102657,Toilet
+102658,Toilet
+102660,Toilet
+102662,Toilet
+102663,Toilet
+102664,Toilet
+102665,Toilet
+102666,Toilet
+102667,Toilet
+102668,Toilet
+102669,Toilet
+102670,Toilet
+102675,Toilet
+102676,Toilet
+102677,Toilet
+102678,Toilet
+102679,Toilet
+102682,Toilet
+102684,Toilet
+102685,Toilet
+102687,Toilet
+102688,Toilet
+102689,Toilet
+10269,Laptop
+102690,Toilet
+102692,Toilet
+102694,Toilet
+102697,Toilet
+102698,Toilet
+102699,Toilet
+10270,Laptop
+102701,Toilet
+102702,Toilet
+102703,Toilet
+102704,Toilet
+102706,Toilet
+102707,Toilet
+102708,Toilet
+102710,Toilet
+102714,Kettle
+102715,Kettle
+102720,Kettle
+102724,Kettle
+102726,Kettle
+102730,Kettle
+102732,Kettle
+102736,Kettle
+102738,Kettle
+102739,Kettle
+102753,Kettle
+102756,Kettle
+102761,Kettle
+102763,Kettle
+102765,Kettle
+102768,Kettle
+102773,Kettle
+102786,Kettle
+102798,Window
+1028,Faucet
+10280,Laptop
+102801,Window
+102802,Window
+102803,Window
+102804,Window
+102810,Switch
+102812,Switch
+102817,Switch
+102829,Camera
+102831,Camera
+102834,Camera
+102836,Switch
+102839,Switch
+102843,Switch
+102844,Switch
+102845,Camera
+102852,Camera
+102856,Switch
+102860,Switch
+102864,Switch
+102872,Switch
+102873,Camera
+102874,Camera
+102876,Camera
+102882,Camera
+10289,Laptop
+102892,Camera
+102896,Window
+102901,CoffeeMachine
+102903,Window
+102905,Window
+102906,Window
+102909,Pen
+102910,Pen
+102911,Pen
+102915,Pen
+102916,Pen
+102917,Pen
+102918,Pen
+102922,Pen
+102931,Pen
+102938,Pen
+102939,Pen
+102940,Pen
+102942,Pen
+102943,Pen
+102944,Pen
+102945,Pen
+102946,Pen
+102952,Pen
+102957,Pen
+102960,Pen
+102961,Pen
+102962,Pen
+102963,Pen
+102965,Pen
+102966,Pen
+102970,Pen
+102973,Pen
+102975,Pen
+102977,Window
+102980,Pen
+102982,Pen
+102990,Stapler
+102992,TrashCan
+102996,TrashCan
+103002,CoffeeMachine
+103007,TrashCan
+103008,TrashCan
+103009,CoffeeMachine
+103010,TrashCan
+103013,TrashCan
+103015,Window
+103020,Mouse
+103022,Mouse
+103023,Mouse
+103024,Mouse
+103025,Mouse
+103026,Mouse
+103028,Eyeglasses
+103029,Eyeglasses
+103030,CoffeeMachine
+103031,CoffeeMachine
+103032,Window
+103037,CoffeeMachine
+103038,CoffeeMachine
+103040,Window
+103041,CoffeeMachine
+103042,Window
+103043,CoffeeMachine
+103044,Window
+103048,CoffeeMachine
+10305,Laptop
+103051,Window
+103052,Window
+103056,Window
+103057,CoffeeMachine
+103058,Window
+10306,Laptop
+103060,CoffeeMachine
+103062,CoffeeMachine
+103063,Window
+103064,CoffeeMachine
+103065,CoffeeMachine
+103069,CoffeeMachine
+103070,Window
+103072,CoffeeMachine
+103079,CoffeeMachine
+103080,CoffeeMachine
+103084,CoffeeMachine
+103086,CoffeeMachine
+103092,CoffeeMachine
+103095,Stapler
+103099,Stapler
+103100,Stapler
+103101,CoffeeMachine
+103104,Stapler
+103110,CoffeeMachine
+103111,Stapler
+103113,Stapler
+103118,CoffeeMachine
+103121,CoffeeMachine
+103123,CoffeeMachine
+103124,CoffeeMachine
+103127,CoffeeMachine
+103128,CoffeeMachine
+103129,CoffeeMachine
+103134,CoffeeMachine
+103135,Window
+103137,CoffeeMachine
+103138,CoffeeMachine
+103140,CoffeeMachine
+103143,CoffeeMachine
+103144,CoffeeMachine
+103146,CoffeeMachine
+103148,Window
+103149,Window
+103150,Window
+103152,Fan
+103153,Fan
+103154,Fan
+103171,Fan
+103177,Eyeglasses
+103178,Eyeglasses
+103186,Eyeglasses
+103189,Eyeglasses
+103194,Eyeglasses
+103201,Kettle
+103207,Kettle
+103208,Kettle
+103222,Kettle
+103223,Kettle
+103227,Toilet
+103230,Toilet
+103233,Toilet
+103234,Toilet
+103235,Window
+103236,Window
+103238,Window
+103239,Window
+103242,Window
+103251,Phone
+103252,Phone
+103253,Window
+103255,Window
+103268,Window
+103271,Stapler
+103273,Stapler
+103275,Stapler
+103276,Stapler
+103280,Stapler
+103283,Stapler
+103285,Phone
+103292,Stapler
+103293,Stapler
+103297,Stapler
+103299,Stapler
+103301,Stapler
+103303,Stapler
+103305,Stapler
+103307,Stapler
+103311,Window
+103312,Window
+103315,Window
+103316,Window
+103318,Window
+103320,Window
+103321,Window
+103323,Window
+103325,Window
+103329,Window
+103332,Window
+103333,Window
+103340,Window
+103347,Phone
+103350,Phone
+103352,Dispenser
+103353,Dispenser
+103354,Dispenser
+103355,Dispenser
+103356,Dispenser
+103357,Dispenser
+103358,Dispenser
+103359,Dispenser
+103360,Dispenser
+103361,WashingMachine
+103369,WashingMachine
+103372,Dispenser
+103377,Dispenser
+103379,Dispenser
+103380,Dispenser
+103394,Dispenser
+103397,Dispenser
+1034,Faucet
+103404,Dispenser
+103405,Dispenser
+103407,Dispenser
+103408,Dispenser
+103410,Dispenser
+103412,Dispenser
+103416,Dispenser
+103419,Dispenser
+103422,Dispenser
+103424,Dispenser
+103425,WashingMachine
+103452,WashingMachine
+103469,Toaster
+10347,Refrigerator
+103473,Toaster
+103477,Toaster
+103480,WashingMachine
+103482,Toaster
+103483,Toaster
+103485,Toaster
+103486,Toaster
+103490,WashingMachine
+103498,Toaster
+103502,Toaster
+103503,Lighter
+103508,WashingMachine
+103513,Lighter
+103514,Toaster
+103515,Lighter
+103516,Lighter
+103518,WashingMachine
+103521,WashingMachine
+103528,WashingMachine
+103540,Window
+103545,Toaster
+103547,Toaster
+103548,Toaster
+103549,Toaster
+103553,Toaster
+103555,Toaster
+103556,Toaster
+103558,Toaster
+103559,Toaster
+103560,Toaster
+10357,TrashCan
+103572,Knife
+103575,Knife
+103582,Knife
+103583,Knife
+103585,Knife
+103619,Dispenser
+103635,TrashCan
+103639,TrashCan
+103646,TrashCan
+103647,TrashCan
+103669,Window
+103684,Window
+103699,Phone
+103700,Knife
+103706,Knife
+103713,Knife
+103716,Knife
+103723,Knife
+103725,Knife
+103728,Knife
+103729,Knife
+10373,Refrigerator
+103733,Knife
+103735,Knife
+103739,Knife
+103740,Knife
+103755,Suitcase
+103757,Suitcase
+103761,Suitcase
+103762,Suitcase
+103770,Suitcase
+103775,WashingMachine
+103776,WashingMachine
+103778,WashingMachine
+103781,WashingMachine
+103789,Stapler
+103792,Stapler
+103813,Phone
+103814,Phone
+103828,Phone
+10383,Laptop
+103853,Printer
+103859,Printer
+103863,Printer
+103866,Printer
+103867,Printer
+103869,Printer
+103872,Printer
+103878,Printer
+103886,Phone
+103892,Phone
+103894,Printer
+103916,Phone
+103917,Phone
+103925,Phone
+103927,Phone
+103935,Phone
+103941,Phone
+103963,Globe
+103964,Globe
+103965,Globe
+103967,Globe
+103968,Globe
+103969,Globe
+103971,Globe
+103972,Printer
+103974,Printer
+103978,Printer
+103981,Printer
+103988,Printer
+103989,Printer
+103990,Globe
+103996,Printer
+104000,Printer
+104004,Printer
+104007,Printer
+104009,Printer
+104011,Printer
+104016,Printer
+104020,Printer
+104027,Printer
+104030,Printer
+104036,Remote
+104038,Remote
+104039,Remote
+104040,Remote
+104041,Remote
+104044,Remote
+104045,Remote
+10449,Scissors
+10450,Scissors
+10489,Refrigerator
+10495,Scissors
+10499,Scissors
+10502,Scissors
+1052,Faucet
+1053,Faucet
+10537,Scissors
+10557,Scissors
+10558,Scissors
+10559,Scissors
+10561,Scissors
+10562,Scissors
+10564,Scissors
+10567,Scissors
+10569,Scissors
+10584,TrashCan
+10586,Refrigerator
+10612,Refrigerator
+10620,Refrigerator
+10622,Scissors
+10626,Laptop
+10627,Refrigerator
+10638,Refrigerator
+10655,Refrigerator
+10685,Refrigerator
+10686,Scissors
+10697,Laptop
+10751,Refrigerator
+10797,Refrigerator
+10844,Scissors
+10849,Refrigerator
+10867,Refrigerator
+10885,Laptop
+10889,Scissors
+10893,Scissors
+10894,Scissors
+10895,Scissors
+10900,Refrigerator
+10902,Scissors
+10905,Refrigerator
+10907,Scissors
+10915,Laptop
+10944,Refrigerator
+10960,Scissors
+10962,Scissors
+10968,Scissors
+10973,Scissors
+10975,Scissors
+11013,Scissors
+11020,Scissors
+11021,Scissors
+11026,Scissors
+11028,Scissors
+11029,Scissors
+11036,Scissors
+11040,Scissors
+11047,Scissors
+11052,Scissors
+11075,Laptop
+11077,Scissors
+11080,Scissors
+11089,Scissors
+11099,Scissors
+11100,Scissors
+11103,Scissors
+11111,Scissors
+11113,Scissors
+11124,TrashCan
+11156,Laptop
+11178,Refrigerator
+11211,Refrigerator
+11229,TrashCan
+11231,Refrigerator
+11242,Laptop
+11248,Laptop
+11259,TrashCan
+11279,TrashCan
+11299,Refrigerator
+11304,Refrigerator
+11361,TrashCan
+11395,Laptop
+11405,Laptop
+11406,Laptop
+11429,Laptop
+11477,Laptop
+11550,Refrigerator
+11581,Laptop
+11586,Laptop
+11622,Dishwasher
+11661,Dishwasher
+11691,Laptop
+11700,Dishwasher
+11709,Refrigerator
+11712,Refrigerator
+11778,Laptop
+11818,TrashCan
+11826,Dishwasher
+11846,Refrigerator
+11876,Laptop
+11888,Laptop
+11945,Laptop
+11951,TrashCan
+12036,Refrigerator
+12038,Refrigerator
+12042,Refrigerator
+12043,Refrigerator
+12050,Refrigerator
+12054,Refrigerator
+12055,Refrigerator
+12059,Refrigerator
+12065,Dishwasher
+12066,Refrigerator
+12073,Laptop
+12085,Dishwasher
+12092,Dishwasher
+12231,TrashCan
+12248,Refrigerator
+12249,Refrigerator
+12250,Refrigerator
+12252,Refrigerator
+12259,Dishwasher
+12289,TrashCan
+12349,Dishwasher
+12414,Dishwasher
+12428,Dishwasher
+12445,TrashCan
+12447,TrashCan
+12477,TrashCan
+12480,Dishwasher
+12483,TrashCan
+12484,Dishwasher
+12530,Dishwasher
+12531,Dishwasher
+12536,Dishwasher
+12540,Dishwasher
+12543,Dishwasher
+12552,Dishwasher
+12553,Dishwasher
+12558,Dishwasher
+12559,Dishwasher
+12560,Dishwasher
+12561,Dishwasher
+12562,Dishwasher
+12563,Dishwasher
+12565,Dishwasher
+12579,Dishwasher
+12580,Dishwasher
+12583,Dishwasher
+12587,Dishwasher
+12590,Dishwasher
+12592,Dishwasher
+12594,Dishwasher
+12596,Dishwasher
+12597,Dishwasher
+12605,Dishwasher
+12606,Dishwasher
+12614,Dishwasher
+12617,Dishwasher
+12621,Dishwasher
+12654,Dishwasher
+12727,Keyboard
+12738,Keyboard
+1280,Faucet
+12829,Keyboard
+12836,Keyboard
+12838,Keyboard
+1288,Faucet
+12880,Keyboard
+12886,Keyboard
+12902,Keyboard
+12917,Keyboard
+12953,Keyboard
+12956,Keyboard
+12965,Keyboard
+12968,Keyboard
+12977,Keyboard
+12996,Keyboard
+13023,Keyboard
+13024,Keyboard
+13062,Keyboard
+13064,Keyboard
+13075,Keyboard
+13086,Keyboard
+13095,Keyboard
+13100,Keyboard
+13106,Keyboard
+13120,Keyboard
+13153,Keyboard
+13310,Lamp
+1343,Faucet
+13491,Lamp
+1370,Faucet
+1380,Faucet
+1386,Faucet
+13865,Lamp
+1401,Faucet
+14127,Lamp
+14205,Lamp
+1427,Faucet
+14306,Lamp
+1435,Faucet
+14351,Lamp
+14372,Lamp
+14379,Lamp
+14402,Lamp
+14422,Lamp
+1444,Faucet
+14563,Lamp
+14567,Lamp
+14605,Lamp
+1466,Faucet
+1479,Faucet
+148,Faucet
+1488,Faucet
+149,Faucet
+14905,Lamp
+1492,Faucet
+14962,Lamp
+14977,Lamp
+14986,Lamp
+15028,Lamp
+152,Faucet
+1528,Faucet
+153,Faucet
+15335,Lamp
+154,Faucet
+15423,Lamp
+15425,Lamp
+1556,Faucet
+15574,Lamp
+156,Faucet
+15729,Lamp
+15879,Lamp
+15913,Lamp
+1596,Faucet
+16012,Lamp
+16047,Lamp
+16208,Lamp
+16237,Lamp
+1626,Faucet
+1633,Faucet
+16434,Lamp
+1646,Faucet
+1653,Faucet
+16547,Lamp
+1667,Faucet
+1668,Faucet
+167,Faucet
+16794,Lamp
+168,Faucet
+16832,Lamp
+1712,Faucet
+1721,Faucet
+1741,Faucet
+1785,Faucet
+1788,Faucet
+179,Chair
+1794,Faucet
+1795,Faucet
+1802,Faucet
+1817,Faucet
+1823,Faucet
+1832,Faucet
+1885,Faucet
+1886,Faucet
+1896,Faucet
+1901,Faucet
+1903,Faucet
+19179,Table
+19203,Table
+1925,Faucet
+1931,Faucet
+1935,Faucet
+19384,Table
+1941,Faucet
+1961,Faucet
+19740,Table
+19825,Table
+19836,Table
+19855,Table
+1986,Faucet
+19898,Table
+20043,Table
+20077,Table
+2017,Faucet
+20279,Table
+2035,Faucet
+20411,Table
+20453,Table
+2054,Faucet
+20555,Table
+20745,Table
+2082,Faucet
+2083,Faucet
+2084,Faucet
+2095,Faucet
+20985,Table
+2108,Faucet
+2113,Faucet
+2115,Faucet
+2140,Faucet
+21467,Table
+21473,Table
+2170,Faucet
+21718,Table
+22241,Table
+2230,Chair
+22301,Table
+22339,Table
+22367,Table
+22433,Table
+22508,Table
+22692,Table
+22870,Table
+2320,Chair
+23372,Table
+23472,Table
+23511,Table
+2364,Chair
+23650,Table
+23782,Table
+23807,Table
+24152,Table
+2440,Chair
+24644,Table
+2471,Chair
+2490,Chair
+24931,Table
+25160,Table
+25308,Table
+25493,Table
+2554,Chair
+25756,Table
+25913,Table
+25959,Table
+26073,Table
+2627,Chair
+26387,Table
+26503,Table
+26525,Table
+26545,Table
+26608,Table
+26652,Table
+26657,Table
+26670,Table
+26692,Table
+26800,Table
+26806,Table
+26875,Table
+26886,Table
+26899,Table
+27044,Table
+27189,Table
+27267,Table
+27478,Table
+27619,Table
+28164,Table
+28594,Table
+28668,Table
+29133,Table
+29557,Table
+29921,Table
+30238,Table
+3028,Chair
+30341,Table
+30663,Table
+30666,Table
+30739,Table
+30869,Table
+3091,Chair
+31249,Table
+3140,Chair
+3143,Chair
+31601,Table
+3171,Chair
+3193,Chair
+32052,Table
+32086,Table
+32174,Table
+32259,Table
+32324,Table
+32354,Table
+32566,Table
+32601,Table
+32625,Table
+32761,Table
+32932,Table
+33116,Table
+33457,Table
+33810,Table
+3386,Display
+33914,Table
+3392,Display
+3393,Display
+33930,Table
+3395,Display
+3398,Bottle
+34178,Table
+34610,Table
+34617,Table
+35059,StorageFurniture
+35063,Chair
+3517,Bottle
+3519,Bottle
+3520,Bottle
+3558,Bottle
+3571,Bottle
+3574,Bottle
+3596,Bottle
+3614,Bottle
+3615,Bottle
+3616,Bottle
+3618,Bottle
+3625,Bottle
+36250,Chair
+36280,Chair
+3635,Bottle
+36409,Chair
+3655,Bottle
+36761,Chair
+3678,Bottle
+36845,Chair
+36912,Chair
+36983,Chair
+37099,Chair
+37247,Chair
+37351,Chair
+3763,Bottle
+37825,Chair
+37834,Chair
+37954,Chair
+3822,Bottle
+38287,Chair
+3830,Bottle
+38325,Chair
+38357,Chair
+38368,Chair
+38486,Chair
+38510,Chair
+38516,StorageFurniture
+38519,Chair
+3854,Bottle
+3868,Bottle
+38803,Chair
+38841,Chair
+38994,Chair
+39015,Chair
+39047,Chair
+39313,Chair
+3933,Bottle
+3934,Bottle
+39392,Chair
+39432,Chair
+3944,Bottle
+39485,Chair
+39551,Chair
+39628,Chair
+3971,Kettle
+3990,Bottle
+39988,Chair
+40067,Chair
+40069,Chair
+40147,StorageFurniture
+40168,Chair
+40225,Chair
+40417,StorageFurniture
+4043,Bottle
+40453,StorageFurniture
+4064,Bottle
+4084,Bottle
+40890,Chair
+4094,Display
+40982,Chair
+41003,StorageFurniture
+41004,StorageFurniture
+41045,Chair
+4108,TrashCan
+41083,StorageFurniture
+41085,StorageFurniture
+41086,StorageFurniture
+41153,Chair
+4118,Bottle
+41434,Chair
+41438,Chair
+41452,StorageFurniture
+41510,StorageFurniture
+41529,StorageFurniture
+41609,Chair
+41610,Chair
+41653,Chair
+41667,Chair
+41838,Chair
+4200,Bottle
+42001,Chair
+4204,Bottle
+42057,Chair
+4216,Bottle
+4233,Bottle
+42378,Chair
+42452,Chair
+42556,Chair
+42647,Chair
+42856,Chair
+43074,Chair
+4314,Bottle
+4393,Bottle
+4403,Bottle
+4427,Bottle
+44441,Chair
+44445,Chair
+44729,Chair
+44781,StorageFurniture
+44817,StorageFurniture
+44826,StorageFurniture
+44853,StorageFurniture
+44876,Chair
+44961,Chair
+44962,StorageFurniture
+4500,Bottle
+45001,StorageFurniture
+45007,StorageFurniture
+45087,StorageFurniture
+45091,StorageFurniture
+45092,StorageFurniture
+45130,StorageFurniture
+45132,StorageFurniture
+45134,StorageFurniture
+45135,StorageFurniture
+4514,Bottle
+45146,StorageFurniture
+45159,StorageFurniture
+45162,StorageFurniture
+45164,StorageFurniture
+45166,StorageFurniture
+45168,StorageFurniture
+45173,StorageFurniture
+45176,StorageFurniture
+45177,StorageFurniture
+45178,StorageFurniture
+45189,StorageFurniture
+45194,StorageFurniture
+45203,StorageFurniture
+45212,StorageFurniture
+45213,StorageFurniture
+45219,StorageFurniture
+45235,StorageFurniture
+45238,StorageFurniture
+45243,StorageFurniture
+45244,StorageFurniture
+45247,StorageFurniture
+45248,StorageFurniture
+45249,StorageFurniture
+45261,StorageFurniture
+45262,StorageFurniture
+45267,StorageFurniture
+45271,StorageFurniture
+4529,Display
+45290,StorageFurniture
+45297,StorageFurniture
+4530,Display
+45305,StorageFurniture
+45323,StorageFurniture
+4533,Display
+45332,StorageFurniture
+45354,StorageFurniture
+45372,StorageFurniture
+45374,StorageFurniture
+45378,StorageFurniture
+45384,StorageFurniture
+45385,StorageFurniture
+45387,StorageFurniture
+45397,StorageFurniture
+45403,StorageFurniture
+4541,Display
+45413,StorageFurniture
+45415,StorageFurniture
+45419,StorageFurniture
+4542,Display
+45420,StorageFurniture
+45423,StorageFurniture
+45427,StorageFurniture
+45443,StorageFurniture
+45444,StorageFurniture
+45448,StorageFurniture
+45463,StorageFurniture
+45503,StorageFurniture
+45504,StorageFurniture
+45505,StorageFurniture
+45516,StorageFurniture
+4552,Display
+45523,StorageFurniture
+45524,StorageFurniture
+45526,StorageFurniture
+4555,Display
+45573,StorageFurniture
+45575,StorageFurniture
+45594,StorageFurniture
+45600,StorageFurniture
+45606,StorageFurniture
+45612,StorageFurniture
+4562,Display
+45620,StorageFurniture
+45621,StorageFurniture
+45622,StorageFurniture
+45623,StorageFurniture
+4563,Display
+45632,StorageFurniture
+45633,StorageFurniture
+45636,StorageFurniture
+45638,StorageFurniture
+4564,Display
+45642,StorageFurniture
+45645,StorageFurniture
+4566,Display
+45661,StorageFurniture
+45662,StorageFurniture
+45667,StorageFurniture
+45670,StorageFurniture
+45671,StorageFurniture
+45676,StorageFurniture
+45677,StorageFurniture
+45687,StorageFurniture
+45689,StorageFurniture
+45690,StorageFurniture
+45691,StorageFurniture
+45693,StorageFurniture
+45694,StorageFurniture
+45696,StorageFurniture
+45699,StorageFurniture
+4571,Display
+45710,StorageFurniture
+45717,StorageFurniture
+45725,StorageFurniture
+4574,Display
+45746,StorageFurniture
+45747,StorageFurniture
+45749,StorageFurniture
+45756,StorageFurniture
+45759,StorageFurniture
+4576,Display
+45767,StorageFurniture
+45776,StorageFurniture
+45779,StorageFurniture
+4578,Display
+45780,StorageFurniture
+45783,StorageFurniture
+45784,StorageFurniture
+45790,StorageFurniture
+45801,StorageFurniture
+45822,StorageFurniture
+45841,StorageFurniture
+45850,StorageFurniture
+45853,StorageFurniture
+45855,StorageFurniture
+4586,Display
+4589,Display
+4590,Display
+45908,StorageFurniture
+45910,StorageFurniture
+45915,StorageFurniture
+45916,StorageFurniture
+4592,Display
+45922,StorageFurniture
+45936,StorageFurniture
+45937,StorageFurniture
+4594,Display
+45940,StorageFurniture
+45948,StorageFurniture
+45949,StorageFurniture
+45950,StorageFurniture
+45961,StorageFurniture
+45963,StorageFurniture
+45964,StorageFurniture
+45984,StorageFurniture
+46002,StorageFurniture
+46014,StorageFurniture
+46019,StorageFurniture
+46029,StorageFurniture
+46033,StorageFurniture
+46037,StorageFurniture
+46044,StorageFurniture
+46045,StorageFurniture
+46057,StorageFurniture
+46060,StorageFurniture
+4608,Display
+46084,StorageFurniture
+46092,StorageFurniture
+46107,StorageFurniture
+46108,StorageFurniture
+46109,StorageFurniture
+46117,StorageFurniture
+46120,StorageFurniture
+46123,StorageFurniture
+46127,StorageFurniture
+46130,StorageFurniture
+46132,StorageFurniture
+46134,StorageFurniture
+46145,StorageFurniture
+46166,StorageFurniture
+46172,StorageFurniture
+46179,StorageFurniture
+46180,StorageFurniture
+46197,StorageFurniture
+46199,StorageFurniture
+46230,StorageFurniture
+46236,StorageFurniture
+4627,Display
+46277,StorageFurniture
+4628,Display
+4633,Display
+46334,StorageFurniture
+46380,StorageFurniture
+46401,StorageFurniture
+46403,StorageFurniture
+46408,StorageFurniture
+46417,StorageFurniture
+46427,StorageFurniture
+46430,StorageFurniture
+46437,StorageFurniture
+46439,StorageFurniture
+46440,StorageFurniture
+46443,StorageFurniture
+46452,StorageFurniture
+46456,StorageFurniture
+46462,StorageFurniture
+46466,StorageFurniture
+46480,StorageFurniture
+46481,StorageFurniture
+46490,StorageFurniture
+46537,StorageFurniture
+46544,StorageFurniture
+46549,StorageFurniture
+46556,StorageFurniture
+46563,StorageFurniture
+46598,StorageFurniture
+46616,StorageFurniture
+46641,StorageFurniture
+46653,StorageFurniture
+46655,StorageFurniture
+46699,StorageFurniture
+46700,StorageFurniture
+46732,StorageFurniture
+46741,StorageFurniture
+46744,StorageFurniture
+46762,StorageFurniture
+46768,StorageFurniture
+46787,StorageFurniture
+46801,StorageFurniture
+4681,Display
+46825,StorageFurniture
+46839,StorageFurniture
+46847,StorageFurniture
+46856,StorageFurniture
+46859,StorageFurniture
+46874,StorageFurniture
+46879,StorageFurniture
+46889,StorageFurniture
+46893,StorageFurniture
+46896,StorageFurniture
+46906,StorageFurniture
+46922,StorageFurniture
+46944,StorageFurniture
+46955,StorageFurniture
+46966,StorageFurniture
+46981,StorageFurniture
+47021,StorageFurniture
+47024,StorageFurniture
+47088,StorageFurniture
+47089,StorageFurniture
+47099,StorageFurniture
+47133,StorageFurniture
+47168,StorageFurniture
+47178,StorageFurniture
+47180,StorageFurniture
+47182,StorageFurniture
+47183,StorageFurniture
+47185,StorageFurniture
+47187,StorageFurniture
+47207,StorageFurniture
+47227,StorageFurniture
+47233,StorageFurniture
+47235,StorageFurniture
+47238,StorageFurniture
+47252,StorageFurniture
+47254,StorageFurniture
+47278,StorageFurniture
+47281,StorageFurniture
+47290,StorageFurniture
+47296,StorageFurniture
+47315,StorageFurniture
+47316,StorageFurniture
+47388,StorageFurniture
+47391,StorageFurniture
+47419,StorageFurniture
+47438,StorageFurniture
+47466,StorageFurniture
+47514,StorageFurniture
+47529,StorageFurniture
+47565,StorageFurniture
+47570,StorageFurniture
+47577,StorageFurniture
+47578,StorageFurniture
+47585,StorageFurniture
+47595,StorageFurniture
+47601,StorageFurniture
+47613,StorageFurniture
+47632,StorageFurniture
+47645,Box
+47648,StorageFurniture
+47651,StorageFurniture
+47669,StorageFurniture
+47686,StorageFurniture
+47701,StorageFurniture
+47711,StorageFurniture
+47729,StorageFurniture
+47742,StorageFurniture
+47747,StorageFurniture
+47808,StorageFurniture
+47817,StorageFurniture
+47853,StorageFurniture
+47926,StorageFurniture
+47944,StorageFurniture
+47954,StorageFurniture
+47963,StorageFurniture
+47976,StorageFurniture
+48010,StorageFurniture
+48013,StorageFurniture
+48018,StorageFurniture
+48023,StorageFurniture
+48036,StorageFurniture
+48051,StorageFurniture
+48063,StorageFurniture
+48167,StorageFurniture
+48169,StorageFurniture
+48177,StorageFurniture
+48243,StorageFurniture
+48253,StorageFurniture
+48258,StorageFurniture
+48263,StorageFurniture
+48271,StorageFurniture
+48356,StorageFurniture
+48379,StorageFurniture
+48381,StorageFurniture
+48413,StorageFurniture
+48452,StorageFurniture
+48467,StorageFurniture
+48479,StorageFurniture
+48490,StorageFurniture
+48491,StorageFurniture
+48492,Box
+48497,StorageFurniture
+48513,StorageFurniture
+48517,StorageFurniture
+48519,StorageFurniture
+4853,Display
+48623,StorageFurniture
+48686,StorageFurniture
+48700,StorageFurniture
+48721,StorageFurniture
+48740,StorageFurniture
+48746,StorageFurniture
+48797,StorageFurniture
+48855,StorageFurniture
+48859,StorageFurniture
+48876,StorageFurniture
+48878,StorageFurniture
+49025,StorageFurniture
+49038,StorageFurniture
+49042,StorageFurniture
+49062,StorageFurniture
+49132,StorageFurniture
+49133,StorageFurniture
+49140,StorageFurniture
+49182,StorageFurniture
+49188,StorageFurniture
+5050,Display
+5088,Display
+5103,Display
+5306,Display
+5452,Display
+5477,Display
+5601,Bottle
+5688,Bottle
+5696,Clock
+5850,Bottle
+5861,Bottle
+5902,Bottle
+6037,Bottle
+6040,Bottle
+6209,Bottle
+6222,Bottle
+6263,Bottle
+6335,Bottle
+6430,Bottle
+6493,Bottle
+6500,Clock
+6568,Clock
+6608,Clock
+6613,Clock
+6638,Clock
+6641,Clock
+6643,Clock
+6665,Clock
+6728,Clock
+6771,Bottle
+6797,Clock
+6808,Clock
+6813,Clock
+6839,Clock
+6843,Clock
+6917,Clock
+693,Faucet
+6934,Clock
+6953,Clock
+6963,Clock
+7004,Clock
+7007,Clock
+7015,Clock
+7032,Clock
+7037,Clock
+7054,Clock
+7064,Clock
+7068,Clock
+7074,Clock
+7078,Clock
+7104,Clock
+7111,Clock
+7119,Microwave
+7120,Oven
+7128,Microwave
+7167,Microwave
+7179,Oven
+7187,Oven
+7201,Oven
+7220,Oven
+723,Chair
+7236,Microwave
+7263,Microwave
+7273,Microwave
+7290,Oven
+7292,Microwave
+7296,Microwave
+7304,Microwave
+7310,Microwave
+7332,Oven
+7349,Microwave
+7366,Microwave
+7619,Keyboard
+762,Chair
+811,Faucet
+822,Faucet
+857,Faucet
+862,Faucet
+866,Faucet
+8736,Bottle
+8848,Bottle
+885,Faucet
+8867,Door
+8877,Door
+8893,Door
+8897,Door
+8903,Door
+8919,Door
+8930,Door
+8936,Door
+8961,Door
+8983,Door
+8994,Door
+8997,Door
+9003,Door
+9016,Door
+9032,Door
+9035,Door
+9041,Door
+9065,Door
+9070,Door
+908,Faucet
+9107,Door
+9117,Door
+912,Faucet
+9127,Door
+9128,Door
+9148,Door
+9164,Door
+9168,Door
+920,Faucet
+9263,Door
+9277,Door
+9280,Door
+9281,Door
+9288,Door
+929,Faucet
+931,Faucet
+9386,Door
+9388,Door
+9393,Door
+9410,Door
+960,Faucet
+9748,Laptop
+991,Faucet
+9960,Laptop
+9968,Laptop
+9992,Laptop
+9996,Laptop
diff --git a/pyproject.toml b/pyproject.toml
new file mode 100644
index 0000000..84c69b4
--- /dev/null
+++ b/pyproject.toml
@@ -0,0 +1,61 @@
+[project]
+name = "flowbothd"
+version = "0.1.0"
+description = "FlowBotHD: History-Aware Diffusion Handling Ambiguities in Articulated Objects Manipulation"
+readme = "README.md"
+requires-python = ">=3.6"
+license = { file = "LICENSE.txt" }
+authors = [{ email = "yishul@andrew.cmu.edu", name = "Yishu Li" }]
+dependencies = [
+  "hydra-core == 1.3.2",
+  "lightning == 2.0.3",
+  "omegaconf == 2.3.0",
+  "pandas",
+]
+
+[build-system]
+requires = ["setuptools >= 58.0.1", "setuptools-scm", "wheel"]
+build-backend = "setuptools.build_meta"
+
+[project.optional-dependencies]
+develop = [
+  "autoflake == 2.1.1",
+  "black == 23.3.0",
+  "isort == 5.12.0",
+  "mypy == 1.3.0",
+  "pandas-stubs == 2.0.1.230501",
+  "pylint == 2.17.4",
+  "pytest == 7.3.2",
+  "pre-commit == 3.3.3",
+]
+notebooks = ["jupyter"]
+build_docs = ["mkdocs-material", "mkdocstrings[python]"]
+
+# This is required to allow us to have notebooks/ at the top level.
+[tool.setuptools.packages.find]
+where = ["src"]
+
+[tool.setuptools.package-data]
+flowbothd = ["py.typed"]
+
+[tool.isort]
+profile = "black"
+known_third_party = "wandb"
+
+[tool.mypy]
+python_version = 3.9
+warn_return_any = true
+warn_unused_configs = true
+mypy_path = "src"
+namespace_packages = true
+explicit_package_bases = true
+
+[[tool.mypy.overrides]]
+module = ["torchvision.*"]
+ignore_missing_imports = true
+
+[tool.pylint]
+known-third-party = "wandb"
+
+[tool.pylint.TYPECHECK]
+generated-members = 'torch.*'
diff --git a/scripts/README.md b/scripts/README.md
new file mode 100644
index 0000000..deed6b0
--- /dev/null
+++ b/scripts/README.md
@@ -0,0 +1,3 @@
+# Scripts for your porject
+
+If you write some scripts which are meant to be run stand-alone, and not imported as part of the library, put them in this directory.
diff --git a/scripts/eval.py b/scripts/eval.py
new file mode 100644
index 0000000..6516733
--- /dev/null
+++ b/scripts/eval.py
@@ -0,0 +1,392 @@
+from pathlib import Path
+
+import hydra
+import lightning as L
+import omegaconf
+import pandas as pd
+import rpad.partnet_mobility_utils.dataset as rpd
+import rpad.pyg.nets.pointnet2 as pnp
+import torch
+import tqdm
+import wandb
+
+from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule
+from flowbothd.datasets.flowbot import FlowBotDataModule
+from flowbothd.metrics.trajectory import artflownet_loss, flow_metrics
+from flowbothd.models.flow_predictor import FlowPredictorInferenceModule
+from flowbothd.models.flow_trajectory_predictor import (
+    FlowTrajectoryInferenceModule,
+    flow_metrics,
+)
+from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn
+
+data_module_class = {
+    "flowbot": FlowBotDataModule,
+    "trajectory": FlowTrajectoryDataModule,
+}
+
+inference_module_class = {
+    "flowbot": FlowPredictorInferenceModule,
+    "trajectory": FlowTrajectoryInferenceModule,
+}
+
+
+@torch.no_grad()
+@hydra.main(config_path="../configs", config_name="eval", version_base="1.3")
+def main(cfg):
+    ######################################################################
+    # Torch settings.
+    ######################################################################
+
+    # Make deterministic + reproducible.
+    torch.backends.cudnn.deterministic = True
+    torch.backends.cudnn.benchmark = False
+
+    # Since most of us are training on 3090s+, we can use mixed precision.
+    torch.set_float32_matmul_precision("medium")
+
+    # Global seed for reproducibility.
+    L.seed_everything(42)
+
+    ######################################################################
+    # Create the datamodule.
+    # Should be the same one as in training, but we're gonna use val+test
+    # dataloaders.
+    ######################################################################
+    trajectory_len = 1
+    if cfg.dataset.dataset_type == "full-dataset":
+        # Full dataset
+        toy_dataset = None
+    else:
+        # Door dataset
+        toy_dataset = {
+            "id": "door-full-new-noslide",
+            "train-train": [
+                "8877",
+                "8893",
+                "8897",
+                "8903",
+                "8919",
+                "8930",
+                "8961",
+                "8997",
+                "9016",
+                # "9032",   # has slide
+                "9035",
+                "9041",
+                "9065",
+                "9070",
+                "9107",
+                "9117",
+                "9127",
+                "9128",
+                "9148",
+                "9164",
+                "9168",
+                "9277",
+                "9280",
+                "9281",
+                "9288",
+                "9386",
+                "9388",
+                "9410",
+            ],
+            "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+            "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+        }
+
+    # Create FlowBot dataset
+    fully_closed_datamodule = FlowTrajectoryDataModule(
+        root="/home/yishu/datasets/partnet-mobility",
+        batch_size=1,
+        num_workers=30,
+        n_proc=2,
+        seed=42,
+        trajectory_len=1,  # Only used when training trajectory model
+        special_req="fully-closed",
+        toy_dataset=toy_dataset,
+    )
+    randomly_opened_datamodule = FlowTrajectoryDataModule(
+        root="/home/yishu/datasets/partnet-mobility",
+        batch_size=1,
+        num_workers=30,
+        n_proc=2,
+        seed=42,
+        trajectory_len=1,  # Only used when training trajectory model
+        special_req=None,
+        toy_dataset=toy_dataset,
+    )
+
+    ######################################################################
+    # Set up logging in WandB.
+    # This is a different job type (eval), but we want it all grouped
+    # together. Notice that we use our own logging here (not lightning).
+    ######################################################################
+
+    # Create a run.
+    run = wandb.init(
+        entity=cfg.wandb.entity,
+        project=cfg.wandb.project,
+        dir=cfg.wandb.save_dir,
+        config=omegaconf.OmegaConf.to_container(
+            cfg, resolve=True, throw_on_missing=True
+        ),
+        job_type=cfg.job_type,
+        save_code=True,  # This just has the main script.
+        group=cfg.wandb.group,
+    )
+
+    # Log the code.
+    wandb.run.log_code(
+        root=PROJECT_ROOT,
+        include_fn=match_fn(
+            dirs=["configs", "scripts", "src"],
+            extensions=[".py", ".yaml"],
+        ),
+    )
+
+    ######################################################################
+    # Create the network(s) which will be evaluated (same as training).
+    # You might want to put this into a "create_network" function
+    # somewhere so train and eval can be the same.
+    #
+    # We'll also load the weights.
+    ######################################################################
+
+    mask_channel = 1 if cfg.inference.mask_input_channel else 0
+    network = pnp.PN2Dense(
+        in_channels=mask_channel,
+        out_channels=3 * trajectory_len,
+        p=pnp.PN2DenseParams(),
+    )
+
+    # Get the checkpoint file. If it's a wandb reference, download.
+    # Otherwise look to disk.
+    checkpoint_reference = cfg.checkpoint.reference
+    if checkpoint_reference.startswith(cfg.wandb.entity):
+        # download checkpoint locally (if not already cached)
+        artifact_dir = cfg.wandb.artifact_dir
+        artifact = run.use_artifact(checkpoint_reference, type="model")
+        ckpt_file = artifact.get_path("model.ckpt").download(root=artifact_dir)
+    else:
+        ckpt_file = checkpoint_reference
+
+    # Load the network weights.
+    ckpt = torch.load(ckpt_file)
+    network.load_state_dict(
+        {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
+    )
+
+    ######################################################################
+    # Create an inference module, which is basically just a bare-bones
+    # class which runs the model. In this example, we only implement
+    # the "predict_step" function, which may not be the blessed
+    # way to do it vis a vis lightning, but whatever.
+    #
+    # If this is a downstream application or something, you might
+    # want to implement a different interface (like with a "predict"
+    # function), so you can pass in un-batched observations from an
+    # environment, for instance.
+    ######################################################################
+
+    model = inference_module_class[cfg.dataset.name](
+        network, inference_cfg=cfg.inference
+    )
+
+    ######################################################################
+    # Create the trainer.
+    # Bit of a misnomer here, we're not doing training. But we are gonna
+    # use it to set up the model appropriately and do all the batching
+    # etc.
+    #
+    # If this is a different kind of downstream eval, chuck this block.
+    ######################################################################
+
+    trainer = L.Trainer(
+        accelerator="gpu",
+        devices=cfg.resources.gpus,
+        precision="16-mixed",
+        logger=False,
+    )
+
+    ######################################################################
+    # Run the model on the train/val/test sets.
+    # This outputs a list of dictionaries, one for each batch. This
+    # is annoying to work with, so later we'll flatten.
+    #
+    # If a downstream eval, you can swap it out with whatever the eval
+    # function is.
+    ######################################################################
+
+    dataloaders = [
+        # (fully_closed_datamodule.val_dataloader(), "val"),
+        # (randomly_opened_datamodule.unseen_dataloader(), "test"),
+        # # (datamodule.train_val_dataloader(), "train"),   # TODO: FOR DOOR dataset
+        # # Fullset closed
+        # (fully_closed_datamodule.val_dataloader(bsz=1), "val_closed"),
+        # (fully_closed_datamodule.unseen_dataloader(bsz=1), "test_closed"),
+        # # Fullset open
+        # (randomly_opened_datamodule.val_dataloader(bsz=1), "val_open"),
+        # (randomly_opened_datamodule.unseen_dataloader(bsz=1), "test_open"),
+        # Train set
+        (fully_closed_datamodule.train_val_dataloader(bsz=1), "train_closed"),
+        (randomly_opened_datamodule.train_val_dataloader(bsz=1), "train_opened"),
+    ]
+
+    all_objs = (
+        rpd.UMPNET_TRAIN_TRAIN_OBJS + rpd.UMPNET_TRAIN_TEST_OBJS + rpd.UMPNET_TEST_OBJS
+    )
+    id_to_obj_class = {obj_id: obj_class for obj_id, obj_class in all_objs}
+
+    all_directions = []
+    all_metrics = {
+        "flow_loss": [],
+        "rmse": [],
+        "cos_dist": [],
+        "mag_error": [],
+    }
+
+    for loader, name in dataloaders:
+        metrics = []
+        outputs = trainer.predict(
+            model,
+            dataloaders=[loader],
+        )
+
+        for batch, preds in zip(tqdm.tqdm(loader), outputs):
+            st = 0
+            for data in batch.to_data_list():
+                f_pred = preds[st : st + data.num_nodes]
+                f_pred = f_pred.reshape(f_pred.shape[0], -1, 3)
+                f_ix = data.mask.bool()
+                if torch.sum(f_ix) == 0:
+                    continue
+                if cfg.dataset.name == "trajectory":
+                    f_target = data.delta
+                else:
+                    f_target = data.flow
+                    f_target = f_target.reshape(f_target.shape[0], -1, 3)
+
+                n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(f_pred.device)  # type: ignore
+                flow_loss = artflownet_loss(f_pred, f_target, n_nodes)
+                rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+
+                all_metrics["flow_loss"].append(flow_loss)
+                all_metrics["rmse"].append(rmse)
+                all_metrics["cos_dist"].append(cos_dist)
+                all_metrics["mag_error"].append(mag_error)
+
+                all_directions.append(cos_dist)
+
+                metrics.append(
+                    {
+                        "id": data.id,
+                        # "obj_class": id_to_obj_class[data.id],  # TODO: FOR DOOR dataset
+                        "obj_class": "door",
+                        "metrics": {
+                            "rmse": rmse.cpu().item(),
+                            "cos_dist": cos_dist.cpu().item(),
+                            "mag_error": mag_error.cpu().item(),
+                            "pos@0.7": int(cos_dist.cpu().item() > 0.7),
+                            "neg@-0.7": int(cos_dist.cpu().item() < -0.7),
+                        },
+                    }
+                )
+
+                st += data.num_nodes
+
+        rows = [
+            (
+                m["id"],
+                m["obj_class"],
+                m["metrics"]["rmse"],
+                m["metrics"]["cos_dist"],
+                m["metrics"]["mag_error"],
+                m["metrics"]["pos@0.7"],
+                m["metrics"]["neg@-0.7"],
+            )
+            for m in metrics
+        ]
+        raw_df = pd.DataFrame(
+            rows,
+            columns=[
+                "id",
+                "category",
+                "rmse",
+                "cos_dist",
+                "mag_error",
+                "pos@0.7",
+                "neg@-0.7",
+            ],
+        )
+        df = raw_df.groupby("category").mean(numeric_only=True)
+        df.loc["unweighted_mean"] = raw_df.mean(numeric_only=True)
+        df.loc["class_mean"] = df.mean()
+
+        out_file = Path(cfg.log_dir) / f"{cfg.dataset.name}_{trajectory_len}_{name}.csv"
+        print(out_file)
+        # if out_file.exists():
+        #     raise ValueError(f"{out_file} already exists...")
+        df.to_csv(out_file, float_format="%.4f")
+
+        # Log the metrics + table to wandb.
+        table = wandb.Table(dataframe=df.reset_index())
+        run.log({f"{name}_metric_table": table})
+
+    # # Scatter plot
+    # ys = [d.item() for d in all_directions]
+    # xs = [
+    #     "8877",
+    #     "8893",
+    #     "8897",
+    #     "8903",
+    #     "8919",
+    #     "8930",
+    #     "8961",
+    #     "8997",
+    #     "9016",
+    #     "9032",
+    #     "9035",
+    #     "9041",
+    #     "9065",
+    #     "9070",
+    #     "9107",
+    #     "9117",
+    #     "9127",
+    #     "9128",
+    #     "9148",
+    #     "9164",
+    #     "9168",
+    #     "9277",
+    #     "9280",
+    #     "9281",
+    #     "9288",
+    #     "9386",
+    #     "9388",
+    #     "9410",
+    #     "8867",
+    #     "8983",
+    #     "8994",
+    #     "9003",
+    #     "9263",
+    #     "9393",
+    # ]
+    # breakpoint()
+    # colors = sorted(["red"]) * (len(xs) - 6) + ["blue"] * 6
+    # import matplotlib.pyplot as plt
+
+    # fig = plt.figure()
+    # ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])
+    # ax.axhline(y=0)
+    # plt.scatter(xs, ys, s=5, c=colors[: len(ys)])
+    # plt.xticks(rotation=90)
+
+    # plt.savefig("./half_cos_stats.jpeg")
+
+    # All metrics
+    for name in all_metrics.keys():
+        print(f"{name}: {sum(all_metrics[name]) / len(all_metrics[name])}")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/eval_history_diffuser_wta.py b/scripts/eval_history_diffuser_wta.py
new file mode 100644
index 0000000..e1f1e6e
--- /dev/null
+++ b/scripts/eval_history_diffuser_wta.py
@@ -0,0 +1,356 @@
+# Diffuser evaluation scripts
+
+import hydra
+import lightning as L
+import omegaconf
+import pandas as pd
+import torch
+import wandb
+
+from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule
+from flowbothd.models.flow_diffuser_dit import (
+    FlowTrajectoryDiffuserInferenceModule_DiT,
+)
+from flowbothd.models.flow_diffuser_hispndit import (
+    FlowTrajectoryDiffuserInferenceModule_HisPNDiT,
+)
+from flowbothd.models.flow_diffuser_hisdit import (
+    FlowTrajectoryDiffuserInferenceModule_HisDiT,
+)
+from flowbothd.models.flow_diffuser_pndit import (
+    FlowTrajectoryDiffuserInferenceModule_PNDiT,
+)
+from flowbothd.models.modules.dit_models import DiT, PN2DiT, PN2HisDiT
+from flowbothd.models.modules.history_encoder import HistoryEncoder
+from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn
+
+data_module_class = {
+    "trajectory": FlowTrajectoryDataModule,
+}
+
+inference_module_class = {
+    "diffuser_dit": FlowTrajectoryDiffuserInferenceModule_DiT,
+    "diffuser_hisdit": FlowTrajectoryDiffuserInferenceModule_HisDiT,
+    "diffuser_hispndit": FlowTrajectoryDiffuserInferenceModule_HisPNDiT,
+    "diffuser_pndit": FlowTrajectoryDiffuserInferenceModule_PNDiT,
+}
+
+history_network_class = {
+    "encoder": HistoryEncoder,
+}
+
+
+@torch.no_grad()
+@hydra.main(config_path="../configs", config_name="eval_history", version_base="1.3")
+def main(cfg):
+    ######################################################################
+    # Torch settings.
+    ######################################################################
+
+    # Make deterministic + reproducible.
+    torch.backends.cudnn.deterministic = True
+    torch.backends.cudnn.benchmark = False
+
+    # Since most of us are training on 3090s+, we can use mixed precision.
+    torch.set_float32_matmul_precision("highest")
+
+    # Global seed for reproducibility.
+    L.seed_everything(42)
+
+    ######################################################################
+    # Create the datamodule.
+    # Should be the same one as in training, but we're gonna use val+test
+    # dataloaders.
+    ######################################################################
+    trajectory_len = cfg.inference.trajectory_len
+    if cfg.dataset.dataset_type == "full-dataset":
+        # Full dataset
+        toy_dataset = None
+    else:
+        # Door dataset
+        toy_dataset = {
+            "id": "door-full-new-noslide",
+            "train-train": [
+                "8877",
+                "8893",
+                "8897",
+                "8903",
+                "8919",
+                "8930",
+                "8961",
+                "8997",
+                "9016",
+                # "9032",   # has slide
+                "9035",
+                "9041",
+                "9065",
+                "9070",
+                "9107",
+                "9117",
+                "9127",
+                "9128",
+                "9148",
+                "9164",
+                "9168",
+                "9277",
+                "9280",
+                "9281",
+                "9288",
+                "9386",
+                "9388",
+                "9410",
+            ],
+            "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+            "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+        }
+    
+    # Create History dataset
+    fully_closed_datamodule = FlowTrajectoryDataModule(
+        root="/home/yishu/datasets/partnet-mobility",
+        batch_size=1,
+        num_workers=30,
+        n_proc=2,
+        seed=42,
+        trajectory_len=1,  # Only used when training trajectory model
+        special_req="fully-closed",
+        history=True,
+        toy_dataset=toy_dataset,
+        n_repeat=1,
+    )
+
+    randomly_opened_datamodule = FlowTrajectoryDataModule(
+        root="/home/yishu/datasets/partnet-mobility",
+        batch_size=1,
+        num_workers=30,
+        n_proc=2,
+        seed=42,
+        trajectory_len=1,  # Only used when training trajectory model
+        special_req=None,
+        history=True,
+        toy_dataset=toy_dataset,
+        n_repeat=1,
+    )
+
+    ######################################################################
+    # Set up logging in WandB.
+    # This is a different job type (eval), but we want it all grouped
+    # together. Notice that we use our own logging here (not lightning).
+    ######################################################################
+
+    # Create a run.
+    run = wandb.init(
+        entity=cfg.wandb.entity,
+        project=cfg.wandb.project,
+        dir=cfg.wandb.save_dir,
+        config=omegaconf.OmegaConf.to_container(
+            cfg, resolve=True, throw_on_missing=True
+        ),
+        job_type=cfg.job_type,
+        save_code=True,  # This just has the main script.
+        group=cfg.wandb.group,
+    )
+
+    # Log the code.
+    wandb.run.log_code(
+        root=PROJECT_ROOT,
+        include_fn=match_fn(
+            dirs=["configs", "scripts", "src"],
+            extensions=[".py", ".yaml"],
+        ),
+    )
+
+    ######################################################################
+    # Create the network(s) which will be evaluated (same as training).
+    # You might want to put this into a "create_network" function
+    # somewhere so train and eval can be the same.
+    #
+    # We'll also load the weights.
+    ######################################################################
+
+    if "diffuser" in cfg.model.name:
+        if "pn++" in cfg.model.name:
+            in_channels = 3 * cfg.inference.trajectory_len + cfg.model.time_embed_dim
+        else:
+            in_channels = (
+                3 * cfg.inference.trajectory_len
+            )  # Will add 3 as input channel in diffuser
+    else:
+        in_channels = 1 if cfg.inference.mask_input_channel else 0
+
+    # History model
+    if "hispndit" in cfg.model.name:
+        network = {
+            "DiT": PN2HisDiT(
+                history_embed_dim=cfg.model.history_dim,
+                in_channels=3,
+                depth=5,
+                hidden_size=128,
+                num_heads=4,
+                learn_sigma=True,
+            ).cuda(),
+            "History": HistoryEncoder(
+                history_dim=cfg.model.history_dim,
+                history_len=cfg.model.history_len,
+                batch_norm=cfg.model.batch_norm,
+                transformer=False,
+                repeat_dim=False,
+            ).cuda(),
+        }
+
+    elif "hisdit" in cfg.model.name:
+        network = {
+            "DiT": DiT(
+                in_channels=in_channels + 3 + cfg.model.history_dim,
+                depth=5,
+                hidden_size=128,
+                num_heads=4,
+                learn_sigma=True,
+            ).cuda(),
+            "History": history_network_class[cfg.model.history_model](
+                history_dim=cfg.model.history_dim,
+                history_len=cfg.model.history_len,
+                batch_norm=cfg.model.batch_norm,
+            ).cuda(),
+        }
+    elif "pndit" in cfg.model.name:
+        network = PN2DiT(
+            in_channels=in_channels,
+            depth=5,
+            hidden_size=128,
+            patch_size=1,
+            num_heads=4,
+            n_points=cfg.dataset.n_points,
+        ).cuda()
+    elif "dit" in cfg.model.name:
+        network = DiT(
+            in_channels=in_channels + 3,
+            depth=5,
+            hidden_size=128,
+            num_heads=4,
+            learn_sigma=True,
+        ).cuda()
+
+    # # Get the checkpoint file. If it's a wandb reference, download.
+    # # Otherwise look to disk.
+    # checkpoint_reference = cfg.checkpoint.reference
+    # if checkpoint_reference.startswith(cfg.wandb.entity):
+    #     # download checkpoint locally (if not already cached)
+    #     artifact_dir = cfg.wandb.artifact_dir
+    #     artifact = run.use_artifact(checkpoint_reference, type="model")
+    #     ckpt_file = artifact.get_path("model.ckpt").download(root=artifact_dir)
+    # else:
+    #     ckpt_file = checkpoint_reference
+
+    ckpt_file = "TO BE SPECIFIED"
+
+    # # Load the network weights.
+    # ckpt = torch.load(ckpt_file)
+    # network.load_state_dict(
+    #     {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
+    # )
+
+    ######################################################################
+    # Create an inference module, which is basically just a bare-bones
+    # class which runs the model. In this example, we only implement
+    # the "predict_step" function, which may not be the blessed
+    # way to do it vis a vis lightning, but whatever.
+    #
+    # If this is a downstream application or something, you might
+    # want to implement a different interface (like with a "predict"
+    # function), so you can pass in un-batched observations from an
+    # environment, for instance.
+    ######################################################################
+
+    model = inference_module_class[cfg.model.name](
+        network, inference_cfg=cfg.inference, model_cfg=cfg.model
+    )
+    model.load_from_ckpt(ckpt_file)
+    model.eval()
+    model.cuda()
+
+    ######################################################################
+    # Run the model on the train/val/test sets.
+    # This outputs a list of dictionaries, one for each batch. This
+    # is annoying to work with, so later we'll flatten.
+    #
+    # If a downstream eval, you can swap it out with whatever the eval
+    # function is.
+    ######################################################################
+
+    dataloaders = [
+        # (datamodule.train_val_dataloader(), "train"),
+        # (datamodule.train_val_dataloader(bsz=1), "train"),
+        (fully_closed_datamodule.train_val_dataloader(bsz=1), "train_closed"),
+        (randomly_opened_datamodule.train_val_dataloader(bsz=1), "train_open"),
+        (fully_closed_datamodule.val_dataloader(bsz=1), "val_closed"),
+        (randomly_opened_datamodule.val_dataloader(bsz=1), "val_open"),
+        (fully_closed_datamodule.unseen_dataloader(bsz=1), "door_closed"),
+        (randomly_opened_datamodule.unseen_dataloader(bsz=1), "door_open"),
+    ]
+
+    trial_time = 50
+
+    all_metrics = []
+    all_directions = []
+    sample_cnt = 0
+    for loader, name in dataloaders:
+        sample_cnt += len(loader)
+
+        metrics, directions = model.predict_wta(
+            dataloader=loader, mode="delta", trial_times=trial_time
+        )
+        print(f"{name} metric:")
+        print(metrics)
+
+        all_metrics.append(metrics)
+        all_directions += directions
+
+    # # Scatter plot
+    # ys = [d.item() for d in all_directions]
+    # xs = sorted(list(range(sample_cnt)) * trial_time)
+    # xs = [f"{x}" for x in xs]
+    # colors = sorted(["red", "blue", "purple"] * trial_time) * sample_cnt
+    # import matplotlib.pyplot as plt
+
+    # plt.figure()
+    # plt.scatter(xs, ys, s=5, c=colors[: len(xs)])
+    # plt.savefig(f"./{cfg.model.name}_cos_stats.jpeg")
+    eval_set_names = [loader[1] for loader in dataloaders]
+    rows = [
+        (
+            id,
+            m["rmse"],
+            m["cosine_similarity"],
+            m["mag_error"],
+            m["multimodal"],
+            m["pos@0.7"],
+            m["neg@0.7"],
+        )
+        for id, m in zip(eval_set_names, all_metrics)
+    ]
+    df = pd.DataFrame(
+        rows,
+        columns=[
+            "category",
+            "rmse",
+            "cos_similarity",
+            "mag_error",
+            "multimodal",
+            "pos@0.7",
+            "neg@0.7",
+        ],
+    )
+
+    # out_file = Path(cfg.log_dir) / f"{cfg.dataset.name}_{trajectory_len}_{name}.csv"
+    # print(out_file)
+    # # if out_file.exists():
+    # #     raise ValueError(f"{out_file} already exists...")
+    # df.to_csv(out_file, float_format="%.3f")
+
+    # Log the metrics + table to wandb.
+    table = wandb.Table(dataframe=df)
+    run.log({f"eval_wta_metric_table": table})
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/eval_sim.py b/scripts/eval_sim.py
new file mode 100644
index 0000000..c0d585d
--- /dev/null
+++ b/scripts/eval_sim.py
@@ -0,0 +1,373 @@
+# The evaluation script that runs a rollout for each object in the eval-ed dataset and calculates:
+# - success : 90% open
+# - distance to open
+import json
+import os
+import pickle as pkl
+
+import hydra
+import lightning as L
+import numpy as np
+import omegaconf
+import pandas as pd
+import plotly.graph_objects as go
+import rpad.pyg.nets.pointnet2 as pnp
+import torch
+import tqdm
+import wandb
+from rpad.visualize_3d import html
+
+from flowbothd.simulations.simulation import trial_with_prediction
+from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn
+
+
+def load_obj_id_to_category(toy_dataset=None):
+    id_to_cat = {}
+    if toy_dataset is None:
+        # Extract existing classes.
+        with open(f"{PROJECT_ROOT}/scripts/umpnet_data_split_new.json", "r") as f:
+            data = json.load(f)
+
+        for _, category_dict in data.items():
+            for category, split_dict in category_dict.items():
+                for train_or_test, id_list in split_dict.items():
+                    for id in id_list:
+                        id_to_cat[id] = f"{category}_{train_or_test}"
+
+    else:
+        with open(f"{PROJECT_ROOT}/scripts/umpnet_object_list.json", "r") as f:
+            data = json.load(f)
+        for split in ["train-train", "train-test"]:
+            for id in toy_dataset[split]:
+                id_to_cat[id] = split
+    return id_to_cat
+
+
+def load_obj_and_link(id_to_cat):
+    # with open("./scripts/umpnet_object_list.json", "r") as f:
+    # with open(f"{PROJECT_ROOT}/scripts/movable_links_005.json", "r") as f:
+    with open(f"{PROJECT_ROOT}/scripts/movable_links_fullset_000.json", "r") as f:
+        object_link_json = json.load(f)
+    for id in id_to_cat.keys():
+        if id not in object_link_json.keys():
+            object_link_json[id] = []
+    return object_link_json
+
+
+object_ids = [  # Door
+    "8877",
+    "8893",
+    "8897",
+    "8903",
+    "8919",
+    "8930",
+    "8961",
+    "8997",
+    "9016",
+    "9032",
+    "9035",
+    "9041",
+    "9065",
+    "9070",
+    "9107",
+    "9117",
+    "9127",
+    "9128",
+    "9148",
+    "9164",
+    "9168",
+    "9277",
+    "9280",
+    "9281",
+    "9288",
+    "9386",
+    "9388",
+    "9410",
+    "8867",
+    "8983",
+    "8994",
+    "9003",
+    "9263",
+    "9393",
+]
+
+
+@hydra.main(config_path="../configs", config_name="eval_sim", version_base="1.3")
+def main(cfg):
+    ######################################################################
+    # Torch settings.
+    ######################################################################
+
+    # Make deterministic + reproducible.
+    torch.backends.cudnn.deterministic = True
+    torch.backends.cudnn.benchmark = False
+
+    # Since most of us are training on 3090s+, we can use mixed precision.
+    torch.set_float32_matmul_precision("medium")
+
+    # Global seed for reproducibility.
+    L.seed_everything(42)
+
+    ######################################################################
+    # Create the datamodule.
+    # Should be the same one as in training, but we're gonna use val+test
+    # dataloaders.
+    ######################################################################
+    
+    if cfg.dataset.dataset_type == "full-dataset":
+        # Full dataset
+        toy_dataset = None
+    else:
+        # Door dataset
+        toy_dataset = {
+            "id": "door-full-new-noslide",
+            "train-train": [
+                "8877",
+                "8893",
+                "8897",
+                "8903",
+                "8919",
+                "8930",
+                "8961",
+                "8997",
+                "9016",
+                # "9032",   # has slide
+                "9035",
+                "9041",
+                "9065",
+                "9070",
+                "9107",
+                "9117",
+                "9127",
+                "9128",
+                "9148",
+                "9164",
+                "9168",
+                "9277",
+                "9280",
+                "9281",
+                "9288",
+                "9386",
+                "9388",
+                "9410",
+            ],
+            "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+            "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+        }
+
+    id_to_cat = load_obj_id_to_category(toy_dataset)
+    object_to_link = load_obj_and_link(id_to_cat)
+
+
+    ######################################################################
+    # Set up logging in WandB.
+    # This is a different job type (eval), but we want it all grouped
+    # together. Notice that we use our own logging here (not lightning).
+    ######################################################################
+
+    # Create a run.
+    run = wandb.init(
+        entity=cfg.wandb.entity,
+        project=cfg.wandb.project,
+        dir=cfg.wandb.save_dir,
+        config=omegaconf.OmegaConf.to_container(
+            cfg, resolve=True, throw_on_missing=True
+        ),
+        job_type=cfg.job_type,
+        save_code=True,  # This just has the main script.
+        group=cfg.wandb.group,
+    )
+
+    # Log the code.
+    wandb.run.log_code(
+        root=PROJECT_ROOT,
+        include_fn=match_fn(
+            dirs=["configs", "scripts", "src"],
+            extensions=[".py", ".yaml"],
+        ),
+    )
+
+    ######################################################################
+    # Create the network(s) which will be evaluated (same as training).
+    # You might want to put this into a "create_network" function
+    # somewhere so train and eval can be the same.
+    #
+    # We'll also load the weights.
+    ######################################################################
+
+    mask_channel = 1 if cfg.inference.mask_input_channel else 0
+    network = pnp.PN2Dense(
+        in_channels=mask_channel,
+        out_channels=3 * cfg.inference.trajectory_len,
+        p=pnp.PN2DenseParams(),
+    )
+
+    # Get the checkpoint file. If it's a wandb reference, download.
+    # Otherwise look to disk.
+    checkpoint_reference = cfg.checkpoint.reference
+    if checkpoint_reference.startswith(cfg.wandb.entity):
+        # download checkpoint locally (if not already cached)
+        artifact_dir = cfg.wandb.artifact_dir
+        artifact = run.use_artifact(checkpoint_reference, type="model")
+        ckpt_file = artifact.get_path("model.ckpt").download(root=artifact_dir)
+    else:
+        ckpt_file = checkpoint_reference
+
+    # Load the network weights.
+    ckpt = torch.load(ckpt_file)
+    network.load_state_dict(
+        {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
+    )
+    network.eval()
+
+    # Simulation and results.
+    print("Simulating")
+    instance_results_json = {}  # The success or not results
+    flow_results_json = {}  # The flow visualizations
+    os.makedirs("./logs/flow_vis")
+
+    if cfg.website:
+        # Visualization html
+        os.makedirs("./logs/simu_eval/video_assets/")
+        doc = html.PlotlyWebsiteBuilder("Simulation Visualizations")
+
+    obj_cats = list(set(id_to_cat.values()))
+    metric_df = pd.DataFrame(
+        np.zeros((len(set(id_to_cat.values())), 4)),
+        index=obj_cats,
+        columns=["obj_cat", "count", "success_rate", "norm_dist"],
+    )
+    category_counts = {}
+    sim_trajectories = []
+    link_names = []
+
+    # Create the evaluate object lists
+    repeat_time = 5
+    obj_ids = []
+    for obj_id in tqdm.tqdm(list(object_to_link.keys())):
+        obj_cat = id_to_cat[obj_id]
+        # if obj_cat != "Door_test":
+        #     continue
+        if "test" not in obj_cat:
+            continue
+        if obj_id not in id_to_cat.keys():
+            continue
+        available_links = object_to_link[obj_id]
+        if len(available_links) == 0:
+            continue
+        if not os.path.exists(f"/home/yishu/datasets/partnet-mobility/raw/{obj_id}"):
+            continue
+        obj_ids.append(obj_id)
+    obj_ids = obj_ids * repeat_time
+
+    import random
+
+    random.shuffle(obj_ids)
+
+    # for obj_id, available_links in tqdm.tqdm(list(object_to_link.items())):
+    for obj_id in tqdm.tqdm(obj_ids):
+        # # if obj_id not in object_ids:  # For Door dataset
+        # #     continue
+        # if obj_id not in id_to_cat.keys():
+        #     continue
+        # if len(available_links) == 0:
+        #     continue
+
+        # if not os.path.exists(f"/home/yishu/datasets/partnet-mobility/raw/{obj_id}"):
+        #     continue
+
+        available_links = object_to_link[obj_id]
+        obj_cat = id_to_cat[obj_id]
+        print(f"OBJ {obj_id} of {obj_cat}")
+        trial_figs, trial_results, sim_trajectory = trial_with_prediction(
+            obj_id=obj_id,
+            network=network,
+            n_step=30,
+            gui=cfg.gui,
+            all_joint=True,
+            available_joints=available_links,
+            website=cfg.website,
+            sgp=cfg.sgp,
+        )
+        sim_trajectories += sim_trajectory
+        link_names += [f"{obj_id}_{link}" for link in available_links]
+
+        # breakpoint()
+        if len(trial_results) == 0:  # If nothing succeeds
+            continue
+
+        # Wandb table
+        if obj_cat not in category_counts.keys():
+            category_counts[obj_cat] = 0
+        # category_counts[obj_cat] += len(trial_results)
+        for result, link_name in zip(
+            trial_results, [f"{obj_id}_{link}" for link in available_links]
+        ):
+            # print(link_name, result.metric)
+            instance_results_json[
+                link_name
+            ] = result.metric  # record the normalized distance
+            # breakpoint()
+            if result.contact == False:
+                continue
+            category_counts[obj_cat] += 1
+            metric_df.loc[obj_cat]["success_rate"] += result.success
+            metric_df.loc[obj_cat]["norm_dist"] += result.metric
+
+        if cfg.website:
+            # Website visualization
+            for id, (joint_name, fig) in enumerate(trial_figs.items()):
+                tag = f"{obj_id}_{joint_name}"
+                if fig is not None:
+                    doc.add_plot(obj_cat, tag, fig)
+                    with open(
+                        f"./logs/flow_vis/{tag}.pkl", "wb"
+                    ) as f:  # Save the flow visualization (not in website, but for demo)
+                        pkl.dump(fig, f)
+                    doc.add_video(
+                        obj_cat,
+                        f"{tag}{'_NO CONTACT' if not trial_results[id].contact else ''}",
+                        f"http://128.2.178.238:{cfg.website_port}/video_assets/{tag}.mp4",
+                    )
+            # print(trial_results)
+            doc.write_site("./logs/simu_eval")
+
+        if category_counts[obj_cat] == 0:
+            continue
+        wandb_df = metric_df.copy(deep=True)
+        for obj_cat in category_counts.keys():
+            wandb_df.loc[obj_cat]["obj_cat"] = 0 if "train" in obj_cat else 1
+            wandb_df.loc[obj_cat]["success_rate"] /= category_counts[obj_cat]
+            wandb_df.loc[obj_cat]["norm_dist"] /= category_counts[obj_cat]
+            wandb_df.loc[obj_cat]["count"] = category_counts[obj_cat]
+
+        # table = wandb.Table(dataframe=wandb_df.reset_index())
+        table = wandb.Table(dataframe=wandb_df.reset_index())
+        run.log({f"simulation_metric_table": table})
+
+        # Save/update the instance result json
+        with open("./logs/instance_result.json", "w") as f:
+            json.dump(instance_results_json, f)
+
+    print(wandb_df)
+    # for obj_cat in category_counts.keys():
+    #     metric_df.loc[obj_cat]["success_rate"] /= category_counts[obj_cat]
+    #     metric_df.loc[obj_cat]["norm_dist"] /= category_counts[obj_cat]
+    #     metric_df.loc[obj_cat]["category"] = obj_cat
+
+    # table = wandb.Table(dataframe=metric_df)
+    # run.log({f"simulation_metric_table": table})
+    traces = []
+    xs = list(range(31))
+    for id, sim_trajectory in enumerate(sim_trajectories):
+        traces.append(
+            go.Scatter(x=xs, y=sim_trajectory, mode="lines", name=link_names[id])
+        )
+
+    layout = go.Layout(title="Simulation Trajectory Figure")
+    fig = go.Figure(data=traces, layout=layout)
+    wandb.log({"sim_traj_figure": wandb.Plotly(fig)})
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/eval_sim_diffuser.py b/scripts/eval_sim_diffuser.py
new file mode 100644
index 0000000..97a66a1
--- /dev/null
+++ b/scripts/eval_sim_diffuser.py
@@ -0,0 +1,359 @@
+# The evaluation script that runs a rollout for each object in the eval-ed dataset and calculates:
+# - success : 90% open
+# - distance to open
+
+import json
+import os
+
+import hydra
+import lightning as L
+import numpy as np
+import omegaconf
+import pandas as pd
+import plotly.graph_objects as go
+import rpad.pyg.nets.pointnet2 as pnp
+import torch
+import tqdm
+import wandb
+from rpad.visualize_3d import html
+
+from flowbothd.models.flow_diffuser_dgdit import (
+    FlowTrajectoryDiffuserSimulationModule_DGDiT,
+)
+from flowbothd.models.flow_diffuser_dit import (
+    FlowTrajectoryDiffuserSimulationModule_DiT,
+)
+from flowbothd.models.flow_diffuser_pndit import (
+    FlowTrajectoryDiffuserSimulationModule_PNDiT,
+)
+from flowbothd.models.flow_trajectory_diffuser import (
+    FlowTrajectoryDiffuserSimulationModule_PN2,
+)
+from flowbothd.models.modules.dit_models import DGDiT, DiT, PN2DiT
+from flowbothd.simulations.simulation import trial_with_diffuser
+from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn
+
+print(PROJECT_ROOT)
+
+
+def load_obj_id_to_category(toy_dataset=None):
+    id_to_cat = {}
+    if toy_dataset is None:
+        # Extract existing classes.
+        with open(f"{PROJECT_ROOT}/scripts/umpnet_data_split_new.json", "r") as f:
+            data = json.load(f)
+
+        for _, category_dict in data.items():
+            for category, split_dict in category_dict.items():
+                for train_or_test, id_list in split_dict.items():
+                    for id in id_list:
+                        id_to_cat[id] = f"{category}_{train_or_test}"
+
+    else:
+        with open(f"{PROJECT_ROOT}/scripts/umpnet_object_list.json", "r") as f:
+            data = json.load(f)
+        for split in ["train-train", "train-test"]:
+            for id in toy_dataset[split]:
+                id_to_cat[id] = split
+    return id_to_cat
+
+
+def load_obj_and_link(id_to_cat):
+    with open(f"{PROJECT_ROOT}/scripts/movable_links_fullset_000.json", "r") as f:
+        object_link_json = json.load(f)
+    for id in id_to_cat.keys():
+        if id not in object_link_json.keys():
+            object_link_json[id] = []
+    return object_link_json
+
+
+inference_module_class = {
+    "diffuser_pn++": FlowTrajectoryDiffuserSimulationModule_PN2,
+    "diffuser_dgdit": FlowTrajectoryDiffuserSimulationModule_DGDiT,
+    "diffuser_dit": FlowTrajectoryDiffuserSimulationModule_DiT,
+    "diffuser_pndit": FlowTrajectoryDiffuserSimulationModule_PNDiT,
+}
+
+
+@hydra.main(config_path="../configs", config_name="eval_sim", version_base="1.3")
+def main(cfg):
+    ######################################################################
+    # Torch settings.
+    ######################################################################
+
+    # Make deterministic + reproducible.
+    torch.backends.cudnn.deterministic = True
+    torch.backends.cudnn.benchmark = False
+
+    # Since most of us are training on 3090s+, we can use mixed precision.
+    torch.set_float32_matmul_precision("highest")
+
+    # Global seed for reproducibility.
+    L.seed_everything(42)
+
+    ######################################################################
+    # Create the datamodule.
+    # Should be the same one as in training, but we're gonna use val+test
+    # dataloaders.
+    ######################################################################
+   
+    
+    if cfg.dataset.dataset_type == "full-dataset":
+        # Full dataset
+        toy_dataset = None
+    else:
+        # Door dataset
+        toy_dataset = {
+            "id": "door-full-new-noslide",
+            "train-train": [
+                "8877",
+                "8893",
+                "8897",
+                "8903",
+                "8919",
+                "8930",
+                "8961",
+                "8997",
+                "9016",
+                # "9032",   # has slide
+                "9035",
+                "9041",
+                "9065",
+                "9070",
+                "9107",
+                "9117",
+                "9127",
+                "9128",
+                "9148",
+                "9164",
+                "9168",
+                "9277",
+                "9280",
+                "9281",
+                "9288",
+                "9386",
+                "9388",
+                "9410",
+            ],
+            "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+            "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+        }
+
+    id_to_cat = load_obj_id_to_category(toy_dataset)
+    object_to_link = load_obj_and_link(id_to_cat)
+    ######################################################################
+    # Set up logging in WandB.
+    # This is a different job type (eval), but we want it all grouped
+    # together. Notice that we use our own logging here (not lightning).
+    ######################################################################
+
+    # Create a run.
+    run = wandb.init(
+        entity=cfg.wandb.entity,
+        project=cfg.wandb.project,
+        dir=cfg.wandb.save_dir,
+        config=omegaconf.OmegaConf.to_container(
+            cfg, resolve=True, throw_on_missing=True
+        ),
+        job_type=cfg.job_type,
+        save_code=True,  # This just has the main script.
+        group=cfg.wandb.group,
+    )
+
+    # Log the code.
+    wandb.run.log_code(
+        root=PROJECT_ROOT,
+        include_fn=match_fn(
+            dirs=["configs", "scripts", "src"],
+            extensions=[".py", ".yaml"],
+        ),
+    )
+
+    ######################################################################
+    # Create the network(s) which will be evaluated (same as training).
+    # You might want to put this into a "create_network" function
+    # somewhere so train and eval can be the same.
+    #
+    # We'll also load the weights.
+    ######################################################################
+
+    trajectory_len = cfg.inference.trajectory_len
+    if "diffuser" in cfg.model.name:
+        if "pn++" in cfg.model.name:
+            in_channels = 3 * cfg.inference.trajectory_len + cfg.model.time_embed_dim
+        else:
+            in_channels = (
+                3 * cfg.inference.trajectory_len
+            )  # Will add 3 as input channel in diffuser
+    else:
+        in_channels = 1 if cfg.inference.mask_input_channel else 0
+
+    if "pn++" in cfg.model.name:
+        network = pnp.PN2Dense(
+            in_channels=in_channels,
+            out_channels=3 * trajectory_len,
+            p=pnp.PN2DenseParams(),
+        ).cuda()
+    elif "dgdit" in cfg.model.name:
+        network = DGDiT(
+            in_channels=in_channels,
+            depth=5,
+            hidden_size=128,
+            patch_size=1,
+            num_heads=4,
+            n_points=cfg.dataset.n_points,
+        ).cuda()
+    elif "pndit" in cfg.model.name:
+        network = PN2DiT(
+            in_channels=in_channels,
+            depth=5,
+            hidden_size=128,
+            patch_size=1,
+            num_heads=4,
+            n_points=cfg.dataset.n_points,
+        ).cuda()
+    elif "dit" in cfg.model.name:
+        network = DiT(
+            in_channels=in_channels + 3,
+            depth=5,
+            hidden_size=128,
+            num_heads=4,
+            learn_sigma=True,
+        ).cuda()
+
+    # # Get the checkpoint file. If it's a wandb reference, download.
+    # # Otherwise look to disk.
+    # checkpoint_reference = cfg.checkpoint.reference
+    # if checkpoint_reference.startswith(cfg.wandb.entity):
+    #     # download checkpoint locally (if not already cached)
+    #     artifact_dir = cfg.wandb.artifact_dir
+    #     artifact = run.use_artifact(checkpoint_reference, type="model")
+    #     ckpt_file = artifact.get_path("model.ckpt").download(root=artifact_dir)
+    # else:
+    #     ckpt_file = checkpoint_reference
+    # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_dit/2024-03-30/07-12-41/checkpoints/epoch=359-step=199080-val_loss=0.00-weights-only.ckpt"
+    ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_pndit/2024-04-23/05-01-44/checkpoints/epoch=469-step=1038700-val_loss=0.00-weights-only.ckpt"
+
+    # # Load the network weights.
+    # ckpt = torch.load(ckpt_file)
+    # network.load_state_dict(
+    #     {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
+    # )
+    model = inference_module_class[cfg.model.name](
+        network, inference_cfg=cfg.inference, model_cfg=cfg.model
+    ).cuda()
+    model.load_from_ckpt(ckpt_file)
+    model.eval()
+
+    # Simulation and results.
+    print("Simulating")
+    if cfg.website:
+        # Visualization html
+        os.makedirs("./logs/simu_eval/video_assets/")
+        doc = html.PlotlyWebsiteBuilder("Simulation Visualizations")
+    obj_cats = list(set(id_to_cat.values()))
+    metric_df = pd.DataFrame(
+        np.zeros((len(set(id_to_cat.values())), 4)),
+        index=obj_cats,
+        columns=["obj_cat", "count", "success_rate", "norm_dist"],
+    )
+    category_counts = {}
+    sim_trajectories = []
+    link_names = []
+
+    # Create the evaluate object lists
+    repeat_time = 5
+    obj_ids = []
+    for obj_id, obj_cat in tqdm.tqdm(list(id_to_cat.items())):
+        if "test" not in obj_cat:
+            continue
+        if not os.path.exists(f"/home/yishu/datasets/partnet-mobility/raw/{obj_id}"):
+            continue
+        available_links = object_to_link[obj_id]
+        if len(available_links) == 0:
+            continue
+        obj_ids.append(obj_id)
+    obj_ids = obj_ids * repeat_time
+
+    import random
+
+    random.shuffle(obj_ids)
+
+    # for obj_id, obj_cat in tqdm.tqdm(list(id_to_cat.items())):
+    #     if not os.path.exists(f"/home/yishu/datasets/partnet-mobility/raw/{obj_id}"):
+    #         continue
+    #     available_links = object_to_link[obj_id]
+    #     if len(available_links) == 0:
+    #         continue
+    for obj_id in tqdm.tqdm(obj_ids):
+        obj_cat = id_to_cat[obj_id]
+        # if "test" not in obj_cat:
+        #     continue
+        # if not os.path.exists(f"/home/yishu/datasets/partnet-mobility/raw/{obj_id}"):
+        #     continue
+        available_links = object_to_link[obj_id]
+        print(f"OBJ {obj_id} of {obj_cat}")
+        trial_figs, trial_results, sim_trajectory = trial_with_diffuser(
+            obj_id=obj_id,
+            model=model,
+            n_step=30,
+            gui=False,
+            website=cfg.website,
+            all_joint=True,
+            available_joints=available_links,
+            sgp=cfg.sgp,
+            consistency_check=cfg.consistency_check,
+        )
+        sim_trajectories += sim_trajectory
+        link_names += [f"{obj_id}_{link}" for link in available_links]
+
+        # Wandb table
+        if obj_cat not in category_counts.keys():
+            category_counts[obj_cat] = 0
+        category_counts[obj_cat] += len(trial_results)
+        for result in trial_results:
+            metric_df.loc[obj_cat]["success_rate"] += result.success
+            metric_df.loc[obj_cat]["norm_dist"] += result.metric
+
+        if cfg.website:
+            # Website visualization
+            for id, (joint_name, fig) in enumerate(trial_figs.items()):
+                tag = f"{obj_id}_{joint_name}"
+                if fig is not None:
+                    doc.add_plot(obj_cat, tag, fig)
+                doc.add_video(
+                    obj_cat,
+                    f"{tag}{'_NO CONTACT' if not trial_results[id].contact else ''}",
+                    f"http://128.2.178.238:{cfg.website_port}/video_assets/{tag}.mp4",
+                )
+            # print(trial_results)
+            doc.write_site("./logs/simu_eval")
+
+        if category_counts[obj_cat] == 0:
+            continue
+        wandb_df = metric_df.copy(deep=True)
+        for obj_cat in category_counts.keys():
+            wandb_df.loc[obj_cat]["success_rate"] /= category_counts[obj_cat]
+            wandb_df.loc[obj_cat]["norm_dist"] /= category_counts[obj_cat]
+            wandb_df.loc[obj_cat]["count"] = category_counts[obj_cat]
+            wandb_df.loc[obj_cat]["obj_cat"] = 0 if "train" in obj_cat else 1
+
+        table = wandb.Table(dataframe=wandb_df.reset_index())
+        run.log({f"simulation_metric_table": table})
+
+    print(wandb_df)
+
+    traces = []
+    xs = list(range(31))
+    for id, sim_trajectory in enumerate(sim_trajectories):
+        traces.append(
+            go.Scatter(x=xs, y=sim_trajectory, mode="lines", name=link_names[id])
+        )
+
+    layout = go.Layout(title="Simulation Trajectory Figure")
+    fig = go.Figure(data=traces, layout=layout)
+    wandb.log({"sim_traj_figure": wandb.Plotly(fig)})
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/eval_sim_diffuser_history.py b/scripts/eval_sim_diffuser_history.py
new file mode 100644
index 0000000..19fb9a8
--- /dev/null
+++ b/scripts/eval_sim_diffuser_history.py
@@ -0,0 +1,441 @@
+# The evaluation script that runs a rollout for each object in the eval-ed dataset and calculates:
+# - success : 90% open
+# - distance to open
+
+import json
+import os
+import pickle as pkl
+
+import hydra
+import lightning as L
+import numpy as np
+import omegaconf
+import pandas as pd
+import plotly.graph_objects as go
+import rpad.pyg.nets.pointnet2 as pnp
+import torch
+import tqdm
+import wandb
+from rpad.visualize_3d import html
+
+from flowbothd.models.flow_diffuser_dgdit import (
+    FlowTrajectoryDiffuserSimulationModule_DGDiT,
+)
+from flowbothd.models.flow_diffuser_dit import (
+    FlowTrajectoryDiffuserSimulationModule_DiT,
+)
+from flowbothd.models.flow_diffuser_hisdit import (
+    FlowTrajectoryDiffuserSimulationModule_HisDiT,
+)
+from flowbothd.models.flow_diffuser_hispndit import (
+    FlowTrajectoryDiffuserSimulationModule_HisPNDiT,
+)
+from flowbothd.models.flow_diffuser_pndit import (
+    FlowTrajectoryDiffuserSimulationModule_PNDiT,
+)
+from flowbothd.models.flow_trajectory_diffuser import (
+    FlowTrajectoryDiffuserSimulationModule_PN2,
+)
+from flowbothd.models.modules.dit_models import (
+    DGDiT,
+    DiT,
+    PN2DiT,
+    PN2HisDiT,
+)
+from flowbothd.models.modules.history_encoder import HistoryEncoder
+from flowbothd.simulations.simulation import trial_with_diffuser_history
+from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn
+
+PROJECT_ROOT = "/home/yishu/FlowBotHD" #"YOUR CURRENT DIRECTORY"
+
+
+def load_obj_id_to_category(toy_dataset=None):
+    id_to_cat = {}
+    if toy_dataset is None:
+        # Extract existing classes.
+        with open(f"{PROJECT_ROOT}/scripts/umpnet_data_split_new.json", "r") as f:
+            data = json.load(f)
+
+        for _, category_dict in data.items():
+            for category, split_dict in category_dict.items():
+                for train_or_test, id_list in split_dict.items():
+                    for id in id_list:
+                        id_to_cat[id] = f"{category}_{train_or_test}"
+
+    else:
+        with open(f"{PROJECT_ROOT}/scripts/umpnet_object_list.json", "r") as f:
+            data = json.load(f)
+        for split in ["train-train", "train-test"]:
+            # for split in ["train-test"]:
+            for id in toy_dataset[split]:
+                id_to_cat[id] = split
+    return id_to_cat
+
+
+def load_obj_and_link(id_to_cat):
+    # with open("./scripts/umpnet_object_list.json", "r") as f:
+    # with open(f"{PROJECT_ROOT}/scripts/movable_links_005.json", "r") as f:
+    with open(f"{PROJECT_ROOT}/scripts/movable_links_fullset_000.json", "r") as f:
+        object_link_json = json.load(f)
+    for id in id_to_cat.keys():
+        if id not in object_link_json.keys():
+            object_link_json[id] = []
+    return object_link_json
+
+
+inference_module_class = {
+    "diffuser_pn++": FlowTrajectoryDiffuserSimulationModule_PN2,
+    "diffuser_dgdit": FlowTrajectoryDiffuserSimulationModule_DGDiT,
+    "diffuser_dit": FlowTrajectoryDiffuserSimulationModule_DiT,
+    "diffuser_pndit": FlowTrajectoryDiffuserSimulationModule_PNDiT,
+}
+
+
+@hydra.main(config_path="../configs", config_name="eval_sim", version_base="1.3")
+def main(cfg):
+    ######################################################################
+    # Torch settings.
+    ######################################################################
+
+    # Make deterministic + reproducible.
+    torch.backends.cudnn.deterministic = True
+    torch.backends.cudnn.benchmark = False
+
+    # Since most of us are training on 3090s+, we can use mixed precision.
+    torch.set_float32_matmul_precision("highest")
+
+    # Global seed for reproducibility.
+    L.seed_everything(20030208)
+    np.random.seed(20030208)
+    torch.manual_seed(20030208)
+
+    ######################################################################
+    # Create the datamodule.
+    # Should be the same one as in training, but we're gonna use val+test
+    # dataloaders.
+    ######################################################################
+
+    if cfg.dataset.dataset_type == "full-dataset":
+        # Full dataset
+        toy_dataset = None
+    else:
+        # Door dataset
+        toy_dataset = {
+            "id": "door-full-new-noslide",
+            "train-train": [
+                "8877",
+                "8893",
+                "8897",
+                "8903",
+                "8919",
+                "8930",
+                "8961",
+                "8997",
+                "9016",
+                # "9032",   # has slide
+                "9035",
+                "9041",
+                "9065",
+                "9070",
+                "9107",
+                "9117",
+                "9127",
+                "9128",
+                "9148",
+                "9164",
+                "9168",
+                "9277",
+                "9280",
+                "9281",
+                "9288",
+                "9386",
+                "9388",
+                "9410",
+            ],
+            "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+            "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+        }
+
+    id_to_cat = load_obj_id_to_category(toy_dataset)
+    object_to_link = load_obj_and_link(id_to_cat)
+    ######################################################################
+    # Set up logging in WandB.
+    # This is a different job type (eval), but we want it all grouped
+    # together. Notice that we use our own logging here (not lightning).
+    ######################################################################
+
+    # Create a run.
+    run = wandb.init(
+        entity=cfg.wandb.entity,
+        project=cfg.wandb.project,
+        dir=cfg.wandb.save_dir,
+        config=omegaconf.OmegaConf.to_container(
+            cfg, resolve=True, throw_on_missing=True
+        ),
+        job_type=cfg.job_type,
+        save_code=True,  # This just has the main script.
+        group=cfg.wandb.group,
+    )
+
+    # Log the code.
+    wandb.run.log_code(
+        root=PROJECT_ROOT,
+        include_fn=match_fn(
+            dirs=["configs", "scripts", "src"],
+            extensions=[".py", ".yaml"],
+        ),
+    )
+
+    ######################################################################
+    # Create the network(s) which will be evaluated (same as training).
+    # You might want to put this into a "create_network" function
+    # somewhere so train and eval can be the same.
+    #
+    # We'll also load the weights.
+    ######################################################################
+
+    # If we still need original diffusion method
+    if "his" not in cfg.model.name:
+        trajectory_len = cfg.inference.trajectory_len
+        if "diffuser" in cfg.model.name:
+            if "pn++" in cfg.model.name:
+                in_channels = (
+                    3 * cfg.inference.trajectory_len + cfg.model.time_embed_dim
+                )
+            else:
+                in_channels = (
+                    3 * cfg.inference.trajectory_len
+                )  # Will add 3 as input channel in diffuser
+        else:
+            in_channels = 1 if cfg.inference.mask_input_channel else 0
+
+        if "pn++" in cfg.model.name:
+            network = pnp.PN2Dense(
+                in_channels=in_channels,
+                out_channels=3 * trajectory_len,
+                p=pnp.PN2DenseParams(),
+            ).cuda()
+        elif "dgdit" in cfg.model.name:
+            network = DGDiT(
+                in_channels=in_channels,
+                depth=5,
+                hidden_size=128,
+                patch_size=1,
+                num_heads=4,
+                n_points=cfg.dataset.n_points,
+            ).cuda()
+        elif "pndit" in cfg.model.name:
+            network = PN2DiT(
+                in_channels=in_channels,
+                depth=5,
+                hidden_size=128,
+                patch_size=1,
+                num_heads=4,
+                n_points=cfg.dataset.n_points,
+            ).cuda()
+        elif "dit" in cfg.model.name:
+            network = DiT(
+                in_channels=in_channels + 3,
+                depth=5,
+                hidden_size=128,
+                num_heads=4,
+                learn_sigma=True,
+            ).cuda()
+
+        # # Get the checkpoint file. If it's a wandb reference, download.
+        # # Otherwise look to disk.
+        # checkpoint_reference = cfg.checkpoint.reference
+        # if checkpoint_reference.startswith(cfg.wandb.entity):
+        #     # download checkpoint locally (if not already cached)
+        #     artifact_dir = cfg.wandb.artifact_dir
+        #     artifact = run.use_artifact(checkpoint_reference, type="model")
+        #     ckpt_file = artifact.get_path("model.ckpt").download(root=artifact_dir)
+        # else:
+        #     ckpt_file = checkpoint_reference
+        ckpt_file = "TO BE SPECIFIED"
+
+        # # Load the network weights.
+        # ckpt = torch.load(ckpt_file)
+        # network.load_state_dict(
+        #     {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
+        # )
+        model = inference_module_class[cfg.model.name](
+            network, inference_cfg=cfg.inference, model_cfg=cfg.model
+        ).cuda()
+        model.load_from_ckpt(ckpt_file)
+        model.eval()
+
+    # History model
+    if "hispndit" in cfg.model.name:
+        network = {
+            "DiT": PN2HisDiT(
+                history_embed_dim=cfg.model.history_dim,
+                in_channels=3,
+                depth=5,
+                hidden_size=128,
+                num_heads=4,
+                learn_sigma=True,
+            ).cuda(),
+            "History": HistoryEncoder(
+                history_dim=cfg.model.history_dim,
+                history_len=cfg.model.history_len,
+                batch_norm=cfg.model.batch_norm,
+                transformer=False,
+                repeat_dim=False,
+            ).cuda(),
+        }
+        history_model = FlowTrajectoryDiffuserSimulationModule_HisPNDiT(
+            network, inference_cfg=cfg.inference, model_cfg=cfg.model
+        ).cuda()
+    elif "hisdit" in cfg.model.name:
+        network = {
+            "DiT": DiT(
+                in_channels=3 + 3 + 128,
+                depth=5,
+                hidden_size=128,
+                num_heads=4,
+                learn_sigma=True,
+            ).cuda(),
+            "History": HistoryEncoder(
+                history_dim=128, history_len=1, batch_norm=False
+            ).cuda(),
+        }
+        history_model = FlowTrajectoryDiffuserSimulationModule_HisDiT(
+            network, inference_cfg=cfg.inference, model_cfg=cfg.model
+        ).cuda()
+        
+    ckpt_file = "/home/yishu/FlowBotHD/logs/train_trajectory_diffuser_hispndit/2024-11-02/22-44-51/checkpoints/epoch=64-step=2795-val_loss=0.00-weights-only.ckpt"#"TO BE SPECIFIED"
+    history_model.load_from_ckpt(ckpt_file)
+    history_model.eval()
+
+    # Simulation and results.
+    print("Simulating")
+    instance_results_json = {}  # The success or not results
+    flow_results_json = {}  # The flow visualizations
+    os.makedirs("./logs/flow_vis")
+
+    if cfg.website:
+        # Visualization html
+        os.makedirs("./logs/simu_eval/video_assets/")
+        doc = html.PlotlyWebsiteBuilder("Simulation Visualizations")
+    obj_cats = list(set(id_to_cat.values()))
+    metric_df = pd.DataFrame(
+        np.zeros((len(set(id_to_cat.values())), 4)),
+        index=obj_cats,
+        columns=["obj_cat", "count", "success_rate", "norm_dist"],
+    )
+    category_counts = {}
+    sim_trajectories = []
+    link_names = []
+
+    # Create the evaluate object lists
+    repeat_time = 5
+    obj_ids = []
+    for obj_id, obj_cat in tqdm.tqdm(list(id_to_cat.items())):
+        if "test" not in obj_cat:
+            continue
+        if not os.path.exists(f"/home/yishu/datasets/partnet-mobility/raw/{obj_id}"):
+            continue
+        available_links = object_to_link[obj_id]
+        if len(available_links) == 0:
+            continue
+        obj_ids.append(obj_id)
+    obj_ids = obj_ids * repeat_time
+
+    import random
+
+    random.shuffle(obj_ids)
+
+    # for obj_id, obj_cat in tqdm.tqdm(list(id_to_cat.items())):
+    for obj_id in tqdm.tqdm(obj_ids):
+        obj_cat = id_to_cat[obj_id]
+        # if "test" not in obj_cat:
+        #     continue
+        # if not os.path.exists(f"/home/yishu/datasets/partnet-mobility/raw/{obj_id}"):
+        #     continue
+        available_links = object_to_link[obj_id]
+        # if len(available_links) == 0:
+        #     continue
+        print(f"OBJ {obj_id} of {obj_cat}")
+        trial_figs, trial_results, sim_trajectory = trial_with_diffuser_history(
+            obj_id=obj_id,
+            # model=model,
+            model=history_model,  # All history model!!!
+            history_model=history_model,
+            n_step=30,
+            gui=False,
+            website=cfg.website,
+            all_joint=True,
+            available_joints=available_links,
+            consistency_check=cfg.consistency_check,
+            history_filter=cfg.history_filter,
+        )
+        sim_trajectories += sim_trajectory
+        link_names += [f"{obj_id}_{link}" for link in available_links]
+
+        # Wandb table
+        if obj_cat not in category_counts.keys():
+            category_counts[obj_cat] = 0
+        category_counts[obj_cat] += len(trial_results)
+
+        # for result in trial_results:
+        for result, link_name in zip(
+            trial_results, [f"{obj_id}_{link}" for link in available_links]
+        ):
+            instance_results_json[
+                link_name
+            ] = result.metric  # record the normalized distance
+            metric_df.loc[obj_cat]["success_rate"] += result.success
+            metric_df.loc[obj_cat]["norm_dist"] += result.metric
+
+        if cfg.website:
+            # Website visualization
+            for id, (joint_name, fig) in enumerate(trial_figs.items()):
+                tag = f"{obj_id}_{joint_name}"
+                if fig is not None:
+                    doc.add_plot(obj_cat, tag, fig)
+                    with open(
+                        f"./logs/flow_vis/{tag}.pkl", "wb"
+                    ) as f:  # Save the flow visualization (not in website, but for demo)
+                        pkl.dump(fig, f)
+
+                    doc.add_video(
+                        obj_cat,
+                        f"{tag}{'_NO CONTACT' if not trial_results[id].contact else ''}",
+                        f"http://128.2.178.238:{cfg.website_port}/video_assets/{tag}.mp4",
+                    )
+            # print(trial_results)
+            doc.write_site("./logs/simu_eval")
+
+        if category_counts[obj_cat] == 0:
+            continue
+        wandb_df = metric_df.copy(deep=True)
+        for obj_cat in category_counts.keys():
+            wandb_df.loc[obj_cat]["success_rate"] /= category_counts[obj_cat]
+            wandb_df.loc[obj_cat]["norm_dist"] /= category_counts[obj_cat]
+            wandb_df.loc[obj_cat]["count"] = category_counts[obj_cat]
+            wandb_df.loc[obj_cat]["obj_cat"] = 0 if "train" in obj_cat else 1
+
+        table = wandb.Table(dataframe=wandb_df.reset_index())
+        run.log({f"simulation_metric_table": table})
+
+        # Save/update the instance result json
+        with open("./logs/instance_result.json", "w") as f:
+            json.dump(instance_results_json, f)
+
+    print(wandb_df)
+
+    traces = []
+    xs = list(range(31))
+    for id, sim_trajectory in enumerate(sim_trajectories):
+        traces.append(
+            go.Scatter(x=xs, y=sim_trajectory, mode="lines", name=link_names[id])
+        )
+
+    layout = go.Layout(title="Simulation Trajectory Figure")
+    fig = go.Figure(data=traces, layout=layout)
+    wandb.log({"sim_traj_figure": wandb.Plotly(fig)})
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/movable_links_fullset_000.json b/scripts/movable_links_fullset_000.json
new file mode 100644
index 0000000..4b60e35
--- /dev/null
+++ b/scripts/movable_links_fullset_000.json
@@ -0,0 +1 @@
+{"100015": ["link_0"], "100017": ["link_0"], "100021": ["link_0"], "100023": ["link_0"], "100025": ["link_0"], "100028": ["link_0"], "100032": ["link_0"], "100033": ["link_0"], "100038": ["link_0"], "100040": ["link_0"], "100045": ["link_0"], "100047": ["link_0"], "100051": ["link_0"], "100054": ["link_0"], "100055": [], "100056": ["link_0"], "100057": ["link_0"], "100058": ["link_0"], "100060": ["link_0"], "100613": ["link_0"], "100619": ["link_0"], "100623": ["link_0"], "100693": ["link_0"], "102080": ["link_0"], "102085": ["link_0"], "19179": ["link_0", "link_1"], "19855": ["link_0"], "19898": ["link_4", "link_1", "link_2", "link_3", "link_5", "link_6", "link_7"], "20043": ["link_1", "link_2"], "20411": ["link_0"], "20555": ["link_0"], "20745": ["link_0", "link_1", "link_2"], "20985": ["link_1"], "22241": ["link_0"], "22301": ["link_0", "link_1", "link_2"], "22339": ["link_0"], "22367": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5", "link_6", "link_7"], "22508": ["link_0"], "23372": ["link_2", "link_3", "link_0", "link_1"], "23511": ["link_0"], "24644": ["link_0"], "24931": ["link_1"], "25308": ["link_0", "link_1"], "25913": ["link_1", "link_2"], "26503": ["link_1"], "26525": ["link_0"], "26652": ["link_1"], "26657": ["link_1", "link_2"], "26670": ["link_0"], "26806": ["link_0"], "27044": ["link_0"], "27189": ["link_0"], "28164": ["link_0", "link_1"], "28668": ["link_0"], "29921": ["link_1"], "30238": ["link_1", "link_2"], "30341": ["link_1"], "30666": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5", "link_6", "link_7", "link_8"], "30739": [], "31249": ["link_3", "link_4", "link_0", "link_1"], "31601": ["link_1", "link_2"], "32052": [], "32086": ["link_0", "link_1"], "32174": ["link_0"], "32259": ["link_0", "link_1", "link_2"], "32324": ["link_0", "link_1", "link_2"], "32566": ["link_0"], "32601": ["link_2", "link_3"], "32625": ["link_0", "link_1", "link_2", "link_3"], "32761": ["link_0", "link_1", "link_2", "link_3"], "32932": ["link_0", "link_1"], "33116": ["link_0", "link_2"], "33457": ["link_0", "link_2"], "33810": ["link_0", "link_2", "link_3", "link_4", "link_5"], "33930": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "34178": ["link_2", "link_3", "link_0"], "34610": ["link_0", "link_1", "link_2", "link_3", "link_4"], "34617": ["link_1", "link_2"], "7290": [], "7220": [], "7120": ["link_1", "link_2"], "7179": ["link_3", "link_4"], "7187": [], "7201": ["link_3", "link_4"], "7332": ["link_4"], "101773": ["link_1"], "101808": ["link_1"], "101908": ["link_0", "link_1"], "101909": [], "101917": ["link_0"], "101924": ["link_0", "link_1"], "101930": ["link_0"], "101931": ["link_0"], "101940": ["link_0"], "101943": ["link_0"], "101946": ["link_0", "link_4"], "101947": ["link_1", "link_2", "link_6"], "101971": ["link_0"], "102001": ["link_0", "link_1"], "102018": ["link_0", "link_2", "link_5"], "102019": [], "102044": [], "12536": [], "12617": [], "12560": [], "12597": ["link_0"], "12552": ["link_0"], "12654": ["link_0"], "12530": [], "12565": [], "12563": ["link_0"], "12414": ["link_1"], "12558": ["link_0"], "12594": ["link_1"], "12579": [], "12621": ["link_0"], "11622": ["link_0"], "11661": ["link_1"], "11700": ["link_0"], "11826": ["link_2"], "12065": ["link_2"], "12092": ["link_0"], "12259": ["link_0"], "12349": ["link_0"], "12428": ["link_4"], "12480": ["link_1"], "12484": ["link_2", "link_3"], "12531": ["link_0"], "12540": ["link_0"], "12543": ["link_0"], "12553": [], "12559": ["link_0"], "12561": ["link_0"], "12562": ["link_0"], "12580": ["link_0"], "12583": ["link_1"], "12587": ["link_0"], "12590": ["link_0"], "12592": [], "12596": ["link_0"], "12605": ["link_0"], "12606": ["link_1"], "12614": ["link_1"], "9016": ["link_2"], "9164": [], "9041": [], "9410": ["link_1"], "9388": [], "9107": [], "9070": ["link_0"], "9386": [], "9168": ["link_1"], "8867": ["link_1"], "8893": ["link_2"], "8897": ["link_2"], "8903": [], "8919": ["link_2"], "8930": ["link_4"], "8961": ["link_1", "link_2"], "8983": [], "8997": ["link_0", "link_1"], "9003": [], "9065": ["link_1"], "9117": [], "9128": ["link_1", "link_2"], "9280": [], "9281": [], "9288": [], "102423": [], "102278": ["link_0", "link_7", "link_12", "link_13"], "102389": ["link_1"], "102418": ["link_0", "link_3", "link_4", "link_5", "link_7", "link_8"], "101363": ["link_0"], "101564": ["link_13", "link_1", "link_7"], "101579": ["link_0"], "101584": [], "101591": ["link_4", "link_3"], "101593": ["link_0", "link_1"], "101594": ["link_0"], "101599": ["link_14"], "101603": ["link_0"], "101604": ["link_13"], "101605": [], "101611": ["link_0", "link_1"], "101612": ["link_0"], "101619": ["link_3"], "102301": ["link_0"], "102309": ["link_0"], "102311": ["link_0"], "102316": ["link_0", "link_1"], "102318": ["link_0"], "102380": [], "102381": [], "102384": [], "102387": ["link_0"], "47645": ["link_0"], "48492": ["link_2"], "100129": [], "100141": ["link_0"], "100174": ["link_0"], "100189": ["link_0"], "100214": ["link_0"], "100243": ["link_0"], "100247": [], "100664": ["link_0"], "102377": ["link_0", "link_1"], "102379": ["link_0"], "102456": ["link_0"], "100438": ["link_0"], "100444": ["link_0"], "100454": ["link_0"], "100470": ["link_0"], "100473": ["link_0"], "102358": ["link_0"], "103350": ["link_14"], "103593": ["link_0"], "103886": ["link_0"], "26875": ["link_1"], "100282": [], "100283": ["link_0"], "103351": [], "103361": ["link_0"], "103369": ["link_0"], "103425": [], "103452": ["link_0", "link_5"], "103480": ["link_6"], "103490": [], "103508": [], "103518": ["link_0"], "103521": ["link_0", "link_3"], "103528": [], "103775": ["link_0"], "103776": [], "103778": ["link_0"], "22433": ["link_1"], "23782": [], "26899": [], "27267": [], "101315": ["link_0"], "46966": ["link_0"], "11231": ["link_2"], "102257": ["link_0"], "103297": ["link_1"], "102634": ["link_0", "link_1"], "48379": ["link_0", "link_1"], "103236": ["link_0", "link_1"], "12050": ["link_0", "link_2"], "41086": ["link_0"], "10586": ["link_1"], "46859": ["link_0", "link_1"], "103669": ["link_0"], "45949": ["link_0", "link_1", "link_2"], "102177": [], "47808": ["link_0", "link_1"], "47853": ["link_0", "link_1", "link_2"], "103789": ["link_1"], "47180": ["link_1"], "103015": [], "12248": ["link_0", "link_1"], "45790": ["link_0", "link_1"], "45238": ["link_0", "link_1", "link_2", "link_3"], "41085": ["link_0", "link_1", "link_2", "link_3"], "47651": ["link_1"], "45378": ["link_0", "link_1"], "102707": ["link_2", "link_1"], "101377": [], "45354": ["link_0", "link_1"], "45756": ["link_0", "link_1", "link_2"], "45779": ["link_1"], "45940": ["link_0", "link_1", "link_2"], "45503": ["link_0", "link_1", "link_2"], "103234": ["link_1"], "3971": ["link_0"], "47578": ["link_0", "link_1", "link_2", "link_3"], "11178": ["link_0", "link_1"], "45922": ["link_1"], "47315": ["link_0"], "46408": ["link_0"], "45159": ["link_0", "link_1"], "46616": ["link_0"], "46981": ["link_0", "link_1", "link_2"], "100982": ["link_0", "link_2"], "100521": ["link_0"], "102984": ["link_0", "link_1"], "103307": ["link_1"], "45622": ["link_0", "link_1", "link_2"], "47281": ["link_0"], "101305": ["link_0"], "45594": ["link_0", "link_1"], "46134": ["link_0", "link_1"], "47235": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "102697": ["link_2", "link_3"], "49038": ["link_1", "link_2"], "46655": ["link_0", "link_1"], "100970": [], "9968": ["link_1"], "48013": ["link_0", "link_1"], "102801": ["link_1"], "45689": ["link_0", "link_1"], "46825": ["link_0", "link_1"], "47648": ["link_0", "link_1", "link_2", "link_5"], "10626": ["link_0"], "100561": ["link_0"], "9912": ["link_1"], "45676": ["link_0", "link_1"], "47817": ["link_1"], "10270": ["link_1"], "46893": ["link_0", "link_1", "link_2", "link_3"], "10211": ["link_1"], "45853": ["link_0"], "102667": ["link_2", "link_3"], "102244": ["link_1"], "48700": ["link_0", "link_1"], "102720": ["link_0"], "10697": ["link_1"], "10143": ["link_0", "link_2"], "101320": ["link_0"], "103008": ["link_0"], "47529": ["link_0", "link_1"], "47021": ["link_0"], "41510": ["link_0", "link_1"], "11712": ["link_0", "link_1"], "11242": [], "12038": ["link_1"], "45573": ["link_0", "link_1"], "100599": ["link_0"], "45178": [], "46230": ["link_0", "link_1", "link_2"], "102715": ["link_0"], "47443": ["link_0", "link_1"], "103635": ["link_0"], "103299": [], "46874": ["link_0", "link_1", "link_2"], "47207": ["link_0", "link_1", "link_2", "link_3"], "46641": ["link_0", "link_1", "link_2"], "103276": ["link_1"], "102675": ["link_1"], "7265": ["link_0"], "48746": ["link_0"], "46180": ["link_0", "link_1"], "100885": [], "47944": ["link_0", "link_1", "link_2"], "7304": ["link_1"], "45130": ["link_0"], "10280": ["link_0"], "103321": [], "12477": ["link_0"], "102977": ["link_0"], "11304": ["link_0", "link_1"], "103056": [], "47585": ["link_0", "link_1", "link_2", "link_3", "link_5", "link_6", "link_19"], "45855": ["link_0"], "46699": ["link_0"], "45323": ["link_0"], "102165": ["link_1"], "46179": ["link_0"], "45176": ["link_0"], "11945": ["link_1"], "103052": ["link_0"], "102985": ["link_1"], "103013": ["link_0"], "49062": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45623": ["link_0"], "49140": ["link_1", "link_2", "link_3", "link_4"], "100590": [], "46549": ["link_0", "link_1", "link_2", "link_3"], "100031": ["link_0"], "103222": ["link_0"], "102773": ["link_0"], "101313": ["link_0"], "102714": ["link_0"], "102724": ["link_0"], "102726": ["link_1", "link_0"], "102732": ["link_0"], "102736": ["link_0"], "102739": ["link_1"], "102753": ["link_0"], "102761": ["link_0"], "102763": ["link_0"], "102765": ["link_0"], "102768": [], "102786": ["link_0"], "103201": ["link_0"], "103207": ["link_0"], "103208": [], "100920": [], "102839": ["link_0", "link_2", "link_3", "link_4", "link_5"], "102860": ["link_0"], "102812": [], "102856": ["link_5", "link_6"], "103540": ["link_2"], "103319": ["link_0", "link_1"], "103070": ["link_2"], "103063": ["link_4", "link_5"], "103077": [], "103148": ["link_0"], "102798": [], "102802": ["link_0", "link_1"], "102803": ["link_0"], "102804": ["link_1"], "102805": ["link_1", "link_2"], "102896": ["link_0", "link_1"], "102903": ["link_0", "link_1"], "102905": ["link_1"], "102906": ["link_0", "link_1"], "102981": ["link_0", "link_1"], "103032": [], "103042": ["link_1"], "103044": ["link_0", "link_1"], "103050": ["link_0", "link_1"], "103058": [], "103150": ["link_0"], "103235": ["link_0"], "103238": [], "103242": ["link_0"], "103253": ["link_2"], "103255": ["link_0", "link_1"], "103268": ["link_0"], "103316": [], "103318": ["link_1"], "103320": [], "103323": ["link_0"], "103325": [], "103329": ["link_0"], "103332": ["link_0", "link_1"], "103333": ["link_1"], "103339": ["link_0"], "103340": ["link_0", "link_1"], "103684": ["link_1", "link_2"], "40147": ["link_0", "link_1"], "40417": ["link_0", "link_1", "link_4", "link_2", "link_3"], "41083": ["link_0", "link_1", "link_2", "link_3"], "44781": ["link_0", "link_1", "link_2"], "44817": ["link_0", "link_1", "link_2", "link_3"], "44826": ["link_0", "link_1"], "44853": ["link_0", "link_1", "link_2"], "44962": ["link_0", "link_1", "link_2"], "45092": ["link_0", "link_1", "link_2", "link_3"], "45132": ["link_0", "link_1", "link_2"], "45135": ["link_0", "link_1", "link_2"], "45146": ["link_1", "link_0"], "45162": ["link_0", "link_1"], "45168": ["link_1", "link_0"], "45194": ["link_0", "link_1", "link_2", "link_3"], "45219": ["link_0", "link_1", "link_2", "link_3"], "45235": ["link_0", "link_1"], "45248": ["link_0"], "45261": ["link_0", "link_1", "link_2", "link_3", "link_4"], "45262": ["link_0", "link_1", "link_2", "link_3"], "45271": ["link_2", "link_3", "link_5", "link_0", "link_1"], "45290": ["link_0", "link_1", "link_2"], "45374": ["link_0", "link_1", "link_2", "link_3"], "45413": ["link_0"], "45427": ["link_0", "link_1", "link_2"], "45575": ["link_0", "link_1"], "45612": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45620": ["link_0"], "45632": ["link_0", "link_1", "link_2"], "45636": ["link_0", "link_1", "link_2", "link_3"], "45642": ["link_2"], "45661": ["link_0", "link_1", "link_2"], "45677": ["link_0", "link_1", "link_2", "link_3"], "45687": ["link_0", "link_1"], "45694": ["link_0", "link_1"], "45710": ["link_0", "link_1", "link_2", "link_3"], "45746": ["link_1", "link_2"], "45759": ["link_0", "link_1", "link_2", "link_3"], "45784": ["link_0", "link_1"], "45801": ["link_1", "link_2", "link_3"], "45822": ["link_0"], "45841": ["link_1", "link_2", "link_3"], "45910": ["link_0"], "45948": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45984": ["link_0", "link_1"], "46014": ["link_1", "link_2", "link_3", "link_4"], "46045": ["link_0", "link_1"], "46060": ["link_0", "link_1", "link_2", "link_3"], "46084": ["link_0", "link_1", "link_2", "link_3"], "46107": ["link_1"], "46109": ["link_1", "link_2", "link_3", "link_4", "link_5"], "46123": ["link_0", "link_1", "link_2"], "46127": ["link_1"], "46130": ["link_0", "link_1", "link_2"], "46132": ["link_1"], "46145": ["link_0", "link_1", "link_2", "link_3", "link_5"], "46172": ["link_1", "link_2", "link_3", "link_4", "link_5", "link_6", "link_7", "link_8", "link_9", "link_10", "link_11"], "46236": ["link_1"], "46334": ["link_1", "link_2"], "46380": ["link_1", "link_2", "link_3", "link_4"], "46439": ["link_0"], "46440": ["link_0", "link_1", "link_2"], "46443": ["link_0", "link_1", "link_2", "link_3"], "46452": ["link_0", "link_2"], "46462": ["link_0", "link_1", "link_2"], "46466": ["link_0", "link_1", "link_2", "link_3"], "46537": ["link_0", "link_1", "link_2"], "46544": ["link_0", "link_1", "link_2"], "46556": ["link_0"], "46598": ["link_0", "link_1", "link_2", "link_3"], "46653": ["link_1", "link_2", "link_3"], "46741": ["link_0", "link_1"], "46762": ["link_0", "link_1", "link_2", "link_3"], "46768": ["link_0", "link_1"], "46839": ["link_0", "link_1", "link_2", "link_3"], "46856": ["link_0", "link_1", "link_2", "link_3"], "47024": ["link_0", "link_1"], "47089": ["link_0", "link_1", "link_2", "link_3"], "47168": ["link_0"], "47178": ["link_0", "link_1", "link_2"], "47183": ["link_0", "link_1", "link_2"], "47185": ["link_0", "link_1"], "47233": ["link_0", "link_1", "link_2"], "47252": ["link_0", "link_1", "link_2"], "47254": ["link_0", "link_1"], "47296": ["link_0", "link_1", "link_2"], "47391": ["link_1"], "47438": ["link_0", "link_1", "link_2"], "47565": ["link_1", "link_2"], "47570": ["link_0", "link_1"], "47711": ["link_1", "link_2", "link_3"], "47926": ["link_0", "link_2", "link_3", "link_4", "link_5"], "47963": ["link_1"], "48010": ["link_0", "link_1", "link_2"], "48051": ["link_1", "link_2", "link_3", "link_4", "link_5"], "48063": ["link_2"], "48169": ["link_0", "link_1", "link_2", "link_3", "link_5"], "48253": ["link_0", "link_1", "link_2"], "48258": ["link_0", "link_1", "link_2", "link_3"], "48263": ["link_1", "link_2", "link_3", "link_4"], "48491": ["link_1", "link_2", "link_3"], "48513": ["link_0", "link_1"], "48517": ["link_0", "link_1", "link_2"], "48740": ["link_0", "link_1", "link_2"], "48855": ["link_1", "link_2", "link_0"], "48876": ["link_0", "link_1", "link_2"], "48878": ["link_1", "link_0"], "11818": ["link_0"], "102996": ["link_0"], "11229": ["link_0"], "11259": ["link_0"], "100520": [], "100523": [], "100526": ["link_0"], "100531": ["link_0"], "100532": [], "100568": [], "100579": ["link_0"], "100586": ["link_0"], "100600": ["link_0"], "100609": ["link_0"], "100611": ["link_0"], "100616": ["link_0"], "102255": [], "102263": [], "102314": ["link_0"], "102333": [], "9748": ["link_0"], "9960": ["link_0"], "9992": [], "9996": ["link_0"], "10098": [], "10101": ["link_1"], "10125": ["link_1"], "10213": ["link_1"], "10238": ["link_0"], "10239": ["link_0"], "10243": [], "10248": ["link_1"], "10269": [], "10289": [], "10305": ["link_0"], "10306": ["link_1"], "10383": ["link_1"], "10707": ["link_1"], "11030": [], "11141": ["link_1"], "11156": ["link_1"], "11395": ["link_1"], "11405": ["link_0"], "11406": ["link_1"], "11778": ["link_0"], "11429": ["link_1"], "11477": ["link_1"], "11888": ["link_0"], "10885": ["link_0"], "11854": ["link_0"], "10915": ["link_0"], "11586": ["link_1"], "11581": ["link_0"], "11876": ["link_1"], "10036": ["link_1"], "10068": [], "10144": ["link_0"], "10347": ["link_0", "link_1"], "10373": ["link_0"], "10489": ["link_1", "link_2"], "10612": ["link_1", "link_2"], "10620": ["link_1", "link_2"], "10638": ["link_1", "link_2"], "10627": ["link_1", "link_2"], "10655": ["link_1", "link_2"], "10685": ["link_2"], "10751": ["link_1", "link_2"], "10797": ["link_1"], "10849": ["link_0"], "10867": ["link_0", "link_2"], "10900": ["link_0", "link_1"], "10905": ["link_0"], "10944": ["link_0"], "11211": ["link_0"], "11299": ["link_1", "link_2"], "11550": ["link_0", "link_1"], "11709": ["link_0", "link_2"], "12036": ["link_0", "link_1"], "12042": ["link_1"], "12043": ["link_1", "link_2"], "12054": ["link_0"], "12059": ["link_0", "link_1"], "12066": ["link_1", "link_2"], "12249": ["link_0"], "12250": [], "12252": ["link_0"], "102620": ["link_0"], "102621": ["link_1", "link_3"], "102622": ["link_0"], "102630": ["link_0"], "102645": ["link_2", "link_1"], "102648": ["link_2", "link_3"], "102651": ["link_0"], "102652": ["link_0"], "102654": ["link_1", "link_2"], "102658": ["link_0"], "102663": ["link_1", "link_2"], "102666": ["link_1"], "102668": ["link_2", "link_3"], "102669": ["link_0"], "102670": ["link_2", "link_3"], "102676": ["link_2", "link_3", "link_1"], "102677": ["link_2"], "102687": ["link_0"], "102689": ["link_2"], "102692": ["link_1"], "102694": ["link_0"], "102699": [], "102701": ["link_0", "link_1"], "102703": ["link_2"], "102708": ["link_1"], "103646": [], "102252": ["link_0", "link_1"], "103007": [], "103012": [], "102158": ["link_2"], "101384": [], "102163": [], "103634": ["link_0"], "103647": ["link_1"], "102219": ["link_0", "link_1"], "102992": ["link_1"], "103633": ["link_0"], "102229": ["link_0"], "103639": [], "10584": ["link_0"], "11124": ["link_0"], "11279": [], "11361": ["link_0"], "12447": [], "12483": ["link_0"], "101378": ["link_0"], "102153": ["link_2"], "102154": ["link_2"], "102155": ["link_2"], "102156": ["link_2"], "102173": [], "102181": ["link_1"], "102182": [], "102186": ["link_1"], "102189": ["link_2"], "102192": [], "102259": ["link_2"], "7236": ["link_0"], "7263": ["link_0"], "7292": [], "7310": ["link_0"], "7366": ["link_1"], "7167": ["link_0"], "7128": ["link_0"], "7349": ["link_1", "link_2"], "102990": ["link_1"], "103095": ["link_1"], "103099": [], "103100": [], "103104": ["link_1"], "103111": [], "103113": ["link_1"], "103271": ["link_1"], "103273": ["link_1"], "103280": ["link_1"], "103283": ["link_0", "link_1"], "103292": ["link_1"], "103293": ["link_1"], "103301": [], "103303": ["link_1"], "103305": ["link_1"], "103792": [], "100849": ["link_0"], "100965": ["link_0"], "100980": ["link_1", "link_2"], "103040": [], "103135": [], "35059": ["link_0"], "38516": ["link_0"], "41003": ["link_0", "link_1", "link_2", "link_3"], "41004": ["link_0"], "41452": ["link_0"], "41529": ["link_0"], "45001": ["link_0", "link_1"], "45007": ["link_1"], "45087": ["link_1"], "45091": ["link_1"], "45134": ["link_1"], "45164": ["link_0"], "45166": [], "45173": ["link_0"], "45177": ["link_1"], "45189": ["link_0", "link_1", "link_3"], "45203": ["link_0"], "45212": ["link_0"], "45244": ["link_0"], "45247": ["link_1"], "45249": ["link_0"], "45267": ["link_0"], "45297": [], "45305": ["link_0", "link_1"], "45372": ["link_0"], "45384": ["link_0"], "45385": ["link_1"], "45387": ["link_0", "link_1", "link_2", "link_3"], "45397": ["link_0", "link_1"], "45415": ["link_0"], "45420": ["link_0", "link_1"], "45423": ["link_0", "link_1"], "45443": ["link_1"], "45444": ["link_0", "link_1"], "45448": ["link_0"], "45463": ["link_0", "link_1"], "45504": ["link_1"], "45505": ["link_0", "link_1"], "45516": [], "45523": ["link_1", "link_2"], "45524": ["link_0"], "45526": ["link_0"], "45600": ["link_1"], "45606": ["link_0"], "45621": ["link_0"], "45633": ["link_1"], "45638": ["link_0"], "45645": ["link_0"], "45662": ["link_0", "link_1"], "45667": ["link_1"], "45670": ["link_0", "link_1"], "45671": ["link_1"], "45690": ["link_0"], "45691": ["link_0"], "45693": ["link_1"], "45696": ["link_0", "link_1", "link_2", "link_3"], "45699": ["link_1"], "45717": ["link_1"], "45747": ["link_0", "link_1"], "45749": ["link_0", "link_1", "link_2", "link_3"], "45767": ["link_0", "link_1"], "45776": ["link_0", "link_1"], "45780": ["link_0", "link_1"], "45783": ["link_0"], "45850": ["link_1"], "45908": ["link_1"], "45915": ["link_1"], "45916": ["link_0"], "45936": ["link_0"], "45950": ["link_0"], "45961": ["link_0"], "45963": ["link_0", "link_1"], "45964": ["link_0"], "46002": ["link_0", "link_1", "link_2", "link_3"], "46019": ["link_0", "link_1"], "46029": ["link_0"], "46033": ["link_0"], "46037": ["link_0", "link_1"], "46044": ["link_1"], "46057": ["link_1"], "46092": ["link_0"], "46108": ["link_2", "link_3"], "46117": ["link_1"], "46166": ["link_1", "link_2"], "46197": ["link_0"], "46277": ["link_0", "link_1"], "46401": ["link_0"], "46417": ["link_0"], "46427": ["link_0"], "46430": ["link_1"], "46437": ["link_0", "link_1"], "46456": ["link_0", "link_1"], "46480": ["link_0", "link_1"], "46481": ["link_0", "link_1"], "46490": ["link_0", "link_1"], "46563": ["link_0", "link_1"], "46700": ["link_0", "link_1"], "46732": ["link_0", "link_1"], "46744": ["link_1"], "46787": ["link_1"], "46801": ["link_0", "link_1"], "46889": ["link_0"], "46906": ["link_1"], "46922": ["link_0"], "46944": ["link_0"], "46955": ["link_0", "link_1", "link_2", "link_3"], "47099": ["link_0", "link_1"], "47133": ["link_1"], "47182": ["link_0"], "47187": ["link_0"], "47227": ["link_0", "link_1", "link_2", "link_3"], "47278": ["link_0", "link_1"], "47290": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "47388": ["link_0"], "47419": ["link_1"], "47514": ["link_1"], "47577": ["link_0", "link_1"], "47595": ["link_0", "link_1", "link_2", "link_3"], "47601": ["link_1"], "47613": ["link_0", "link_1"], "47632": ["link_1"], "47669": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "47686": ["link_0"], "47701": ["link_0", "link_1", "link_2", "link_3"], "47729": ["link_1"], "47742": ["link_0"], "47747": ["link_0", "link_1"], "47976": ["link_0", "link_1", "link_2", "link_3"], "48018": ["link_0", "link_1", "link_2", "link_3"], "48023": ["link_1"], "48036": ["link_1"], "48167": ["link_0"], "48177": ["link_0", "link_1"], "48243": ["link_1"], "48271": ["link_0"], "48356": ["link_0", "link_1", "link_2", "link_3"], "48381": ["link_0", "link_2"], "48413": ["link_1"], "48452": ["link_0"], "48467": ["link_1"], "48490": ["link_1"], "48519": ["link_1"], "48623": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "48721": ["link_0"], "49025": ["link_0", "link_1", "link_2", "link_3"], "49042": ["link_0", "link_1"], "49132": ["link_0", "link_1"], "49133": ["link_0", "link_1"], "49182": ["link_0"], "49188": ["link_0", "link_1", "link_2", "link_3", "link_4"], "45403": ["link_1"], "45419": ["link_1"], "45725": ["link_1", "link_2", "link_4", "link_5", "link_6", "link_7", "link_8", "link_9", "link_10", "link_11", "link_12", "link_13"], "47088": ["link_0", "link_1", "link_2"], "12055": ["link_0"], "100732": [], "45213": ["link_0"], "46847": ["link_0", "link_2"], "45332": ["link_0", "link_1"], "45243": ["link_0", "link_1", "link_2", "link_3"], "46120": ["link_0", "link_1"], "46896": ["link_0", "link_1"], "46199": ["link_0", "link_1", "link_4", "link_5"], "101613": [], "103781": [], "26073": ["link_0"], "26387": [], "23472": ["link_0", "link_1"], "20453": ["link_0"], "23807": ["link_0", "link_1", "link_2", "link_3"], "21467": [], "20279": ["link_0"], "32354": ["link_0", "link_1", "link_2"], "19836": ["link_0", "link_1", "link_2"], "19825": [], "25493": ["link_0", "link_1", "link_2"], "22692": ["link_0", "link_1"], "29133": ["link_0", "link_1"], "29557": ["link_0"], "26608": ["link_4", "link_5", "link_6", "link_7"], "33914": ["link_0"], "27619": ["link_0", "link_1"], "30869": ["link_0"], "30663": ["link_0", "link_1", "link_2"], "8877": ["link_1", "link_2"], "8936": []}
diff --git a/scripts/multimodal_diffuser.py b/scripts/multimodal_diffuser.py
new file mode 100644
index 0000000..4fb1187
--- /dev/null
+++ b/scripts/multimodal_diffuser.py
@@ -0,0 +1,180 @@
+# Find multimodal cases in diffusion models
+
+import rpad.pyg.nets.pointnet2 as pnp
+import torch
+import tqdm
+from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation
+from hydra import compose, initialize
+
+from flowbothd.metrics.trajectory import (
+    flow_metrics,
+    normalize_trajectory,
+)
+from flowbothd.models.flow_trajectory_diffuser import (
+    FlowTrajectoryDiffusionModule,
+)
+
+initialize(config_path="../configs", version_base="1.3")
+cfg = compose(config_name="train")
+
+
+ckpt_path = "/home/yishu/flowbothd/logs/train_trajectory/2023-08-31/16-13-10/checkpoints/epoch=394-step=310470-val_loss=0.00-weights-only.ckpt"
+network = pnp.PN2Dense(
+    in_channels=67,
+    out_channels=3,
+    p=pnp.PN2DenseParams(),
+)
+
+model = FlowTrajectoryDiffusionModule(network, cfg.training, cfg.model)
+ckpt = torch.load(ckpt_path)
+model.load_state_dict(ckpt["state_dict"])
+model = model.cuda()
+
+
+import torch_geometric.loader as tgl
+
+from flowbothd.datasets.flow_trajectory_dataset_pyg import (
+    FlowTrajectoryPyGDataset,
+)
+
+datamodule = FlowTrajectoryPyGDataset(
+    root="/home/yishu/datasets/partnet-mobility/raw",
+    split="umpnet-train-test",
+    randomize_joints=True,
+    randomize_camera=True,
+    # batch_size=1,
+    # num_workers=30,
+    # n_proc=2,
+    seed=42,
+    trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model
+)
+val_dataloader = tgl.DataLoader(datamodule, 1, shuffle=False, num_workers=0)
+
+samples = list(enumerate(val_dataloader))
+
+
+@torch.no_grad()
+def diffuse_visual(batch, model):  # 1 sample batch
+    model.eval()
+
+    animation = FlowNetAnimation()
+    pcd = batch.pos.cpu().numpy()
+    mask = batch.mask.cpu().long().numpy()
+
+    fix_noise = torch.randn_like(batch.delta, device="cuda")
+
+    bs = batch.delta.shape[0] // 1200
+    # batch.traj_noise = torch.randn_like(batch.delta, device="cuda")
+    batch.traj_noise = fix_noise
+    # batch.traj_noise = normalize_trajectory(batch.traj_noise)
+    # breakpoint()
+
+    # import time
+    # batch_time = 0
+    # model_time = 0
+    # noise_scheduler_time = 0
+    # self.noise_scheduler_inference.set_timesteps(self.num_inference_timesteps)
+    # print(self.noise_scheduler_inference.timesteps)
+    # for t in self.noise_scheduler_inference.timesteps:
+    for t in model.noise_scheduler.timesteps:
+        # tm = time.time()
+        batch.timesteps = torch.zeros(bs, device=model.device) + t  # Uniform t steps
+        batch.timesteps = batch.timesteps.long()
+        # batch_time += time.time() - tm
+
+        # tm = time.time()
+        model_output = model(batch)  # bs * 1200, traj_len * 3
+        model_output = model_output.reshape(
+            model_output.shape[0], -1, 3
+        )  # bs * 1200, traj_len, 3
+
+        batch.traj_noise = model.noise_scheduler.step(
+            # batch.traj_noise = self.noise_scheduler_inference.step(
+            model_output.reshape(
+                -1, model.sample_size, model_output.shape[1], model_output.shape[2]
+            ),
+            t,
+            batch.traj_noise.reshape(
+                -1, model.sample_size, model_output.shape[1], model_output.shape[2]
+            ),
+        ).prev_sample
+        batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)
+
+        # print(batch.traj_noise)
+        if t % 50 == 0:
+            flow = batch.traj_noise.squeeze().cpu().numpy()
+            # print(flow[mask])
+            # segmented_flow = np.zeros_like(flow, dtype=np.float32)
+            # segmented_flow[mask] = flow[mask]
+            # print("seg", segmented_flow, "flow", flow)
+            animation.add_trace(
+                torch.as_tensor(pcd),
+                # torch.as_tensor([pcd[mask]]),
+                # torch.as_tensor([flow[mask].detach().cpu().numpy()]),
+                torch.as_tensor([pcd]),
+                torch.as_tensor([flow]),
+                "red",
+            )
+
+    f_pred = batch.traj_noise
+    f_pred = normalize_trajectory(f_pred)
+    # largest_mag: float = torch.linalg.norm(
+    #     f_pred, ord=2, dim=-1
+    # ).max()
+    # f_pred = f_pred / (largest_mag + 1e-6)
+
+    # Compute the loss.
+    n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to("cuda")  # type: ignore
+    f_ix = batch.mask.bool()
+    f_target = batch.delta
+    f_target = normalize_trajectory(f_target)
+
+    f_target = f_target.float()
+    # loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+    # Compute some metrics on flow-only regions.
+    rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], batch.delta[f_ix])
+
+    return cos_dist, animation
+
+
+# repeat_times = 10
+# for sample in tqdm.tqdm(samples):
+#     sample_id = sample[0]
+#     sample = sample[1]
+#     batch = sample.cuda()
+#     has_correct = False
+#     correct_dist = 0
+#     has_incorrect = False
+#     incorrect_dist = 10
+#     for _ in range(repeat_times):
+#         cos_dist, animation = diffuse_visual(batch, model)
+#         if cos_dist > 0.5:
+#             correct_dist = max(correct_dist, cos_dist)
+#             has_correct = True
+#             correct_animation = animation
+#         elif cos_dist < -0.5:
+#             incorrect_dist = min(incorrect_dist, cos_dist)
+#             has_incorrect = True
+#             incorrect_animation = animation
+#     if has_correct and has_incorrect:
+#         print(sample_id, correct_dist, incorrect_dist)
+
+repeat_times = 3
+mean_cos_dist = 0
+count = 0
+for sample in tqdm.tqdm(samples):
+    sample_id = sample[0]
+    sample = sample[1]
+    batch = sample.cuda()
+    best_cos_dist = -1
+    count += 1
+    for _ in range(repeat_times):
+        cos_dist, animation = diffuse_visual(batch, model)
+        best_cos_dist = max(best_cos_dist, cos_dist)
+
+    print(best_cos_dist)
+    mean_cos_dist += best_cos_dist
+
+mean_cos_dist /= count
+print(mean_cos_dist)
diff --git a/scripts/train.py b/scripts/train.py
new file mode 100644
index 0000000..fc04d1c
--- /dev/null
+++ b/scripts/train.py
@@ -0,0 +1,433 @@
+import json
+
+import hydra
+import lightning as L
+import omegaconf
+
+# Modules
+import rpad.pyg.nets.pointnet2 as pnp_orig
+import torch
+import wandb
+from lightning.pytorch.callbacks import ModelCheckpoint
+from lightning.pytorch.loggers import WandbLogger
+
+from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule
+from flowbothd.datasets.flowbot import FlowBotDataModule
+from flowbothd.models.flow_diffuser_dgdit import (
+    FlowTrajectoryDiffusionModule_DGDiT,
+)
+from flowbothd.models.flow_diffuser_dit import (
+    FlowTrajectoryDiffusionModule_DiT,
+)
+from flowbothd.models.flow_diffuser_hisdit import (
+    FlowTrajectoryDiffusionModule_HisDiT,
+)
+from flowbothd.models.flow_diffuser_hispndit import (
+    FlowTrajectoryDiffusionModule_HisPNDiT,
+)
+from flowbothd.models.flow_diffuser_pndit import (
+    FlowTrajectoryDiffusionModule_PNDiT,
+)
+
+# Regression Models
+from flowbothd.models.flow_predictor import FlowPredictorTrainingModule
+
+# Diffusion Models
+from flowbothd.models.flow_trajectory_diffuser import (
+    FlowTrajectoryDiffusionModule_PN2,
+)
+from flowbothd.models.flow_trajectory_predictor import (
+    FlowTrajectoryTrainingModule,
+)
+from flowbothd.models.modules.dit_models import (
+    DGDiT,
+    DiT,
+    PN2DiT,
+    PN2HisDiT,
+)
+from flowbothd.models.modules.history_encoder import HistoryEncoder
+from flowbothd.utils.script_utils import (
+    PROJECT_ROOT,
+    LogPredictionSamplesCallback,
+    match_fn,
+)
+
+data_module_class = {
+    "flowbot": FlowBotDataModule,
+    "trajectory": FlowTrajectoryDataModule,
+}
+training_module_class = {
+    "flowbot_pn++": FlowPredictorTrainingModule,
+    "trajectory_pn++": FlowTrajectoryTrainingModule,
+    "trajectory_diffuser_pn++": FlowTrajectoryDiffusionModule_PN2,
+    "trajectory_diffuser_pndit": FlowTrajectoryDiffusionModule_PNDiT,
+    "trajectory_diffuser_dgdit": FlowTrajectoryDiffusionModule_DGDiT,
+    "trajectory_diffuser_dit": FlowTrajectoryDiffusionModule_DiT,
+    # With history
+    "trajectory_diffuser_hisdit": FlowTrajectoryDiffusionModule_HisDiT,
+    "trajectory_diffuser_hispndit": FlowTrajectoryDiffusionModule_HisPNDiT,
+}
+history_network_class = {
+    "encoder": HistoryEncoder,
+}
+
+
+@hydra.main(config_path="../configs", config_name="train", version_base="1.3")
+def main(cfg):
+    print(
+        json.dumps(
+            omegaconf.OmegaConf.to_container(cfg, resolve=True, throw_on_missing=False),
+            sort_keys=True,
+            indent=4,
+        )
+    )
+    ######################################################################
+    # Torch settings.
+    ######################################################################
+
+    # Make deterministic + reproducible.
+    torch.backends.cudnn.deterministic = True
+    torch.backends.cudnn.benchmark = False
+
+    # Since most of us are training on 3090s+, we can use mixed precision.
+    # torch.set_float32_matmul_precision("medium")
+    torch.set_float32_matmul_precision("highest")
+
+    # Global seed for reproducibility.
+    L.seed_everything(cfg.seed)
+
+    ######################################################################
+    # Create the datamodule.
+    # The datamodule is responsible for all the data loading, including
+    # downloading the data, and splitting it into train/val/test.
+    #
+    # This could be swapped out for a different datamodule in-place,
+    # or with an if statement, or by using hydra.instantiate.
+    ######################################################################
+
+    trajectory_len = cfg.training.trajectory_len
+    special_req = cfg.dataset.special_req if cfg.dataset.special_req != "randomly-open" else None
+    if cfg.dataset.dataset_type == "full-dataset":
+        # Full dataset
+        toy_dataset = None
+    else:
+        # Door dataset
+        toy_dataset = {
+            "id": "door-full-new-noslide",
+            "train-train": [
+                "8877",
+                "8893",
+                "8897",
+                "8903",
+                "8919",
+                "8930",
+                "8961",
+                "8997",
+                "9016",
+                # "9032",   # has slide
+                "9035",
+                "9041",
+                "9065",
+                "9070",
+                "9107",
+                "9117",
+                "9127",
+                "9128",
+                "9148",
+                "9164",
+                "9168",
+                "9277",
+                "9280",
+                "9281",
+                "9288",
+                "9386",
+                "9388",
+                "9410",
+            ],
+            "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+            "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+        }
+
+
+    # Create flow dataset
+    datamodule = data_module_class[cfg.dataset.name](
+        root=cfg.dataset.data_dir,
+        batch_size=cfg.training.batch_size,
+        num_workers=cfg.resources.num_workers,
+        n_proc=cfg.resources.n_proc_per_worker,
+        seed=cfg.seed,
+        history="his" in cfg.model.name,
+        randomize_size=cfg.dataset.randomize_size,
+        augmentation=cfg.dataset.augmentation,
+        trajectory_len=trajectory_len,  # Only used when training trajectory model
+        special_req=special_req,  # special_req="fully-closed"
+        n_repeat=200
+        if special_req == "half-half-01"
+        else (50 if special_req is None else 100),
+        toy_dataset=toy_dataset,
+    )
+    train_loader = datamodule.train_dataloader()
+    if "diffuser" in cfg.model.name:
+        cfg.training.train_sample_number = len(train_loader)
+    eval_sample_bsz = 1 if cfg.training.wta else cfg.training.batch_size
+    train_val_loader = datamodule.train_val_dataloader(bsz=eval_sample_bsz)
+
+    if special_req == "half-half" and toy_dataset is not None:  # half-half doors
+        # For half-half training:
+        # - Unseen loader: randomly opened doors
+        # - Validation loader: fully closed doors
+        randomly_opened_datamodule = data_module_class[cfg.dataset.name](
+            root=cfg.dataset.data_dir,
+            batch_size=cfg.training.batch_size,
+            num_workers=cfg.resources.num_workers,
+            n_proc=cfg.resources.n_proc_per_worker,
+            seed=cfg.seed,
+            history="his" in cfg.model.name,
+            randomize_size=cfg.dataset.randomize_size,
+            augmentation=cfg.dataset.augmentation,
+            trajectory_len=trajectory_len,  # Only used when training trajectory model
+            special_req=None,  # special_req="fully-closed"
+            toy_dataset=toy_dataset,
+        )
+        fully_closed_datamodule = data_module_class[cfg.dataset.name](
+            root=cfg.dataset.data_dir,
+            batch_size=cfg.training.batch_size,
+            num_workers=cfg.resources.num_workers,
+            n_proc=cfg.resources.n_proc_per_worker,
+            seed=cfg.seed,
+            history="his" in cfg.model.name,
+            randomize_size=cfg.dataset.randomize_size,
+            augmentation=cfg.dataset.augmentation,
+            trajectory_len=trajectory_len,  # Only used when training trajectory model
+            special_req="fully-closed",  # special_req="fully-closed"
+            toy_dataset=toy_dataset,
+        )
+        val_loader = fully_closed_datamodule.val_dataloader(bsz=eval_sample_bsz)
+        unseen_loader = randomly_opened_datamodule.unseen_dataloader(
+            bsz=eval_sample_bsz
+        )
+    else:  # half-half full dataset
+        val_loader = datamodule.val_dataloader(bsz=eval_sample_bsz)
+        unseen_loader = datamodule.unseen_dataloader(bsz=eval_sample_bsz)
+
+    ######################################################################
+    # Create the network(s) which will be trained by the Training Module.
+    # The network should (ideally) be lightning-independent. This allows
+    # us to use the network in other projects, or in other training
+    # configurations.
+    #
+    # This might get a bit more complicated if we have multiple networks,
+    # but we can just customize the training module and the Hydra configs
+    # to handle that case. No need to over-engineer it. You might
+    # want to put this into a "create_network" function somewhere so train
+    # and eval can be the same.
+    #
+    # If it's a custom network, a good idea is to put the custom network
+    # in `flowbothd.nets.my_net`.
+    ######################################################################
+
+    # Model architecture is dataset-dependent, so we have a helper
+    # function to create the model (while separating out relevant vals).
+
+    if "diffuser" in cfg.model.name:
+        if "pn++" in cfg.model.name:
+            in_channels = 3 * cfg.training.trajectory_len + cfg.model.time_embed_dim
+        else:
+            in_channels = (
+                3 * cfg.training.trajectory_len
+            )  # Will add 3 as input channel in diffuser
+    else:
+        in_channels = 1 if cfg.training.mask_input_channel else 0
+
+    if "pn++" in cfg.model.name:
+        network = pnp_orig.PN2Dense(
+            in_channels=in_channels,
+            out_channels=3 * trajectory_len,
+            p=pnp_orig.PN2DenseParams(),
+        ).cuda()
+    elif "dgdit" in cfg.model.name:
+        network = DGDiT(
+            in_channels=in_channels,
+            depth=5,
+            hidden_size=128,
+            patch_size=1,
+            num_heads=4,
+            n_points=cfg.dataset.n_points,
+        ).cuda()
+    elif "hisdit" in cfg.model.name:
+        network = {
+            "DiT": DiT(
+                in_channels=in_channels + 3 + cfg.model.history_dim,
+                depth=5,
+                hidden_size=128,
+                num_heads=4,
+                learn_sigma=True,
+            ).cuda(),
+            "History": history_network_class[cfg.model.history_model](
+                history_dim=cfg.model.history_dim,
+                history_len=cfg.model.history_len,
+                batch_norm=cfg.model.batch_norm,
+                repeat_dim=True,
+            ).cuda(),
+        }
+    elif "hispndit" in cfg.model.name:
+        network = {
+            "DiT": PN2HisDiT(
+                history_embed_dim=cfg.model.history_dim,
+                in_channels=in_channels,
+                depth=5,
+                hidden_size=128,
+                num_heads=4,
+                # depth=8,
+                # hidden_size=256,
+                # num_heads=4,
+                learn_sigma=True,
+            ).cuda(),
+            "History": history_network_class[cfg.model.history_model](
+                history_dim=cfg.model.history_dim,
+                history_len=cfg.model.history_len,
+                batch_norm=cfg.model.batch_norm,
+                transformer=False,
+                repeat_dim=False,
+            ).cuda(),
+        }
+    elif "pndit" in cfg.model.name:
+        network = PN2DiT(
+            in_channels=in_channels,
+            depth=5,
+            hidden_size=128,
+            patch_size=1,
+            num_heads=4,
+            n_points=cfg.dataset.n_points,
+        )
+    elif "dit" in cfg.model.name:
+        network = DiT(
+            in_channels=in_channels + 3,
+            depth=5,
+            hidden_size=128,
+            num_heads=4,
+            # depth=12,
+            # hidden_size=384,
+            # num_heads=6,
+            learn_sigma=True,
+        ).cuda()
+
+    ######################################################################
+    # Create the training module.
+    # The training module is responsible for all the different parts of
+    # training, including the network, the optimizer, the loss function,
+    # and the logging.
+    ######################################################################
+
+    model = training_module_class[cfg.training.name](
+        network, training_cfg=cfg.training, model_cfg=cfg.model
+    )
+
+    ######################################################################
+    # Set up logging in WandB.
+    # This is a bit complicated, because we want to log the codebase,
+    # the model, and the checkpoints.
+    ######################################################################
+
+    # If no group is provided, then we should create a new one (so we can allocate)
+    # evaluations to this group later.
+    if cfg.wandb.group is None:
+        id = wandb.util.generate_id()
+        group = "experiment-" + id
+    else:
+        group = cfg.wandb.group
+
+    logger = WandbLogger(
+        entity=cfg.wandb.entity,
+        project=cfg.wandb.project,
+        log_model=True,  # Only log the last checkpoint to wandb, and only the LAST model checkpoint.
+        save_dir=cfg.wandb.save_dir,
+        config=omegaconf.OmegaConf.to_container(
+            cfg, resolve=True, throw_on_missing=True
+        ),
+        job_type=cfg.job_type,
+        save_code=True,  # This just has the main script.
+        group=group,
+    )
+
+    ######################################################################
+    # Create the trainer.
+    # The trainer is responsible for running the training loop, and
+    # logging the results.
+    #
+    # There are a few callbacks (which we could customize):
+    # - LogPredictionSamplesCallback: Logs some examples from the dataset,
+    #       and the model's predictions.
+    # - ModelCheckpoint #1: Saves the latest model.
+    # - ModelCheckpoint #2: Saves the best model (according to validation
+    #       loss), and logs it to wandb.
+    ######################################################################
+
+    trainer = L.Trainer(
+        accelerator="gpu",
+        devices=cfg.resources.gpus,
+        # precision="16-mixed",
+        precision="32-true",
+        max_epochs=cfg.training.epochs,
+        logger=logger,
+        check_val_every_n_epoch=cfg.training.check_val_every_n_epoch,
+        callbacks=[
+            # Callback which logs whatever visuals (i.e. dataset examples, preds, etc.) we want.
+            LogPredictionSamplesCallback(
+                logger=logger,
+                eval_per_n_epoch=cfg.training.check_val_every_n_epoch,
+                eval_dataloader_lengths=[
+                    len(val_loader),
+                    len(train_val_loader),
+                    len(unseen_loader),
+                ],
+            ),
+            # This checkpoint callback saves the latest model during training, i.e. so we can resume if it crashes.
+            # It saves everything, and you can load by referencing last.ckpt.
+            ModelCheckpoint(
+                dirpath=cfg.lightning.checkpoint_dir,
+                filename="{epoch}-{step}",
+                monitor="step",
+                mode="max",
+                save_weights_only=False,
+                save_last=True,
+            ),
+            # This checkpoint will get saved to WandB. The Callback mechanism in lightning is poorly designed, so we have to put it last.
+            ModelCheckpoint(
+                dirpath=cfg.lightning.checkpoint_dir,
+                filename="{epoch}-{step}-{val_loss:.2f}-weights-only",
+                monitor="val_wta/rmse" if cfg.training.wta else "val/rmse",
+                mode="min",
+                save_weights_only=True,
+            ),
+        ],
+    )
+
+    ######################################################################
+    # Log the code to wandb.
+    # This is somewhat custom, you'll have to edit this to include whatever
+    # additional files you want, but basically it just logs all the files
+    # in the project root inside dirs, and with extensions.
+    ######################################################################
+
+    # Log the code used to train the model. Make sure not to log too much, because it will be too big.
+    wandb.run.log_code(
+        root=PROJECT_ROOT,
+        include_fn=match_fn(
+            dirs=["configs", "scripts", "src"],
+            extensions=[".py", ".yaml"],
+        ),
+    )
+
+    ######################################################################
+    # Train the model.
+    ######################################################################
+
+    trainer.fit(model, train_loader, [val_loader, unseen_loader])
+
+    # If we want to resume training
+    # trainer.fit(model, train_loader, [val_loader, unseen_loader], ckpt_path='/home/yishu/flowbothd/logs/train_trajectory/2023-09-11/19-01-57/checkpoints/last.ckpt')
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/umpnet_data_split.json b/scripts/umpnet_data_split.json
new file mode 100644
index 0000000..c5d3771
--- /dev/null
+++ b/scripts/umpnet_data_split.json
@@ -0,0 +1,1025 @@
+{
+"train": {
+"Refrigerator": {
+"train": [
+"12250",
+"10849",
+"12059",
+"11299",
+"10655",
+"12066",
+"11211",
+"12036",
+"11709",
+"10685",
+"10867",
+"10612",
+"10068",
+"10905",
+"10144",
+"10797",
+"12043",
+"10373",
+"10944",
+"10627",
+"10900",
+"10036",
+"12249",
+"12054",
+"10347",
+"10620",
+"10751",
+"11550",
+"12252",
+"10638",
+"12042",
+"12055",
+"10489"
+],
+"test": [
+"11304",
+"12248",
+"11178",
+"11712",
+"10586",
+"12050",
+"12038",
+"10143",
+"11231"
+]
+},
+"FoldingChair": {
+"train": [
+"100609",
+"102255",
+"100532",
+"100586",
+"102333",
+"100526",
+"100531",
+"102263",
+"102314",
+"100579",
+"100611",
+"100520",
+"100523",
+"100568",
+"100616",
+"100600"
+],
+"test": [
+"100599",
+"100561",
+"100521",
+"100590"
+]
+},
+"Laptop": {
+"train": [
+"11429",
+"11406",
+"10248",
+"9960",
+"11888",
+"11477",
+"10101",
+"11156",
+"11030",
+"10289",
+"10243",
+"10885",
+"10383",
+"9992",
+"10238",
+"11141",
+"11854",
+"9748",
+"11395",
+"10306",
+"11876",
+"10269",
+"11586",
+"9996",
+"11778",
+"10707",
+"10125",
+"10213",
+"10239",
+"10305",
+"10915",
+"11405",
+"11581",
+"10098"
+],
+"test": [
+"9968",
+"10697",
+"11945",
+"10270",
+"11242",
+"9912",
+"10211",
+"10280",
+"10626"
+]
+},
+"Stapler": {
+"train": [
+"103271",
+"103792",
+"103303",
+"103305",
+"103099",
+"103280",
+"102990",
+"103111",
+"103095",
+"103292",
+"103301",
+"103283",
+"103293",
+"103100",
+"103104",
+"103273",
+"103113"
+],
+"test": [
+"103789",
+"103307",
+"103297",
+"103276",
+"103299"
+]
+},
+"TrashCan": {
+"train": [
+"102192",
+"103639",
+"103012",
+"11259",
+"102155",
+"102992",
+"102153",
+"102189",
+"11361",
+"102154",
+"102996",
+"12447",
+"101378",
+"11124",
+"102173",
+"102156",
+"102229",
+"102181",
+"103634",
+"103007",
+"102186",
+"103633",
+"102163",
+"11279",
+"102182",
+"103646",
+"102158",
+"11229",
+"102259",
+"12483",
+"103647",
+"101384",
+"102252",
+"102219",
+"11818",
+"10584"
+],
+"test": [
+"103013",
+"102244",
+"102257",
+"102165",
+"103635",
+"103008",
+"101377",
+"102177",
+"100732",
+"12477"
+]
+},
+"Microwave": {
+"train": [
+"7236",
+"7128",
+"7349",
+"7310",
+"7366",
+"7167",
+"7263",
+"7292"
+],
+"test": [
+"7304",
+"7265"
+]
+},
+"Toilet": {
+"train": [
+"102663",
+"102670",
+"102676",
+"102687",
+"102669",
+"102652",
+"102699",
+"102677",
+"102708",
+"102658",
+"102622",
+"102666",
+"102668",
+"102689",
+"102648",
+"102703",
+"102654",
+"102621",
+"102630",
+"102645",
+"102692",
+"102620",
+"102701",
+"102694",
+"102651"
+],
+"test": [
+"102697",
+"103234",
+"102634",
+"102707",
+"101320",
+"102667",
+"102675"
+]
+},
+"Window": {
+"train": [
+"103058",
+"102981",
+"103235",
+"103050",
+"103040",
+"103340",
+"102803",
+"103318",
+"103238",
+"103268",
+"102906",
+"103032",
+"103148",
+"103684",
+"103135",
+"103044",
+"103325",
+"102805",
+"103319",
+"103242",
+"102903",
+"103253",
+"103332",
+"103255",
+"102802",
+"103329",
+"103316",
+"103540",
+"103320",
+"103333",
+"103070",
+"102896",
+"103077",
+"102798",
+"102804",
+"103339",
+"103323",
+"102905",
+"103042",
+"103063",
+"103150"
+],
+"test": [
+"102985",
+"103236",
+"103052",
+"103669",
+"102984",
+"102801",
+"103321",
+"100982",
+"102977",
+"103056",
+"103015"
+]
+},
+"StorageFurniture": {
+"train": [
+"48167",
+"45677",
+"47686",
+"45948",
+"45662",
+"45212",
+"49025",
+"45523",
+"45636",
+"45749",
+"48721",
+"46741",
+"45162",
+"45213",
+"44962",
+"48878",
+"41004",
+"46172",
+"46856",
+"45964",
+"46057",
+"46544",
+"46443",
+"47514",
+"45374",
+"46437",
+"47711",
+"45526",
+"48491",
+"46166",
+"45248",
+"48490",
+"46045",
+"45235",
+"48063",
+"45444",
+"46653",
+"47570",
+"46440",
+"46033",
+"45091",
+"44781",
+"41452",
+"45448",
+"45261",
+"46480",
+"45690",
+"46847",
+"47926",
+"46439",
+"45950",
+"48271",
+"45670",
+"45419",
+"46490",
+"46889",
+"44817",
+"45984",
+"45759",
+"47388",
+"45146",
+"46014",
+"45372",
+"45746",
+"48623",
+"46092",
+"45822",
+"44853",
+"45504",
+"46732",
+"46598",
+"48010",
+"46117",
+"46922",
+"48356",
+"46906",
+"45850",
+"47632",
+"47182",
+"46380",
+"47227",
+"45132",
+"47669",
+"45725",
+"46787",
+"45801",
+"45332",
+"45717",
+"48253",
+"48051",
+"44826",
+"45776",
+"45645",
+"45908",
+"48263",
+"47278",
+"47391",
+"45219",
+"45620",
+"46130",
+"45385",
+"45916",
+"45305",
+"46839",
+"46019",
+"45910",
+"48258",
+"45694",
+"45244",
+"41083",
+"46466",
+"46002",
+"45413",
+"46145",
+"45621",
+"45463",
+"47133",
+"45963",
+"46456",
+"45001",
+"47701",
+"48177",
+"45249",
+"45642",
+"46127",
+"49042",
+"45243",
+"48855",
+"47183",
+"46768",
+"45247",
+"45505",
+"47252",
+"45290",
+"47168",
+"45783",
+"49132",
+"49182",
+"47976",
+"46481",
+"46277",
+"46430",
+"46132",
+"47290",
+"46084",
+"47024",
+"45696",
+"48243",
+"45575",
+"48452",
+"46417",
+"45203",
+"46401",
+"45661",
+"46955",
+"48413",
+"46744",
+"45423",
+"45271",
+"46044",
+"46197",
+"48169",
+"45443",
+"46060",
+"46109",
+"46944",
+"45092",
+"47419",
+"46108",
+"46462",
+"46029",
+"45173",
+"41529",
+"41003",
+"48018",
+"48517",
+"45632",
+"45267",
+"40147",
+"47233",
+"47296",
+"45841",
+"48876",
+"45606",
+"45007",
+"45262",
+"45403",
+"45134",
+"45687",
+"46700",
+"48513",
+"47601",
+"45693",
+"40417",
+"45427",
+"45166",
+"47438",
+"45297",
+"46334",
+"45671",
+"48036",
+"45177",
+"45961",
+"48740",
+"35059",
+"47742",
+"49188",
+"47613",
+"46107",
+"45710",
+"49133",
+"45384",
+"48519",
+"45767",
+"46037",
+"45784",
+"47088",
+"45189",
+"46556",
+"46801",
+"38516",
+"45524",
+"45164",
+"46236",
+"45747",
+"46427",
+"48467",
+"45638",
+"47185",
+"45516",
+"46120",
+"45915",
+"46537",
+"47178",
+"47963",
+"47565",
+"46452",
+"47747",
+"45667",
+"47099",
+"45633",
+"45135",
+"47187",
+"45691",
+"45087",
+"45194",
+"47577",
+"47729",
+"45699",
+"45420",
+"47254",
+"48023",
+"46762",
+"45397",
+"45387",
+"48381",
+"45612",
+"46563",
+"45780",
+"45936",
+"47595",
+"45415",
+"45168",
+"46123",
+"45600",
+"47089"
+],
+"test": [
+"45354",
+"46655",
+"45779",
+"45573",
+"48700",
+"47235",
+"46699",
+"48013",
+"45238",
+"47443",
+"47021",
+"45855",
+"47944",
+"48746",
+"46825",
+"45178",
+"47529",
+"46893",
+"47578",
+"47180",
+"46179",
+"47817",
+"46408",
+"47315",
+"46966",
+"45949",
+"46874",
+"46134",
+"41086",
+"47651",
+"45623",
+"49140",
+"45853",
+"45159",
+"45594",
+"47853",
+"46549",
+"45689",
+"45676",
+"45922",
+"47808",
+"45940",
+"45622",
+"49038",
+"49062",
+"41085",
+"46896",
+"46641",
+"45790",
+"45130",
+"46981",
+"47207",
+"46859",
+"46199",
+"47281",
+"41510",
+"48379",
+"47585",
+"45378",
+"47648",
+"46180",
+"45323",
+"45503",
+"46616",
+"45176",
+"46230",
+"45756"
+]
+},
+"Switch": {
+"train": [
+"102856",
+"100980",
+"102860",
+"100849",
+"102812",
+"100965",
+"102839",
+"100920"
+],
+"test": [
+"100970",
+"100885"
+]
+},
+"Kettle": {
+"train": [
+"102761",
+"102739",
+"102765",
+"102786",
+"102773",
+"102724",
+"102753",
+"102768",
+"102736",
+"101313",
+"102726",
+"103222",
+"103201",
+"102714",
+"103208",
+"102732",
+"102763",
+"103207",
+"100031"
+],
+"test": [
+"101315",
+"102720",
+"101305",
+"102715",
+"3971"
+]
+},
+"Toy": {
+"train": [
+"toy1"
+],
+"test": [
+"toy2"
+]
+}
+},
+"test": {
+"Box": {
+"train": [],
+"test": [
+"100664",
+"48492",
+"100214",
+"100141",
+"100174",
+"102379",
+"102377",
+"100243",
+"100129",
+"47645",
+"100247",
+"102456",
+"100189"
+]
+},
+"Phone": {
+"train": [],
+"test": [
+"103886",
+"103350",
+"103593"
+]
+},
+"Dishwasher": {
+"train": [],
+"test": [
+"12428",
+"12565",
+"12592",
+"12579",
+"11622",
+"12561",
+"12596",
+"12065",
+"11700",
+"12558",
+"12605",
+"12594",
+"12349",
+"12559",
+"12617",
+"12654",
+"12092",
+"12590",
+"12484",
+"12563",
+"12606",
+"12614",
+"12531",
+"12480",
+"12414",
+"12580",
+"12560",
+"11826",
+"12597",
+"12540",
+"12553",
+"11661",
+"12536",
+"12587",
+"12562",
+"12583",
+"12259",
+"12621",
+"12552",
+"12543",
+"12530"
+]
+},
+"Safe": {
+"train": [],
+"test": [
+"102278",
+"101593",
+"101619",
+"101605",
+"101591",
+"102316",
+"101603",
+"101612",
+"102387",
+"101594",
+"101604",
+"101613",
+"101564",
+"102389",
+"102311",
+"102381",
+"101363",
+"102318",
+"102418",
+"102384",
+"102380",
+"101584",
+"101599",
+"102309",
+"101579",
+"102423",
+"102301",
+"101611"
+]
+},
+"Oven": {
+"train": [],
+"test": [
+"7220",
+"101943",
+"101908",
+"101917",
+"101924",
+"102044",
+"102018",
+"101946",
+"102019",
+"102001",
+"101773",
+"101909",
+"7201",
+"7120",
+"101930",
+"101971",
+"101940",
+"101931",
+"101947",
+"101808",
+"7187",
+"7179",
+"7332",
+"7290"
+]
+},
+"WashingMachine": {
+"train": [],
+"test": [
+"103425",
+"103518",
+"100283",
+"103521",
+"103508",
+"103351",
+"103361",
+"103480",
+"103452",
+"103528",
+"100282",
+"103369",
+"103778",
+"103781",
+"103776",
+"103775",
+"103490"
+]
+},
+"Table": {
+"train": [],
+"test": [
+"33930",
+"26875",
+"32324",
+"26525",
+"26073",
+"20555",
+"26670",
+"22241",
+"26387",
+"26806",
+"26503",
+"22508",
+"30739",
+"30238",
+"23472",
+"27044",
+"27189",
+"20745",
+"20453",
+"22301",
+"23807",
+"22433",
+"21467",
+"24644",
+"32174",
+"20279",
+"32086",
+"26899",
+"32354",
+"31249",
+"23372",
+"19836",
+"19825",
+"24931",
+"34610",
+"25913",
+"25493",
+"33116",
+"25308",
+"19898",
+"29921",
+"31601",
+"22692",
+"29133",
+"28668",
+"29557",
+"32601",
+"28164",
+"34617",
+"26652",
+"30341",
+"23782",
+"26608",
+"32259",
+"33914",
+"20043",
+"33810",
+"27619",
+"19179",
+"30666",
+"23511",
+"32566",
+"32625",
+"22367",
+"27267",
+"30869",
+"32052",
+"33457",
+"22339",
+"32761",
+"26657",
+"19855",
+"20411",
+"32932",
+"30663",
+"34178",
+"20985"
+]
+},
+"KitchenPot": {
+"train": [],
+"test": [
+"100054",
+"100015",
+"102085",
+"100021",
+"100032",
+"100017",
+"100619",
+"100058",
+"100047",
+"100040",
+"100025",
+"102080",
+"100055",
+"100056",
+"100028",
+"100613",
+"100051",
+"100693",
+"100623",
+"100023",
+"100060",
+"100033",
+"100045",
+"100057",
+"100038"
+]
+},
+"Bucket": {
+"train": [],
+"test": [
+"102358",
+"100473",
+"100438",
+"100470",
+"100454",
+"100444"
+]
+},
+"Door": {
+"train": [],
+"test": [
+"9386",
+"9281",
+"8897",
+"8983",
+"9003",
+"9168",
+"8903",
+"9107",
+"9288",
+"9280",
+"9016",
+"8930",
+"9410",
+"9041",
+"8877",
+"8867",
+"8997",
+"8893",
+"8936",
+"9070",
+"9164",
+"8961",
+"9065",
+"8919",
+"9388",
+"9128",
+"9117"
+]
+}
+}
+}
diff --git a/scripts/umpnet_data_split_new.json b/scripts/umpnet_data_split_new.json
new file mode 100644
index 0000000..bc72f18
--- /dev/null
+++ b/scripts/umpnet_data_split_new.json
@@ -0,0 +1 @@
+{"train": {"Refrigerator": {"train": ["12250", "10849", "12059", "11299", "10655", "12066", "11211", "12036", "11709", "10685", "10867", "10612", "10068", "10905", "10144", "10797", "12043", "10373", "10944", "10627", "10900", "10036", "12249", "12054", "10347", "10620", "10751", "11550", "12252", "10638", "12042", "12055", "10489"], "test": ["11304", "12248", "11178", "11712", "10586", "12050", "12038", "10143", "11231"]}, "FoldingChair": {"train": ["100609", "102255", "100532", "100586", "102333", "100526", "100531", "102263", "102314", "100579", "100611", "100520", "100523", "100568", "100616", "100600"], "test": ["100599", "100561", "100521", "100590"]}, "Laptop": {"train": ["11429", "11406", "10248", "9960", "11888", "11477", "10101", "11156", "11030", "10289", "10243", "10885", "10383", "9992", "10238", "11141", "11854", "9748", "11395", "10306", "11876", "10269", "11586", "9996", "11778", "10707", "10125", "10213", "10239", "10305", "10915", "11405", "11581", "10098"], "test": ["9968", "10697", "11945", "10270", "11242", "9912", "10211", "10280", "10626"]}, "Stapler": {"train": ["103271", "103792", "103303", "103305", "103099", "103280", "102990", "103111", "103095", "103292", "103301", "103283", "103293", "103100", "103104", "103273", "103113"], "test": ["103789", "103307", "103297", "103276", "103299"]}, "TrashCan": {"train": ["102192", "103639", "103012", "11259", "102155", "102992", "102153", "102189", "11361", "102154", "102996", "12447", "101378", "11124", "102173", "102156", "102229", "102181", "103634", "103007", "102186", "103633", "102163", "11279", "102182", "103646", "102158", "11229", "102259", "12483", "103647", "101384", "102252", "102219", "11818", "10584"], "test": ["103013", "102244", "102257", "102165", "103635", "103008", "101377", "102177", "100732", "12477"]}, "Microwave": {"train": ["7236", "7128", "7349", "7310", "7366", "7167", "7263", "7292"], "test": ["7304", "7265"]}, "Toilet": {"train": ["102663", "102670", "102676", "102687", "102669", "102652", "102699", "102677", "102708", "102658", "102622", "102666", "102668", "102689", "102648", "102703", "102654", "102621", "102630", "102645", "102692", "102620", "102701", "102694", "102651"], "test": ["102697", "103234", "102634", "102707", "101320", "102667", "102675"]}, "Window": {"train": ["103058", "102981", "103235", "103050", "103040", "103340", "102803", "103318", "103238", "103268", "102906", "103032", "103148", "103684", "103135", "103044", "103325", "102805", "103319", "103242", "102903", "103253", "103332", "103255", "102802", "103329", "103316", "103540", "103320", "103333", "103070", "102896", "103077", "102798", "102804", "103339", "103323", "102905", "103042", "103063", "103150"], "test": ["102985", "103236", "103052", "103669", "102984", "102801", "103321", "100982", "102977", "103056", "103015"]}, "StorageFurniture": {"train": ["48167", "45677", "47686", "45948", "45662", "45212", "49025", "45523", "45636", "45749", "48721", "46741", "45162", "45213", "44962", "48878", "41004", "46172", "46856", "45964", "46057", "46544", "46443", "47514", "45374", "46437", "47711", "45526", "48491", "46166", "45248", "48490", "46045", "45235", "48063", "45444", "46653", "47570", "46440", "46033", "45091", "44781", "41452", "45448", "45261", "46480", "45690", "46847", "47926", "46439", "45950", "48271", "45670", "45419", "46490", "46889", "44817", "45984", "45759", "47388", "45146", "46014", "45372", "45746", "48623", "46092", "45822", "44853", "45504", "46732", "46598", "48010", "46117", "46922", "48356", "46906", "45850", "47632", "47182", "46380", "47227", "45132", "47669", "45725", "46787", "45801", "45332", "45717", "48253", "48051", "44826", "45776", "45645", "45908", "48263", "47278", "47391", "45219", "45620", "46130", "45385", "45916", "45305", "46839", "46019", "45910", "48258", "45694", "45244", "41083", "46466", "46002", "45413", "46145", "45621", "45463", "47133", "45963", "46456", "45001", "47701", "48177", "45249", "45642", "46127", "49042", "45243", "48855", "47183", "46768", "45247", "45505", "47252", "45290", "47168", "45783", "49132", "49182", "47976", "46481", "46277", "46430", "46132", "47290", "46084", "47024", "45696", "48243", "45575", "48452", "46417", "45203", "46401", "45661", "46955", "48413", "46744", "45423", "45271", "46044", "46197", "48169", "45443", "46060", "46109", "46944", "45092", "47419", "46108", "46462", "46029", "45173", "41529", "41003", "48018", "48517", "45632", "45267", "40147", "47233", "47296", "45841", "48876", "45606", "45007", "45262", "45403", "45134", "45687", "46700", "48513", "47601", "45693", "40417", "45427", "45166", "47438", "45297", "46334", "45671", "48036", "45177", "45961", "48740", "35059", "47742", "49188", "47613", "46107", "45710", "49133", "45384", "48519", "45767", "46037", "45784", "47088", "45189", "46556", "46801", "38516", "45524", "45164", "46236", "45747", "46427", "48467", "45638", "47185", "45516", "46120", "45915", "46537", "47178", "47963", "47565", "46452", "47747", "45667", "47099", "45633", "45135", "47187", "45691", "45087", "45194", "47577", "47729", "45699", "45420", "47254", "48023", "46762", "45397", "45387", "48381", "45612", "46563", "45780", "45936", "47595", "45415", "45168", "46123", "45600", "47089"], "test": ["45354", "46655", "45779", "45573", "48700", "47235", "46699", "48013", "45238", "47443", "47021", "45855", "47944", "48746", "46825", "45178", "47529", "46893", "47578", "47180", "46179", "47817", "46408", "47315", "46966", "45949", "46874", "46134", "41086", "47651", "45623", "49140", "45853", "45159", "45594", "47853", "46549", "45689", "45676", "45922", "47808", "45940", "45622", "49038", "49062", "41085", "46896", "46641", "45790", "45130", "46981", "47207", "46859", "46199", "47281", "41510", "48379", "47585", "45378", "47648", "46180", "45323", "45503", "46616", "45176", "46230", "45756"]}, "Switch": {"train": ["102856", "100980", "102860", "100849", "102812", "100965", "102839", "100920"], "test": ["100970", "100885"]}, "Kettle": {"train": ["102761", "102739", "102765", "102786", "102773", "102724", "102753", "102768", "102736", "101313", "102726", "103222", "103201", "102714", "103208", "102732", "102763", "103207", "100031"], "test": ["101315", "102720", "101305", "102715", "3971"]}, "Box": {"train": ["100247", "100129", "100664", "100174", "100189", "102379", "48492", "100214", "100243", "47645"], "test": ["102456", "102377", "100141"]}, "Phone": {"train": ["103886", "103350"], "test": ["103593"]}, "Dishwasher": {"train": ["12553", "12543", "11661", "12349", "12560", "12092", "12558", "11700", "12536", "11622", "12617", "12596", "12561", "12605", "12590", "12552", "12414", "12540", "12579", "12428", "12614", "12580", "12597", "12594", "12559", "12530", "12606", "12562", "12259", "12592", "12587", "12065"], "test": ["12621", "12484", "12565", "12563", "12480", "12654", "12531", "12583", "11826"]}, "Safe": {"train": ["102389", "102309", "101564", "101579", "101603", "101604", "102423", "102301", "101599", "101593", "101611", "102278", "102316", "102311", "101363", "101605", "101594", "101612", "102384", "101591", "102318", "102387"], "test": ["101613", "102418", "101584", "102381", "101619", "102380"]}, "Oven": {"train": ["101930", "7187", "7220", "101909", "102019", "101924", "101808", "101947", "7201", "101917", "101946", "7120", "102001", "101943", "7332", "7179", "102018", "101971", "101940"], "test": ["101908", "7290", "101931", "101773", "102044"]}, "WashingMachine": {"train": ["103452", "103490", "103369", "103528", "103351", "103776", "100283", "100282", "103425", "103480", "103781", "103361", "103508"], "test": ["103521", "103778", "103518", "103775"]}, "Table": {"train": ["33457", "19825", "22508", "23807", "19855", "28668", "22692", "19836", "32932", "29557", "24644", "20043", "23511", "19898", "29921", "27044", "31249", "23472", "34617", "32324", "33914", "30341", "20453", "23782", "27267", "30739", "26899", "33930", "34610", "25308", "19179", "20411", "25913", "26806", "26670", "31601", "32052", "22339", "33116", "30663", "20745", "20555", "20279", "26608", "32761", "20985", "30869", "32566", "27189", "32174", "26657", "23372", "26652", "24931", "26525", "26503", "22433", "32259", "32601", "26875", "22241"], "test": ["30666", "22301", "26387", "22367", "29133", "26073", "28164", "32086", "25493", "21467", "30238", "33810", "34178", "27619", "32625", "32354"]}, "KitchenPot": {"train": ["100015", "100045", "102080", "100017", "100038", "100023", "100047", "100056", "102085", "100054", "100055", "100058", "100619", "100025", "100032", "100057", "100040", "100060", "100693", "100051"], "test": ["100623", "100021", "100033", "100613", "100028"]}, "Bucket": {"train": ["102358", "100454", "100470", "100438"], "test": ["100444", "100473"]}, "Door": {"train": ["9281", "9107", "8997", "9280", "9070", "8919", "9168", "8983", "9016", "9117", "9041", "9164", "8936", "8897", "9386", "9288", "8903", "9128", "8930", "8961", "9003"], "test": ["9065", "8867", "9410", "9388", "8893", "8877"]}}, "test": {"Door": {"train": [], "test": ["9065", "8867", "9410", "9388", "8893", "8877"]}}}
diff --git a/scripts/umpnet_obj_splits/test_test_split.txt b/scripts/umpnet_obj_splits/test_test_split.txt
new file mode 100644
index 0000000..ad17890
--- /dev/null
+++ b/scripts/umpnet_obj_splits/test_test_split.txt
@@ -0,0 +1,238 @@
+OpenCabinetDrawerGripper_100015_link_0-v0
+OpenCabinetDrawerGripper_100017_link_0-v0
+OpenCabinetDrawerGripper_100021_link_0-v0
+OpenCabinetDrawerGripper_100023_link_0-v0
+OpenCabinetDrawerGripper_100025_link_0-v0
+OpenCabinetDrawerGripper_100028_link_0-v0
+OpenCabinetDrawerGripper_100032_link_0-v0
+OpenCabinetDrawerGripper_100033_link_0-v0
+OpenCabinetDrawerGripper_100038_link_0-v0
+OpenCabinetDrawerGripper_100040_link_0-v0
+OpenCabinetDrawerGripper_100045_link_0-v0
+OpenCabinetDrawerGripper_100047_link_0-v0
+OpenCabinetDrawerGripper_100051_link_0-v0
+OpenCabinetDrawerGripper_100054_link_0-v0
+OpenCabinetDrawerGripper_100055_link_0-v0
+OpenCabinetDrawerGripper_100056_link_0-v0
+OpenCabinetDrawerGripper_100057_link_0-v0
+OpenCabinetDrawerGripper_100058_link_0-v0
+OpenCabinetDrawerGripper_100060_link_0-v0
+OpenCabinetDrawerGripper_100613_link_0-v0
+OpenCabinetDrawerGripper_100619_link_0-v0
+OpenCabinetDrawerGripper_100623_link_0-v0
+OpenCabinetDrawerGripper_100693_link_0-v0
+OpenCabinetDrawerGripper_102080_link_0-v0
+OpenCabinetDrawerGripper_102085_link_0-v0
+OpenCabinetDrawerGripper_19179_link_0-v0
+OpenCabinetDrawerGripper_19855_link_0-v0
+OpenCabinetDrawerGripper_19898_link_0-v0
+OpenCabinetDrawerGripper_20043_link_0-v0
+OpenCabinetDrawerGripper_20411_link_0-v0
+OpenCabinetDrawerGripper_20555_link_0-v0
+OpenCabinetDrawerGripper_20745_link_0-v0
+OpenCabinetDrawerGripper_20985_link_0-v0
+OpenCabinetDrawerGripper_22241_link_0-v0
+OpenCabinetDrawerGripper_22301_link_0-v0
+OpenCabinetDrawerGripper_22339_link_0-v0
+OpenCabinetDrawerGripper_22367_link_0-v0
+OpenCabinetDrawerGripper_22508_link_0-v0
+OpenCabinetDrawerGripper_23372_link_0-v0
+OpenCabinetDrawerGripper_23511_link_0-v0
+OpenCabinetDrawerGripper_24644_link_0-v0
+OpenCabinetDrawerGripper_24931_link_0-v0
+OpenCabinetDrawerGripper_25308_link_0-v0
+OpenCabinetDrawerGripper_25913_link_0-v0
+OpenCabinetDrawerGripper_26503_link_0-v0
+OpenCabinetDrawerGripper_26525_link_0-v0
+OpenCabinetDrawerGripper_26652_link_0-v0
+OpenCabinetDrawerGripper_26657_link_0-v0
+OpenCabinetDrawerGripper_26670_link_0-v0
+OpenCabinetDrawerGripper_26806_link_0-v0
+OpenCabinetDrawerGripper_27044_link_0-v0
+OpenCabinetDrawerGripper_27189_link_0-v0
+OpenCabinetDrawerGripper_28164_link_0-v0
+OpenCabinetDrawerGripper_28668_link_0-v0
+OpenCabinetDrawerGripper_29921_link_0-v0
+OpenCabinetDrawerGripper_30238_link_0-v0
+OpenCabinetDrawerGripper_30341_link_0-v0
+OpenCabinetDrawerGripper_30666_link_0-v0
+OpenCabinetDrawerGripper_30739_link_0-v0
+OpenCabinetDrawerGripper_31249_link_0-v0
+OpenCabinetDrawerGripper_31601_link_0-v0
+OpenCabinetDrawerGripper_32052_link_0-v0
+OpenCabinetDoorGripper_32086_link_0-v0
+OpenCabinetDrawerGripper_32174_link_0-v0
+OpenCabinetDrawerGripper_32259_link_0-v0
+OpenCabinetDrawerGripper_32324_link_0-v0
+OpenCabinetDoorGripper_32566_link_0-v0
+OpenCabinetDrawerGripper_32601_link_0-v0
+OpenCabinetDrawerGripper_32625_link_0-v0
+OpenCabinetDrawerGripper_32761_link_0-v0
+OpenCabinetDrawerGripper_32932_link_0-v0
+OpenCabinetDrawerGripper_33116_link_0-v0
+OpenCabinetDrawerGripper_33457_link_0-v0
+OpenCabinetDrawerGripper_33810_link_0-v0
+OpenCabinetDrawerGripper_33930_link_0-v0
+OpenCabinetDrawerGripper_34178_link_0-v0
+OpenCabinetDrawerGripper_34610_link_0-v0
+OpenCabinetDrawerGripper_34617_link_0-v0
+OpenCabinetDoorGripper_7290_link_0-v0
+OpenCabinetDoorGripper_7220_link_0-v0
+OpenCabinetDoorGripper_7120_link_0-v0
+OpenCabinetDoorGripper_7179_link_0-v0
+OpenCabinetDoorGripper_7187_link_0-v0
+OpenCabinetDoorGripper_7201_link_0-v0
+OpenCabinetDoorGripper_7332_link_0-v0
+OpenCabinetDoorGripper_101773_link_0-v0
+OpenCabinetDoorGripper_101808_link_0-v0
+OpenCabinetDoorGripper_101908_link_0-v0
+OpenCabinetDoorGripper_101909_link_0-v0
+OpenCabinetDoorGripper_101917_link_0-v0
+OpenCabinetDoorGripper_101924_link_0-v0
+OpenCabinetDoorGripper_101930_link_0-v0
+OpenCabinetDoorGripper_101931_link_0-v0
+OpenCabinetDoorGripper_101940_link_0-v0
+OpenCabinetDoorGripper_101943_link_0-v0
+OpenCabinetDoorGripper_101946_link_0-v0
+OpenCabinetDoorGripper_101947_link_0-v0
+OpenCabinetDoorGripper_101971_link_0-v0
+OpenCabinetDoorGripper_102001_link_0-v0
+OpenCabinetDoorGripper_102018_link_0-v0
+OpenCabinetDoorGripper_102019_link_0-v0
+OpenCabinetDoorGripper_102044_link_0-v0
+OpenCabinetDoorGripper_12536_link_0-v0
+OpenCabinetDoorGripper_12617_link_0-v0
+OpenCabinetDoorGripper_12560_link_0-v0
+OpenCabinetDoorGripper_12597_link_0-v0
+OpenCabinetDoorGripper_12552_link_0-v0
+OpenCabinetDoorGripper_12654_link_0-v0
+OpenCabinetDoorGripper_12530_link_0-v0
+OpenCabinetDoorGripper_12565_link_0-v0
+OpenCabinetDoorGripper_12563_link_0-v0
+OpenCabinetDoorGripper_12414_link_0-v0
+OpenCabinetDoorGripper_12558_link_0-v0
+OpenCabinetDoorGripper_12594_link_0-v0
+OpenCabinetDoorGripper_12579_link_0-v0
+OpenCabinetDoorGripper_12621_link_0-v0
+OpenCabinetDoorGripper_11622_link_0-v0
+OpenCabinetDoorGripper_11661_link_0-v0
+OpenCabinetDoorGripper_11700_link_0-v0
+OpenCabinetDoorGripper_11826_link_0-v0
+OpenCabinetDoorGripper_12065_link_0-v0
+OpenCabinetDoorGripper_12092_link_0-v0
+OpenCabinetDoorGripper_12259_link_0-v0
+OpenCabinetDoorGripper_12349_link_0-v0
+OpenCabinetDoorGripper_12428_link_0-v0
+OpenCabinetDoorGripper_12480_link_0-v0
+OpenCabinetDoorGripper_12484_link_0-v0
+OpenCabinetDoorGripper_12531_link_0-v0
+OpenCabinetDoorGripper_12540_link_0-v0
+OpenCabinetDoorGripper_12543_link_0-v0
+OpenCabinetDoorGripper_12553_link_0-v0
+OpenCabinetDoorGripper_12559_link_0-v0
+OpenCabinetDoorGripper_12561_link_0-v0
+OpenCabinetDoorGripper_12562_link_0-v0
+OpenCabinetDoorGripper_12580_link_0-v0
+OpenCabinetDoorGripper_12583_link_0-v0
+OpenCabinetDoorGripper_12587_link_0-v0
+OpenCabinetDoorGripper_12590_link_0-v0
+OpenCabinetDoorGripper_12592_link_0-v0
+OpenCabinetDoorGripper_12596_link_0-v0
+OpenCabinetDoorGripper_12605_link_0-v0
+OpenCabinetDoorGripper_12606_link_0-v0
+OpenCabinetDoorGripper_12614_link_0-v0
+OpenCabinetDoorGripper_9016_link_0-v0
+OpenCabinetDoorGripper_9164_link_0-v0
+OpenCabinetDoorGripper_9041_link_0-v0
+OpenCabinetDoorGripper_9410_link_0-v0
+OpenCabinetDoorGripper_9388_link_0-v0
+OpenCabinetDoorGripper_9107_link_0-v0
+OpenCabinetDoorGripper_9070_link_0-v0
+OpenCabinetDoorGripper_9386_link_0-v0
+OpenCabinetDoorGripper_9168_link_0-v0
+OpenCabinetDoorGripper_8867_link_0-v0
+OpenCabinetDoorGripper_8893_link_0-v0
+OpenCabinetDoorGripper_8897_link_0-v0
+OpenCabinetDoorGripper_8903_link_0-v0
+OpenCabinetDoorGripper_8919_link_0-v0
+OpenCabinetDoorGripper_8930_link_0-v0
+OpenCabinetDoorGripper_8961_link_0-v0
+OpenCabinetDoorGripper_8983_link_0-v0
+OpenCabinetDoorGripper_8997_link_0-v0
+OpenCabinetDoorGripper_9003_link_0-v0
+OpenCabinetDoorGripper_9065_link_0-v0
+OpenCabinetDoorGripper_9117_link_0-v0
+OpenCabinetDoorGripper_9128_link_0-v0
+OpenCabinetDoorGripper_9280_link_0-v0
+OpenCabinetDoorGripper_9281_link_0-v0
+OpenCabinetDoorGripper_9288_link_0-v0
+OpenCabinetDoorGripper_102423_link_0-v0
+OpenCabinetDoorGripper_102278_link_0-v0
+OpenCabinetDoorGripper_102389_link_0-v0
+OpenCabinetDoorGripper_102418_link_0-v0
+OpenCabinetDoorGripper_101363_link_0-v0
+OpenCabinetDoorGripper_101564_link_0-v0
+OpenCabinetDoorGripper_101579_link_0-v0
+OpenCabinetDoorGripper_101584_link_0-v0
+OpenCabinetDoorGripper_101591_link_0-v0
+OpenCabinetDoorGripper_101593_link_0-v0
+OpenCabinetDoorGripper_101594_link_0-v0
+OpenCabinetDoorGripper_101599_link_0-v0
+OpenCabinetDoorGripper_101603_link_0-v0
+OpenCabinetDoorGripper_101604_link_0-v0
+OpenCabinetDoorGripper_101605_link_0-v0
+OpenCabinetDoorGripper_101611_link_0-v0
+OpenCabinetDoorGripper_101612_link_0-v0
+OpenCabinetDoorGripper_101619_link_0-v0
+OpenCabinetDoorGripper_102301_link_0-v0
+OpenCabinetDoorGripper_102309_link_0-v0
+OpenCabinetDoorGripper_102311_link_0-v0
+OpenCabinetDoorGripper_102316_link_0-v0
+OpenCabinetDoorGripper_102318_link_0-v0
+OpenCabinetDoorGripper_102380_link_0-v0
+OpenCabinetDoorGripper_102381_link_0-v0
+OpenCabinetDoorGripper_102384_link_0-v0
+OpenCabinetDoorGripper_102387_link_0-v0
+OpenCabinetDoorGripper_47645_link_0-v0
+OpenCabinetDoorGripper_48492_link_0-v0
+OpenCabinetDoorGripper_100129_link_0-v0
+OpenCabinetDoorGripper_100141_link_0-v0
+OpenCabinetDoorGripper_100174_link_0-v0
+OpenCabinetDoorGripper_100189_link_0-v0
+OpenCabinetDoorGripper_100214_link_0-v0
+OpenCabinetDoorGripper_100243_link_0-v0
+OpenCabinetDoorGripper_100247_link_0-v0
+OpenCabinetDoorGripper_100664_link_0-v0
+OpenCabinetDoorGripper_102377_link_0-v0
+OpenCabinetDoorGripper_102379_link_0-v0
+OpenCabinetDoorGripper_102456_link_0-v0
+OpenCabinetDoorGripper_100438_link_0-v0
+OpenCabinetDoorGripper_100444_link_0-v0
+OpenCabinetDoorGripper_100454_link_0-v0
+OpenCabinetDoorGripper_100470_link_0-v0
+OpenCabinetDoorGripper_100473_link_0-v0
+OpenCabinetDoorGripper_102358_link_0-v0
+OpenCabinetDoorGripper_103350_link_0-v0
+OpenCabinetDoorGripper_103593_link_0-v0
+OpenCabinetDoorGripper_103886_link_0-v0
+OpenCabinetDoorGripper_26875_link_0-v0
+OpenCabinetDoorGripper_100282_link_0-v0
+OpenCabinetDoorGripper_100283_link_0-v0
+OpenCabinetDoorGripper_103351_link_0-v0
+OpenCabinetDoorGripper_103361_link_0-v0
+OpenCabinetDoorGripper_103369_link_0-v0
+OpenCabinetDoorGripper_103425_link_0-v0
+OpenCabinetDoorGripper_103452_link_0-v0
+OpenCabinetDoorGripper_103480_link_0-v0
+OpenCabinetDoorGripper_103490_link_0-v0
+OpenCabinetDoorGripper_103508_link_0-v0
+OpenCabinetDoorGripper_103518_link_0-v0
+OpenCabinetDoorGripper_103521_link_0-v0
+OpenCabinetDoorGripper_103528_link_0-v0
+OpenCabinetDoorGripper_103775_link_0-v0
+OpenCabinetDoorGripper_103776_link_0-v0
+OpenCabinetDoorGripper_103778_link_0-v0
+OpenCabinetDoorGripper_22433_link_0-v0
+OpenCabinetDoorGripper_23782_link_0-v0
+OpenCabinetDoorGripper_26899_link_0-v0
+OpenCabinetDoorGripper_27267_link_0-v0
diff --git a/scripts/umpnet_obj_splits/train_test_split.txt b/scripts/umpnet_obj_splits/train_test_split.txt
new file mode 100644
index 0000000..ed579c6
--- /dev/null
+++ b/scripts/umpnet_obj_splits/train_test_split.txt
@@ -0,0 +1,128 @@
+OpenCabinetDrawerGripper_101315_link_0-v0
+OpenCabinetDoorGripper_46966_link_0-v0
+OpenCabinetDoorGripper_11231_link_0-v0
+OpenCabinetDoorGripper_102257_link_0-v0
+OpenCabinetDoorGripper_103297_link_0-v0
+OpenCabinetDoorGripper_102634_link_0-v0
+OpenCabinetDoorGripper_48379_link_0-v0
+OpenCabinetDrawerGripper_103236_link_0-v0
+OpenCabinetDoorGripper_12050_link_0-v0
+OpenCabinetDoorGripper_41086_link_0-v0
+OpenCabinetDoorGripper_10586_link_0-v0
+OpenCabinetDrawerGripper_46859_link_0-v0
+OpenCabinetDrawerGripper_103669_link_0-v0
+OpenCabinetDrawerGripper_45949_link_0-v0
+OpenCabinetDoorGripper_102177_link_0-v0
+OpenCabinetDoorGripper_47808_link_0-v0
+OpenCabinetDoorGripper_47853_link_0-v0
+OpenCabinetDoorGripper_103789_link_0-v0
+OpenCabinetDoorGripper_47180_link_0-v0
+OpenCabinetDoorGripper_103015_link_0-v0
+OpenCabinetDoorGripper_12248_link_0-v0
+OpenCabinetDrawerGripper_45790_link_0-v0
+OpenCabinetDrawerGripper_45238_link_0-v0
+OpenCabinetDoorGripper_41085_link_0-v0
+OpenCabinetDoorGripper_47651_link_0-v0
+OpenCabinetDoorGripper_45378_link_0-v0
+OpenCabinetDoorGripper_102707_link_0-v0
+OpenCabinetDoorGripper_101377_link_0-v0
+OpenCabinetDoorGripper_45354_link_0-v0
+OpenCabinetDrawerGripper_45756_link_0-v0
+OpenCabinetDoorGripper_45779_link_0-v0
+OpenCabinetDrawerGripper_45940_link_0-v0
+OpenCabinetDoorGripper_45503_link_0-v0
+OpenCabinetDoorGripper_103234_link_0-v0
+OpenCabinetDoorGripper_3971_link_0-v0
+OpenCabinetDrawerGripper_47578_link_0-v0
+OpenCabinetDoorGripper_11178_link_0-v0
+OpenCabinetDoorGripper_45922_link_0-v0
+OpenCabinetDoorGripper_47315_link_0-v0
+OpenCabinetDoorGripper_46408_link_0-v0
+OpenCabinetDoorGripper_45159_link_0-v0
+OpenCabinetDoorGripper_46616_link_0-v0
+OpenCabinetDrawerGripper_46981_link_0-v0
+OpenCabinetDrawerGripper_100982_link_0-v0
+OpenCabinetDoorGripper_100521_link_0-v0
+OpenCabinetDrawerGripper_102984_link_0-v0
+OpenCabinetDoorGripper_103307_link_0-v0
+OpenCabinetDrawerGripper_45622_link_0-v0
+OpenCabinetDoorGripper_47281_link_0-v0
+OpenCabinetDrawerGripper_101305_link_0-v0
+OpenCabinetDoorGripper_45594_link_0-v0
+OpenCabinetDoorGripper_46134_link_0-v0
+OpenCabinetDrawerGripper_47235_link_0-v0
+OpenCabinetDoorGripper_102697_link_0-v0
+OpenCabinetDoorGripper_49038_link_0-v0
+OpenCabinetDoorGripper_46655_link_0-v0
+OpenCabinetDrawerGripper_100970_link_0-v0
+OpenCabinetDoorGripper_9968_link_0-v0
+OpenCabinetDoorGripper_48013_link_0-v0
+OpenCabinetDrawerGripper_102801_link_0-v0
+OpenCabinetDrawerGripper_45689_link_0-v0
+OpenCabinetDoorGripper_46825_link_0-v0
+OpenCabinetDrawerGripper_47648_link_0-v0
+OpenCabinetDoorGripper_10626_link_0-v0
+OpenCabinetDoorGripper_100561_link_0-v0
+OpenCabinetDoorGripper_9912_link_0-v0
+OpenCabinetDrawerGripper_45676_link_0-v0
+OpenCabinetDoorGripper_47817_link_0-v0
+OpenCabinetDoorGripper_10270_link_0-v0
+OpenCabinetDrawerGripper_46893_link_0-v0
+OpenCabinetDoorGripper_10211_link_0-v0
+OpenCabinetDoorGripper_45853_link_0-v0
+OpenCabinetDoorGripper_102667_link_0-v0
+OpenCabinetDoorGripper_102244_link_0-v0
+OpenCabinetDoorGripper_48700_link_0-v0
+OpenCabinetDrawerGripper_102720_link_0-v0
+OpenCabinetDoorGripper_10697_link_0-v0
+OpenCabinetDoorGripper_10143_link_0-v0
+OpenCabinetDoorGripper_101320_link_0-v0
+OpenCabinetDoorGripper_103008_link_0-v0
+OpenCabinetDrawerGripper_47529_link_0-v0
+OpenCabinetDoorGripper_47021_link_0-v0
+OpenCabinetDrawerGripper_41510_link_0-v0
+OpenCabinetDoorGripper_11712_link_0-v0
+OpenCabinetDoorGripper_11242_link_0-v0
+OpenCabinetDoorGripper_12038_link_0-v0
+OpenCabinetDoorGripper_45573_link_0-v0
+OpenCabinetDoorGripper_100599_link_0-v0
+OpenCabinetDoorGripper_45178_link_0-v0
+OpenCabinetDrawerGripper_46230_link_0-v0
+OpenCabinetDrawerGripper_102715_link_0-v0
+OpenCabinetDoorGripper_47443_link_0-v0
+OpenCabinetDoorGripper_103635_link_0-v0
+OpenCabinetDoorGripper_103299_link_0-v0
+OpenCabinetDrawerGripper_46874_link_0-v0
+OpenCabinetDrawerGripper_47207_link_0-v0
+OpenCabinetDrawerGripper_46641_link_0-v0
+OpenCabinetDoorGripper_103276_link_0-v0
+OpenCabinetDoorGripper_102675_link_0-v0
+OpenCabinetDoorGripper_7265_link_0-v0
+OpenCabinetDrawerGripper_48746_link_0-v0
+OpenCabinetDrawerGripper_46180_link_0-v0
+OpenCabinetDoorGripper_100885_link_0-v0
+OpenCabinetDrawerGripper_47944_link_0-v0
+OpenCabinetDoorGripper_7304_link_0-v0
+OpenCabinetDoorGripper_45130_link_0-v0
+OpenCabinetDoorGripper_10280_link_0-v0
+OpenCabinetDrawerGripper_103321_link_0-v0
+OpenCabinetDoorGripper_12477_link_0-v0
+OpenCabinetDrawerGripper_102977_link_0-v0
+OpenCabinetDoorGripper_11304_link_0-v0
+OpenCabinetDoorGripper_103056_link_0-v0
+OpenCabinetDrawerGripper_47585_link_0-v0
+OpenCabinetDrawerGripper_45855_link_0-v0
+OpenCabinetDrawerGripper_46699_link_0-v0
+OpenCabinetDoorGripper_45323_link_0-v0
+OpenCabinetDoorGripper_102165_link_0-v0
+OpenCabinetDoorGripper_46179_link_0-v0
+OpenCabinetDoorGripper_45176_link_0-v0
+OpenCabinetDoorGripper_11945_link_0-v0
+OpenCabinetDrawerGripper_103052_link_0-v0
+OpenCabinetDrawerGripper_102985_link_0-v0
+OpenCabinetDrawerGripper_103013_link_0-v0
+OpenCabinetDoorGripper_49062_link_0-v0
+OpenCabinetDoorGripper_45623_link_0-v0
+OpenCabinetDrawerGripper_49140_link_0-v0
+OpenCabinetDoorGripper_100590_link_0-v0
+OpenCabinetDrawerGripper_46549_link_0-v0
diff --git a/scripts/umpnet_obj_splits/train_train_split.txt b/scripts/umpnet_obj_splits/train_train_split.txt
new file mode 100644
index 0000000..ff977f1
--- /dev/null
+++ b/scripts/umpnet_obj_splits/train_train_split.txt
@@ -0,0 +1,499 @@
+OpenCabinetDrawerGripper_100031_link_0-v0
+OpenCabinetDrawerGripper_103222_link_0-v0
+OpenCabinetDrawerGripper_102773_link_0-v0
+OpenCabinetDrawerGripper_101313_link_0-v0
+OpenCabinetDrawerGripper_102714_link_0-v0
+OpenCabinetDrawerGripper_102724_link_0-v0
+OpenCabinetDrawerGripper_102726_link_0-v0
+OpenCabinetDrawerGripper_102732_link_0-v0
+OpenCabinetDrawerGripper_102736_link_0-v0
+OpenCabinetDrawerGripper_102739_link_0-v0
+OpenCabinetDrawerGripper_102753_link_0-v0
+OpenCabinetDrawerGripper_102761_link_0-v0
+OpenCabinetDrawerGripper_102763_link_0-v0
+OpenCabinetDrawerGripper_102765_link_0-v0
+OpenCabinetDrawerGripper_102768_link_0-v0
+OpenCabinetDrawerGripper_102786_link_0-v0
+OpenCabinetDrawerGripper_103201_link_0-v0
+OpenCabinetDrawerGripper_103207_link_0-v0
+OpenCabinetDrawerGripper_103208_link_0-v0
+OpenCabinetDrawerGripper_100920_link_0-v0
+OpenCabinetDrawerGripper_102839_link_0-v0
+OpenCabinetDrawerGripper_102860_link_0-v0
+OpenCabinetDrawerGripper_102812_link_0-v0
+OpenCabinetDrawerGripper_102856_link_0-v0
+OpenCabinetDrawerGripper_103540_link_0-v0
+OpenCabinetDrawerGripper_103319_link_0-v0
+OpenCabinetDrawerGripper_103070_link_0-v0
+OpenCabinetDrawerGripper_103063_link_0-v0
+OpenCabinetDrawerGripper_103077_link_0-v0
+OpenCabinetDrawerGripper_103148_link_0-v0
+OpenCabinetDrawerGripper_102798_link_0-v0
+OpenCabinetDrawerGripper_102802_link_0-v0
+OpenCabinetDrawerGripper_102803_link_0-v0
+OpenCabinetDrawerGripper_102804_link_0-v0
+OpenCabinetDrawerGripper_102805_link_0-v0
+OpenCabinetDrawerGripper_102896_link_0-v0
+OpenCabinetDrawerGripper_102903_link_0-v0
+OpenCabinetDrawerGripper_102905_link_0-v0
+OpenCabinetDrawerGripper_102906_link_0-v0
+OpenCabinetDrawerGripper_102981_link_0-v0
+OpenCabinetDrawerGripper_103032_link_0-v0
+OpenCabinetDrawerGripper_103042_link_0-v0
+OpenCabinetDrawerGripper_103044_link_0-v0
+OpenCabinetDrawerGripper_103050_link_0-v0
+OpenCabinetDrawerGripper_103058_link_0-v0
+OpenCabinetDrawerGripper_103150_link_0-v0
+OpenCabinetDrawerGripper_103235_link_0-v0
+OpenCabinetDrawerGripper_103238_link_0-v0
+OpenCabinetDrawerGripper_103242_link_0-v0
+OpenCabinetDrawerGripper_103253_link_0-v0
+OpenCabinetDrawerGripper_103255_link_0-v0
+OpenCabinetDrawerGripper_103268_link_0-v0
+OpenCabinetDrawerGripper_103316_link_0-v0
+OpenCabinetDrawerGripper_103318_link_0-v0
+OpenCabinetDrawerGripper_103320_link_0-v0
+OpenCabinetDrawerGripper_103323_link_0-v0
+OpenCabinetDrawerGripper_103325_link_0-v0
+OpenCabinetDrawerGripper_103329_link_0-v0
+OpenCabinetDrawerGripper_103332_link_0-v0
+OpenCabinetDrawerGripper_103333_link_0-v0
+OpenCabinetDrawerGripper_103339_link_0-v0
+OpenCabinetDrawerGripper_103340_link_0-v0
+OpenCabinetDrawerGripper_103684_link_0-v0
+OpenCabinetDrawerGripper_40147_link_0-v0
+OpenCabinetDrawerGripper_40417_link_0-v0
+OpenCabinetDrawerGripper_41083_link_0-v0
+OpenCabinetDrawerGripper_44781_link_0-v0
+OpenCabinetDrawerGripper_44817_link_0-v0
+OpenCabinetDrawerGripper_44826_link_0-v0
+OpenCabinetDrawerGripper_44853_link_0-v0
+OpenCabinetDrawerGripper_44962_link_0-v0
+OpenCabinetDrawerGripper_45092_link_0-v0
+OpenCabinetDrawerGripper_45132_link_0-v0
+OpenCabinetDrawerGripper_45135_link_0-v0
+OpenCabinetDrawerGripper_45146_link_0-v0
+OpenCabinetDrawerGripper_45162_link_0-v0
+OpenCabinetDrawerGripper_45168_link_0-v0
+OpenCabinetDrawerGripper_45194_link_0-v0
+OpenCabinetDrawerGripper_45219_link_0-v0
+OpenCabinetDrawerGripper_45235_link_0-v0
+OpenCabinetDrawerGripper_45248_link_0-v0
+OpenCabinetDrawerGripper_45261_link_0-v0
+OpenCabinetDrawerGripper_45262_link_0-v0
+OpenCabinetDrawerGripper_45271_link_0-v0
+OpenCabinetDrawerGripper_45290_link_0-v0
+OpenCabinetDrawerGripper_45374_link_0-v0
+OpenCabinetDrawerGripper_45413_link_0-v0
+OpenCabinetDrawerGripper_45427_link_0-v0
+OpenCabinetDrawerGripper_45575_link_0-v0
+OpenCabinetDrawerGripper_45612_link_0-v0
+OpenCabinetDrawerGripper_45620_link_0-v0
+OpenCabinetDrawerGripper_45632_link_0-v0
+OpenCabinetDrawerGripper_45636_link_0-v0
+OpenCabinetDrawerGripper_45642_link_0-v0
+OpenCabinetDrawerGripper_45661_link_0-v0
+OpenCabinetDrawerGripper_45677_link_0-v0
+OpenCabinetDrawerGripper_45687_link_0-v0
+OpenCabinetDrawerGripper_45694_link_0-v0
+OpenCabinetDrawerGripper_45710_link_0-v0
+OpenCabinetDrawerGripper_45746_link_0-v0
+OpenCabinetDrawerGripper_45759_link_0-v0
+OpenCabinetDrawerGripper_45784_link_0-v0
+OpenCabinetDrawerGripper_45801_link_0-v0
+OpenCabinetDrawerGripper_45822_link_0-v0
+OpenCabinetDrawerGripper_45841_link_0-v0
+OpenCabinetDrawerGripper_45910_link_0-v0
+OpenCabinetDrawerGripper_45948_link_0-v0
+OpenCabinetDrawerGripper_45984_link_0-v0
+OpenCabinetDrawerGripper_46014_link_0-v0
+OpenCabinetDrawerGripper_46045_link_0-v0
+OpenCabinetDrawerGripper_46060_link_0-v0
+OpenCabinetDrawerGripper_46084_link_0-v0
+OpenCabinetDrawerGripper_46107_link_0-v0
+OpenCabinetDrawerGripper_46109_link_0-v0
+OpenCabinetDrawerGripper_46123_link_0-v0
+OpenCabinetDrawerGripper_46127_link_0-v0
+OpenCabinetDrawerGripper_46130_link_0-v0
+OpenCabinetDrawerGripper_46132_link_0-v0
+OpenCabinetDrawerGripper_46145_link_0-v0
+OpenCabinetDrawerGripper_46172_link_0-v0
+OpenCabinetDrawerGripper_46236_link_0-v0
+OpenCabinetDrawerGripper_46334_link_0-v0
+OpenCabinetDrawerGripper_46380_link_0-v0
+OpenCabinetDrawerGripper_46439_link_0-v0
+OpenCabinetDrawerGripper_46440_link_0-v0
+OpenCabinetDrawerGripper_46443_link_0-v0
+OpenCabinetDrawerGripper_46452_link_0-v0
+OpenCabinetDrawerGripper_46462_link_0-v0
+OpenCabinetDrawerGripper_46466_link_0-v0
+OpenCabinetDrawerGripper_46537_link_0-v0
+OpenCabinetDrawerGripper_46544_link_0-v0
+OpenCabinetDrawerGripper_46556_link_0-v0
+OpenCabinetDrawerGripper_46598_link_0-v0
+OpenCabinetDrawerGripper_46653_link_0-v0
+OpenCabinetDrawerGripper_46741_link_0-v0
+OpenCabinetDrawerGripper_46762_link_0-v0
+OpenCabinetDrawerGripper_46768_link_0-v0
+OpenCabinetDrawerGripper_46839_link_0-v0
+OpenCabinetDrawerGripper_46856_link_0-v0
+OpenCabinetDrawerGripper_47024_link_0-v0
+OpenCabinetDrawerGripper_47089_link_0-v0
+OpenCabinetDrawerGripper_47168_link_0-v0
+OpenCabinetDrawerGripper_47178_link_0-v0
+OpenCabinetDrawerGripper_47183_link_0-v0
+OpenCabinetDrawerGripper_47185_link_0-v0
+OpenCabinetDrawerGripper_47233_link_0-v0
+OpenCabinetDrawerGripper_47252_link_0-v0
+OpenCabinetDrawerGripper_47254_link_0-v0
+OpenCabinetDrawerGripper_47296_link_0-v0
+OpenCabinetDrawerGripper_47391_link_0-v0
+OpenCabinetDrawerGripper_47438_link_0-v0
+OpenCabinetDrawerGripper_47565_link_0-v0
+OpenCabinetDrawerGripper_47570_link_0-v0
+OpenCabinetDrawerGripper_47711_link_0-v0
+OpenCabinetDrawerGripper_47926_link_0-v0
+OpenCabinetDrawerGripper_47963_link_0-v0
+OpenCabinetDrawerGripper_48010_link_0-v0
+OpenCabinetDrawerGripper_48051_link_0-v0
+OpenCabinetDrawerGripper_48063_link_0-v0
+OpenCabinetDrawerGripper_48169_link_0-v0
+OpenCabinetDrawerGripper_48253_link_0-v0
+OpenCabinetDrawerGripper_48258_link_0-v0
+OpenCabinetDrawerGripper_48263_link_0-v0
+OpenCabinetDrawerGripper_48491_link_0-v0
+OpenCabinetDrawerGripper_48513_link_0-v0
+OpenCabinetDrawerGripper_48517_link_0-v0
+OpenCabinetDrawerGripper_48740_link_0-v0
+OpenCabinetDrawerGripper_48855_link_0-v0
+OpenCabinetDrawerGripper_48876_link_0-v0
+OpenCabinetDrawerGripper_48878_link_0-v0
+OpenCabinetDrawerGripper_11818_link_0-v0
+OpenCabinetDrawerGripper_102996_link_0-v0
+OpenCabinetDrawerGripper_11229_link_0-v0
+OpenCabinetDrawerGripper_11259_link_0-v0
+OpenCabinetDoorGripper_100520_link_0-v0
+OpenCabinetDoorGripper_100523_link_0-v0
+OpenCabinetDoorGripper_100526_link_0-v0
+OpenCabinetDoorGripper_100531_link_0-v0
+OpenCabinetDoorGripper_100532_link_0-v0
+OpenCabinetDoorGripper_100568_link_0-v0
+OpenCabinetDoorGripper_100579_link_0-v0
+OpenCabinetDoorGripper_100586_link_0-v0
+OpenCabinetDoorGripper_100600_link_0-v0
+OpenCabinetDoorGripper_100609_link_0-v0
+OpenCabinetDoorGripper_100611_link_0-v0
+OpenCabinetDoorGripper_100616_link_0-v0
+OpenCabinetDoorGripper_102255_link_0-v0
+OpenCabinetDoorGripper_102263_link_0-v0
+OpenCabinetDoorGripper_102314_link_0-v0
+OpenCabinetDoorGripper_102333_link_0-v0
+OpenCabinetDoorGripper_9748_link_0-v0
+OpenCabinetDoorGripper_9960_link_0-v0
+OpenCabinetDoorGripper_9992_link_0-v0
+OpenCabinetDoorGripper_9996_link_0-v0
+OpenCabinetDoorGripper_10098_link_0-v0
+OpenCabinetDoorGripper_10101_link_0-v0
+OpenCabinetDoorGripper_10125_link_0-v0
+OpenCabinetDoorGripper_10213_link_0-v0
+OpenCabinetDoorGripper_10238_link_0-v0
+OpenCabinetDoorGripper_10239_link_0-v0
+OpenCabinetDoorGripper_10243_link_0-v0
+OpenCabinetDoorGripper_10248_link_0-v0
+OpenCabinetDoorGripper_10269_link_0-v0
+OpenCabinetDoorGripper_10289_link_0-v0
+OpenCabinetDoorGripper_10305_link_0-v0
+OpenCabinetDoorGripper_10306_link_0-v0
+OpenCabinetDoorGripper_10383_link_0-v0
+OpenCabinetDoorGripper_10707_link_0-v0
+OpenCabinetDoorGripper_11030_link_0-v0
+OpenCabinetDoorGripper_11141_link_0-v0
+OpenCabinetDoorGripper_11156_link_0-v0
+OpenCabinetDoorGripper_11395_link_0-v0
+OpenCabinetDoorGripper_11405_link_0-v0
+OpenCabinetDoorGripper_11406_link_0-v0
+OpenCabinetDoorGripper_11778_link_0-v0
+OpenCabinetDoorGripper_11429_link_0-v0
+OpenCabinetDoorGripper_11477_link_0-v0
+OpenCabinetDoorGripper_11888_link_0-v0
+OpenCabinetDoorGripper_10885_link_0-v0
+OpenCabinetDoorGripper_11854_link_0-v0
+OpenCabinetDoorGripper_10915_link_0-v0
+OpenCabinetDoorGripper_11586_link_0-v0
+OpenCabinetDoorGripper_11581_link_0-v0
+OpenCabinetDoorGripper_11876_link_0-v0
+OpenCabinetDoorGripper_10036_link_0-v0
+OpenCabinetDoorGripper_10068_link_0-v0
+OpenCabinetDoorGripper_10144_link_0-v0
+OpenCabinetDoorGripper_10347_link_0-v0
+OpenCabinetDoorGripper_10373_link_0-v0
+OpenCabinetDoorGripper_10489_link_0-v0
+OpenCabinetDoorGripper_10612_link_0-v0
+OpenCabinetDoorGripper_10620_link_0-v0
+OpenCabinetDoorGripper_10638_link_0-v0
+OpenCabinetDoorGripper_10627_link_0-v0
+OpenCabinetDoorGripper_10655_link_0-v0
+OpenCabinetDoorGripper_10685_link_0-v0
+OpenCabinetDoorGripper_10751_link_0-v0
+OpenCabinetDoorGripper_10797_link_0-v0
+OpenCabinetDoorGripper_10849_link_0-v0
+OpenCabinetDoorGripper_10867_link_0-v0
+OpenCabinetDoorGripper_10900_link_0-v0
+OpenCabinetDoorGripper_10905_link_0-v0
+OpenCabinetDoorGripper_10944_link_0-v0
+OpenCabinetDoorGripper_11211_link_0-v0
+OpenCabinetDoorGripper_11299_link_0-v0
+OpenCabinetDoorGripper_11550_link_0-v0
+OpenCabinetDoorGripper_11709_link_0-v0
+OpenCabinetDoorGripper_12036_link_0-v0
+OpenCabinetDoorGripper_12042_link_0-v0
+OpenCabinetDoorGripper_12043_link_0-v0
+OpenCabinetDoorGripper_12054_link_0-v0
+OpenCabinetDoorGripper_12059_link_0-v0
+OpenCabinetDoorGripper_12066_link_0-v0
+OpenCabinetDoorGripper_12249_link_0-v0
+OpenCabinetDoorGripper_12250_link_0-v0
+OpenCabinetDoorGripper_12252_link_0-v0
+OpenCabinetDoorGripper_102620_link_0-v0
+OpenCabinetDoorGripper_102621_link_0-v0
+OpenCabinetDoorGripper_102622_link_0-v0
+OpenCabinetDoorGripper_102630_link_0-v0
+OpenCabinetDoorGripper_102645_link_0-v0
+OpenCabinetDoorGripper_102648_link_0-v0
+OpenCabinetDoorGripper_102651_link_0-v0
+OpenCabinetDoorGripper_102652_link_0-v0
+OpenCabinetDoorGripper_102654_link_0-v0
+OpenCabinetDoorGripper_102658_link_0-v0
+OpenCabinetDoorGripper_102663_link_0-v0
+OpenCabinetDoorGripper_102666_link_0-v0
+OpenCabinetDoorGripper_102668_link_0-v0
+OpenCabinetDoorGripper_102669_link_0-v0
+OpenCabinetDoorGripper_102670_link_0-v0
+OpenCabinetDoorGripper_102676_link_0-v0
+OpenCabinetDoorGripper_102677_link_0-v0
+OpenCabinetDoorGripper_102687_link_0-v0
+OpenCabinetDoorGripper_102689_link_0-v0
+OpenCabinetDoorGripper_102692_link_0-v0
+OpenCabinetDoorGripper_102694_link_0-v0
+OpenCabinetDoorGripper_102699_link_0-v0
+OpenCabinetDoorGripper_102701_link_0-v0
+OpenCabinetDoorGripper_102703_link_0-v0
+OpenCabinetDoorGripper_102708_link_0-v0
+OpenCabinetDoorGripper_103646_link_0-v0
+OpenCabinetDoorGripper_102252_link_0-v0
+OpenCabinetDoorGripper_103007_link_0-v0
+OpenCabinetDoorGripper_103012_link_0-v0
+OpenCabinetDoorGripper_102158_link_0-v0
+OpenCabinetDoorGripper_101384_link_0-v0
+OpenCabinetDoorGripper_102163_link_0-v0
+OpenCabinetDoorGripper_103634_link_0-v0
+OpenCabinetDoorGripper_103647_link_0-v0
+OpenCabinetDoorGripper_102219_link_0-v0
+OpenCabinetDoorGripper_102992_link_0-v0
+OpenCabinetDoorGripper_103633_link_0-v0
+OpenCabinetDoorGripper_102229_link_0-v0
+OpenCabinetDoorGripper_103639_link_0-v0
+OpenCabinetDoorGripper_10584_link_0-v0
+OpenCabinetDoorGripper_11124_link_0-v0
+OpenCabinetDoorGripper_11279_link_0-v0
+OpenCabinetDoorGripper_11361_link_0-v0
+OpenCabinetDoorGripper_12447_link_0-v0
+OpenCabinetDoorGripper_12483_link_0-v0
+OpenCabinetDoorGripper_101378_link_0-v0
+OpenCabinetDoorGripper_102153_link_0-v0
+OpenCabinetDoorGripper_102154_link_0-v0
+OpenCabinetDoorGripper_102155_link_0-v0
+OpenCabinetDoorGripper_102156_link_0-v0
+OpenCabinetDoorGripper_102173_link_0-v0
+OpenCabinetDoorGripper_102181_link_0-v0
+OpenCabinetDoorGripper_102182_link_0-v0
+OpenCabinetDoorGripper_102186_link_0-v0
+OpenCabinetDoorGripper_102189_link_0-v0
+OpenCabinetDoorGripper_102192_link_0-v0
+OpenCabinetDoorGripper_102259_link_0-v0
+OpenCabinetDoorGripper_102726_link_0-v0
+OpenCabinetDoorGripper_102739_link_0-v0
+OpenCabinetDoorGripper_7236_link_0-v0
+OpenCabinetDoorGripper_7263_link_0-v0
+OpenCabinetDoorGripper_7292_link_0-v0
+OpenCabinetDoorGripper_7310_link_0-v0
+OpenCabinetDoorGripper_7366_link_0-v0
+OpenCabinetDoorGripper_7167_link_0-v0
+OpenCabinetDoorGripper_7128_link_0-v0
+OpenCabinetDoorGripper_7349_link_0-v0
+OpenCabinetDoorGripper_102990_link_0-v0
+OpenCabinetDoorGripper_103095_link_0-v0
+OpenCabinetDoorGripper_103099_link_0-v0
+OpenCabinetDoorGripper_103100_link_0-v0
+OpenCabinetDoorGripper_103104_link_0-v0
+OpenCabinetDoorGripper_103111_link_0-v0
+OpenCabinetDoorGripper_103113_link_0-v0
+OpenCabinetDoorGripper_103271_link_0-v0
+OpenCabinetDoorGripper_103273_link_0-v0
+OpenCabinetDoorGripper_103280_link_0-v0
+OpenCabinetDoorGripper_103283_link_0-v0
+OpenCabinetDoorGripper_103292_link_0-v0
+OpenCabinetDoorGripper_103293_link_0-v0
+OpenCabinetDoorGripper_103301_link_0-v0
+OpenCabinetDoorGripper_103303_link_0-v0
+OpenCabinetDoorGripper_103305_link_0-v0
+OpenCabinetDoorGripper_103792_link_0-v0
+OpenCabinetDoorGripper_100849_link_0-v0
+OpenCabinetDoorGripper_100965_link_0-v0
+OpenCabinetDoorGripper_100980_link_0-v0
+OpenCabinetDoorGripper_103040_link_0-v0
+OpenCabinetDoorGripper_103135_link_0-v0
+OpenCabinetDoorGripper_35059_link_0-v0
+OpenCabinetDoorGripper_38516_link_0-v0
+OpenCabinetDoorGripper_41003_link_0-v0
+OpenCabinetDoorGripper_41004_link_0-v0
+OpenCabinetDoorGripper_41452_link_0-v0
+OpenCabinetDoorGripper_41529_link_0-v0
+OpenCabinetDoorGripper_45001_link_0-v0
+OpenCabinetDoorGripper_45007_link_0-v0
+OpenCabinetDoorGripper_45087_link_0-v0
+OpenCabinetDoorGripper_45091_link_0-v0
+OpenCabinetDoorGripper_45134_link_0-v0
+OpenCabinetDoorGripper_45164_link_0-v0
+OpenCabinetDoorGripper_45166_link_0-v0
+OpenCabinetDoorGripper_45173_link_0-v0
+OpenCabinetDoorGripper_45177_link_0-v0
+OpenCabinetDoorGripper_45189_link_0-v0
+OpenCabinetDoorGripper_45203_link_0-v0
+OpenCabinetDoorGripper_45212_link_0-v0
+OpenCabinetDoorGripper_45244_link_0-v0
+OpenCabinetDoorGripper_45247_link_0-v0
+OpenCabinetDoorGripper_45249_link_0-v0
+OpenCabinetDoorGripper_45267_link_0-v0
+OpenCabinetDoorGripper_45297_link_0-v0
+OpenCabinetDoorGripper_45305_link_0-v0
+OpenCabinetDoorGripper_45372_link_0-v0
+OpenCabinetDoorGripper_45384_link_0-v0
+OpenCabinetDoorGripper_45385_link_0-v0
+OpenCabinetDoorGripper_45387_link_0-v0
+OpenCabinetDoorGripper_45397_link_0-v0
+OpenCabinetDoorGripper_45415_link_0-v0
+OpenCabinetDoorGripper_45420_link_0-v0
+OpenCabinetDoorGripper_45423_link_0-v0
+OpenCabinetDoorGripper_45443_link_0-v0
+OpenCabinetDoorGripper_45444_link_0-v0
+OpenCabinetDoorGripper_45448_link_0-v0
+OpenCabinetDoorGripper_45463_link_0-v0
+OpenCabinetDoorGripper_45504_link_0-v0
+OpenCabinetDoorGripper_45505_link_0-v0
+OpenCabinetDoorGripper_45516_link_0-v0
+OpenCabinetDoorGripper_45523_link_0-v0
+OpenCabinetDoorGripper_45524_link_0-v0
+OpenCabinetDoorGripper_45526_link_0-v0
+OpenCabinetDoorGripper_45600_link_0-v0
+OpenCabinetDoorGripper_45606_link_0-v0
+OpenCabinetDoorGripper_45621_link_0-v0
+OpenCabinetDoorGripper_45633_link_0-v0
+OpenCabinetDoorGripper_45638_link_0-v0
+OpenCabinetDoorGripper_45645_link_0-v0
+OpenCabinetDoorGripper_45662_link_0-v0
+OpenCabinetDoorGripper_45667_link_0-v0
+OpenCabinetDoorGripper_45670_link_0-v0
+OpenCabinetDoorGripper_45671_link_0-v0
+OpenCabinetDoorGripper_45690_link_0-v0
+OpenCabinetDoorGripper_45691_link_0-v0
+OpenCabinetDoorGripper_45693_link_0-v0
+OpenCabinetDoorGripper_45696_link_0-v0
+OpenCabinetDoorGripper_45699_link_0-v0
+OpenCabinetDoorGripper_45717_link_0-v0
+OpenCabinetDoorGripper_45747_link_0-v0
+OpenCabinetDoorGripper_45749_link_0-v0
+OpenCabinetDoorGripper_45767_link_0-v0
+OpenCabinetDoorGripper_45776_link_0-v0
+OpenCabinetDoorGripper_45780_link_0-v0
+OpenCabinetDoorGripper_45783_link_0-v0
+OpenCabinetDoorGripper_45850_link_0-v0
+OpenCabinetDoorGripper_45908_link_0-v0
+OpenCabinetDoorGripper_45915_link_0-v0
+OpenCabinetDoorGripper_45916_link_0-v0
+OpenCabinetDoorGripper_45936_link_0-v0
+OpenCabinetDoorGripper_45950_link_0-v0
+OpenCabinetDoorGripper_45961_link_0-v0
+OpenCabinetDoorGripper_45963_link_0-v0
+OpenCabinetDoorGripper_45964_link_0-v0
+OpenCabinetDoorGripper_46002_link_0-v0
+OpenCabinetDoorGripper_46019_link_0-v0
+OpenCabinetDoorGripper_46029_link_0-v0
+OpenCabinetDoorGripper_46033_link_0-v0
+OpenCabinetDoorGripper_46037_link_0-v0
+OpenCabinetDoorGripper_46044_link_0-v0
+OpenCabinetDoorGripper_46057_link_0-v0
+OpenCabinetDoorGripper_46092_link_0-v0
+OpenCabinetDoorGripper_46108_link_0-v0
+OpenCabinetDoorGripper_46117_link_0-v0
+OpenCabinetDoorGripper_46166_link_0-v0
+OpenCabinetDoorGripper_46197_link_0-v0
+OpenCabinetDoorGripper_46277_link_0-v0
+OpenCabinetDoorGripper_46401_link_0-v0
+OpenCabinetDoorGripper_46417_link_0-v0
+OpenCabinetDoorGripper_46427_link_0-v0
+OpenCabinetDoorGripper_46430_link_0-v0
+OpenCabinetDoorGripper_46437_link_0-v0
+OpenCabinetDoorGripper_46456_link_0-v0
+OpenCabinetDoorGripper_46480_link_0-v0
+OpenCabinetDoorGripper_46481_link_0-v0
+OpenCabinetDoorGripper_46490_link_0-v0
+OpenCabinetDoorGripper_46563_link_0-v0
+OpenCabinetDoorGripper_46700_link_0-v0
+OpenCabinetDoorGripper_46732_link_0-v0
+OpenCabinetDoorGripper_46744_link_0-v0
+OpenCabinetDoorGripper_46787_link_0-v0
+OpenCabinetDoorGripper_46801_link_0-v0
+OpenCabinetDoorGripper_46889_link_0-v0
+OpenCabinetDoorGripper_46906_link_0-v0
+OpenCabinetDoorGripper_46922_link_0-v0
+OpenCabinetDoorGripper_46944_link_0-v0
+OpenCabinetDoorGripper_46955_link_0-v0
+OpenCabinetDoorGripper_47099_link_0-v0
+OpenCabinetDoorGripper_47133_link_0-v0
+OpenCabinetDoorGripper_47182_link_0-v0
+OpenCabinetDoorGripper_47187_link_0-v0
+OpenCabinetDoorGripper_47227_link_0-v0
+OpenCabinetDoorGripper_47278_link_0-v0
+OpenCabinetDoorGripper_47290_link_0-v0
+OpenCabinetDoorGripper_47388_link_0-v0
+OpenCabinetDoorGripper_47419_link_0-v0
+OpenCabinetDoorGripper_47514_link_0-v0
+OpenCabinetDoorGripper_47577_link_0-v0
+OpenCabinetDoorGripper_47595_link_0-v0
+OpenCabinetDoorGripper_47601_link_0-v0
+OpenCabinetDoorGripper_47613_link_0-v0
+OpenCabinetDoorGripper_47632_link_0-v0
+OpenCabinetDoorGripper_47669_link_0-v0
+OpenCabinetDoorGripper_47686_link_0-v0
+OpenCabinetDoorGripper_47701_link_0-v0
+OpenCabinetDoorGripper_47729_link_0-v0
+OpenCabinetDoorGripper_47742_link_0-v0
+OpenCabinetDoorGripper_47747_link_0-v0
+OpenCabinetDoorGripper_47976_link_0-v0
+OpenCabinetDoorGripper_48018_link_0-v0
+OpenCabinetDoorGripper_48023_link_0-v0
+OpenCabinetDoorGripper_48036_link_0-v0
+OpenCabinetDoorGripper_48167_link_0-v0
+OpenCabinetDoorGripper_48177_link_0-v0
+OpenCabinetDoorGripper_48243_link_0-v0
+OpenCabinetDoorGripper_48271_link_0-v0
+OpenCabinetDoorGripper_48356_link_0-v0
+OpenCabinetDoorGripper_48381_link_0-v0
+OpenCabinetDoorGripper_48413_link_0-v0
+OpenCabinetDoorGripper_48452_link_0-v0
+OpenCabinetDoorGripper_48467_link_0-v0
+OpenCabinetDoorGripper_48490_link_0-v0
+OpenCabinetDoorGripper_48519_link_0-v0
+OpenCabinetDoorGripper_48623_link_0-v0
+OpenCabinetDoorGripper_48721_link_0-v0
+OpenCabinetDoorGripper_49025_link_0-v0
+OpenCabinetDoorGripper_49042_link_0-v0
+OpenCabinetDoorGripper_49132_link_0-v0
+OpenCabinetDoorGripper_49133_link_0-v0
+OpenCabinetDoorGripper_49182_link_0-v0
+OpenCabinetDoorGripper_49188_link_0-v0
+OpenCabinetDoorGripper_45403_link_0-v0
+OpenCabinetDoorGripper_45419_link_0-v0
+OpenCabinetDoorGripper_45725_link_0-v0
+OpenCabinetDoorGripper_47088_link_0-v0
diff --git a/scripts/umpnet_object_list.json b/scripts/umpnet_object_list.json
new file mode 100644
index 0000000..cb511ff
--- /dev/null
+++ b/scripts/umpnet_object_list.json
@@ -0,0 +1 @@
+{"100015": ["link_0"], "100017": ["link_0"], "100021": ["link_0"], "100023": ["link_0"], "100025": ["link_0"], "100028": ["link_0"], "100032": ["link_0"], "100033": ["link_0"], "100038": ["link_0"], "100040": ["link_0"], "100045": ["link_0"], "100047": ["link_0"], "100051": ["link_0"], "100054": ["link_0"], "100055": ["link_0"], "100056": ["link_0"], "100057": ["link_0"], "100058": ["link_0"], "100060": ["link_0"], "100613": ["link_0"], "100619": ["link_0"], "100623": ["link_0"], "100693": ["link_0"], "102080": ["link_0"], "102085": ["link_0"], "19179": ["link_0"], "19855": ["link_0"], "19898": ["link_0"], "20043": ["link_0"], "20411": ["link_0"], "20555": ["link_0"], "20745": ["link_0"], "20985": ["link_0"], "22241": ["link_0"], "22301": ["link_0"], "22339": ["link_0"], "22367": ["link_0"], "22508": ["link_0"], "23372": ["link_0"], "23511": ["link_0"], "24644": ["link_0"], "24931": ["link_0"], "25308": ["link_0"], "25913": ["link_0"], "26503": ["link_0"], "26525": ["link_0"], "26652": ["link_0"], "26657": ["link_0"], "26670": ["link_0"], "26806": ["link_0"], "27044": ["link_0"], "27189": ["link_0"], "28164": ["link_0"], "28668": ["link_0"], "29921": ["link_0"], "30238": ["link_0"], "30341": ["link_0"], "30666": ["link_0"], "30739": ["link_0"], "31249": ["link_0"], "31601": ["link_0"], "32052": ["link_0"], "32086": ["link_0"], "32174": ["link_0"], "32259": ["link_0"], "32324": ["link_0"], "32566": ["link_0"], "32601": ["link_0"], "32625": ["link_0"], "32761": ["link_0"], "32932": ["link_0"], "33116": ["link_0"], "33457": ["link_0"], "33810": ["link_0"], "33930": ["link_0"], "34178": ["link_0"], "34610": ["link_0"], "34617": ["link_0"], "7290": ["link_0"], "7220": ["link_0"], "7120": ["link_0"], "7179": ["link_0"], "7187": ["link_0"], "7201": ["link_0"], "7332": ["link_0"], "101773": ["link_0"], "101808": ["link_0"], "101908": ["link_0"], "101909": ["link_0"], "101917": ["link_0"], "101924": ["link_0"], "101930": ["link_0"], "101931": ["link_0"], "101940": ["link_0"], "101943": ["link_0"], "101946": ["link_0"], "101947": ["link_0"], "101971": ["link_0"], "102001": ["link_0"], "102018": ["link_0"], "102019": ["link_0"], "102044": ["link_0"], "12536": ["link_0"], "12617": ["link_0"], "12560": ["link_0"], "12597": ["link_0"], "12552": ["link_0"], "12654": ["link_0"], "12530": ["link_0"], "12565": ["link_0"], "12563": ["link_0"], "12414": ["link_0"], "12558": ["link_0"], "12594": ["link_0"], "12579": ["link_0"], "12621": ["link_0"], "11622": ["link_0"], "11661": ["link_0"], "11700": ["link_0"], "11826": ["link_0"], "12065": ["link_0"], "12092": ["link_0"], "12259": ["link_0"], "12349": ["link_0"], "12428": ["link_0"], "12480": ["link_0"], "12484": ["link_0"], "12531": ["link_0"], "12540": ["link_0"], "12543": ["link_0"], "12553": ["link_0"], "12559": ["link_0"], "12561": ["link_0"], "12562": ["link_0"], "12580": ["link_0"], "12583": ["link_0"], "12587": ["link_0"], "12590": ["link_0"], "12592": ["link_0"], "12596": ["link_0"], "12605": ["link_0"], "12606": ["link_0"], "12614": ["link_0"], "9016": ["link_0"], "9164": ["link_0"], "9041": ["link_0"], "9410": ["link_0"], "9388": ["link_0"], "9107": ["link_0"], "9070": ["link_0"], "9386": ["link_0"], "9168": ["link_0"], "8867": ["link_0"], "8893": ["link_0"], "8897": ["link_0"], "8903": ["link_0"], "8919": ["link_0"], "8930": ["link_0"], "8961": ["link_0"], "8983": ["link_0"], "8997": ["link_0"], "9003": ["link_0"], "9065": ["link_0"], "9117": ["link_0"], "9128": ["link_0"], "9280": ["link_0"], "9281": ["link_0"], "9288": ["link_0"], "102423": ["link_0"], "102278": ["link_0"], "102389": ["link_0"], "102418": ["link_0"], "101363": ["link_0"], "101564": ["link_0"], "101579": ["link_0"], "101584": ["link_0"], "101591": ["link_0"], "101593": ["link_0"], "101594": ["link_0"], "101599": ["link_0"], "101603": ["link_0"], "101604": ["link_0"], "101605": ["link_0"], "101611": ["link_0"], "101612": ["link_0"], "101619": ["link_0"], "102301": ["link_0"], "102309": ["link_0"], "102311": ["link_0"], "102316": ["link_0"], "102318": ["link_0"], "102380": ["link_0"], "102381": ["link_0"], "102384": ["link_0"], "102387": ["link_0"], "47645": ["link_0"], "48492": ["link_0"], "100129": ["link_0"], "100141": ["link_0"], "100174": ["link_0"], "100189": ["link_0"], "100214": ["link_0"], "100243": ["link_0"], "100247": ["link_0"], "100664": ["link_0"], "102377": ["link_0"], "102379": ["link_0"], "102456": ["link_0"], "100438": ["link_0"], "100444": ["link_0"], "100454": ["link_0"], "100470": ["link_0"], "100473": ["link_0"], "102358": ["link_0"], "103350": ["link_0"], "103593": ["link_0"], "103886": ["link_0"], "26875": ["link_0"], "100282": ["link_0"], "100283": ["link_0"], "103351": ["link_0"], "103361": ["link_0"], "103369": ["link_0"], "103425": ["link_0"], "103452": ["link_0"], "103480": ["link_0"], "103490": ["link_0"], "103508": ["link_0"], "103518": ["link_0"], "103521": ["link_0"], "103528": ["link_0"], "103775": ["link_0"], "103776": ["link_0"], "103778": ["link_0"], "22433": ["link_0"], "23782": ["link_0"], "26899": ["link_0"], "27267": ["link_0"], "101315": ["link_0"], "46966": ["link_0"], "11231": ["link_0"], "102257": ["link_0"], "103297": ["link_0"], "102634": ["link_0"], "48379": ["link_0"], "103236": ["link_0"], "12050": ["link_0"], "41086": ["link_0"], "10586": ["link_0"], "46859": ["link_0"], "103669": ["link_0"], "45949": ["link_0"], "102177": ["link_0"], "47808": ["link_0"], "47853": ["link_0"], "103789": ["link_0"], "47180": ["link_0"], "103015": ["link_0"], "12248": ["link_0"], "45790": ["link_0"], "45238": ["link_0"], "41085": ["link_0"], "47651": ["link_0"], "45378": ["link_0"], "102707": ["link_0"], "101377": ["link_0"], "45354": ["link_0"], "45756": ["link_0"], "45779": ["link_0"], "45940": ["link_0"], "45503": ["link_0"], "103234": ["link_0"], "3971": ["link_0"], "47578": ["link_0"], "11178": ["link_0"], "45922": ["link_0"], "47315": ["link_0"], "46408": ["link_0"], "45159": ["link_0"], "46616": ["link_0"], "46981": ["link_0"], "100982": ["link_0"], "100521": ["link_0"], "102984": ["link_0"], "103307": ["link_0"], "45622": ["link_0"], "47281": ["link_0"], "101305": ["link_0"], "45594": ["link_0"], "46134": ["link_0"], "47235": ["link_0"], "102697": ["link_0"], "49038": ["link_0"], "46655": ["link_0"], "100970": ["link_0"], "9968": ["link_0"], "48013": ["link_0"], "102801": ["link_0"], "45689": ["link_0"], "46825": ["link_0"], "47648": ["link_0"], "10626": ["link_0"], "100561": ["link_0"], "9912": ["link_0"], "45676": ["link_0"], "47817": ["link_0"], "10270": ["link_0"], "46893": ["link_0"], "10211": ["link_0"], "45853": ["link_0"], "102667": ["link_0"], "102244": ["link_0"], "48700": ["link_0"], "102720": ["link_0"], "10697": ["link_0"], "10143": ["link_0"], "101320": ["link_0"], "103008": ["link_0"], "47529": ["link_0"], "47021": ["link_0"], "41510": ["link_0"], "11712": ["link_0"], "11242": ["link_0"], "12038": ["link_0"], "45573": ["link_0"], "100599": ["link_0"], "45178": ["link_0"], "46230": ["link_0"], "102715": ["link_0"], "47443": ["link_0"], "103635": ["link_0"], "103299": ["link_0"], "46874": ["link_0"], "47207": ["link_0"], "46641": ["link_0"], "103276": ["link_0"], "102675": ["link_0"], "7265": ["link_0"], "48746": ["link_0"], "46180": ["link_0"], "100885": ["link_0"], "47944": ["link_0"], "7304": ["link_0"], "45130": ["link_0"], "10280": ["link_0"], "103321": ["link_0"], "12477": ["link_0"], "102977": ["link_0"], "11304": ["link_0"], "103056": ["link_0"], "47585": ["link_0"], "45855": ["link_0"], "46699": ["link_0"], "45323": ["link_0"], "102165": ["link_0"], "46179": ["link_0"], "45176": ["link_0"], "11945": ["link_0"], "103052": ["link_0"], "102985": ["link_0"], "103013": ["link_0"], "49062": ["link_0"], "45623": ["link_0"], "49140": ["link_0"], "100590": ["link_0"], "46549": ["link_0"], "100031": ["link_0"], "103222": ["link_0"], "102773": ["link_0"], "101313": ["link_0"], "102714": ["link_0"], "102724": ["link_0"], "102726": ["link_0", "link_0"], "102732": ["link_0"], "102736": ["link_0"], "102739": ["link_0", "link_0"], "102753": ["link_0"], "102761": ["link_0"], "102763": ["link_0"], "102765": ["link_0"], "102768": ["link_0"], "102786": ["link_0"], "103201": ["link_0"], "103207": ["link_0"], "103208": ["link_0"], "100920": ["link_0"], "102839": ["link_0"], "102860": ["link_0"], "102812": ["link_0"], "102856": ["link_0"], "103540": ["link_0"], "103319": ["link_0"], "103070": ["link_0"], "103063": ["link_0"], "103077": ["link_0"], "103148": ["link_0"], "102798": ["link_0"], "102802": ["link_0"], "102803": ["link_0"], "102804": ["link_0"], "102805": ["link_0"], "102896": ["link_0"], "102903": ["link_0"], "102905": ["link_0"], "102906": ["link_0"], "102981": ["link_0"], "103032": ["link_0"], "103042": ["link_0"], "103044": ["link_0"], "103050": ["link_0"], "103058": ["link_0"], "103150": ["link_0"], "103235": ["link_0"], "103238": ["link_0"], "103242": ["link_0"], "103253": ["link_0"], "103255": ["link_0"], "103268": ["link_0"], "103316": ["link_0"], "103318": ["link_0"], "103320": ["link_0"], "103323": ["link_0"], "103325": ["link_0"], "103329": ["link_0"], "103332": ["link_0"], "103333": ["link_0"], "103339": ["link_0"], "103340": ["link_0"], "103684": ["link_0"], "40147": ["link_0"], "40417": ["link_0"], "41083": ["link_0"], "44781": ["link_0"], "44817": ["link_0"], "44826": ["link_0"], "44853": ["link_0"], "44962": ["link_0"], "45092": ["link_0"], "45132": ["link_0"], "45135": ["link_0"], "45146": ["link_0"], "45162": ["link_0"], "45168": ["link_0"], "45194": ["link_0"], "45219": ["link_0"], "45235": ["link_0"], "45248": ["link_0"], "45261": ["link_0"], "45262": ["link_0"], "45271": ["link_0"], "45290": ["link_0"], "45374": ["link_0"], "45413": ["link_0"], "45427": ["link_0"], "45575": ["link_0"], "45612": ["link_0"], "45620": ["link_0"], "45632": ["link_0"], "45636": ["link_0"], "45642": ["link_0"], "45661": ["link_0"], "45677": ["link_0"], "45687": ["link_0"], "45694": ["link_0"], "45710": ["link_0"], "45746": ["link_0"], "45759": ["link_0"], "45784": ["link_0"], "45801": ["link_0"], "45822": ["link_0"], "45841": ["link_0"], "45910": ["link_0"], "45948": ["link_0"], "45984": ["link_0"], "46014": ["link_0"], "46045": ["link_0"], "46060": ["link_0"], "46084": ["link_0"], "46107": ["link_0"], "46109": ["link_0"], "46123": ["link_0"], "46127": ["link_0"], "46130": ["link_0"], "46132": ["link_0"], "46145": ["link_0"], "46172": ["link_0"], "46236": ["link_0"], "46334": ["link_0"], "46380": ["link_0"], "46439": ["link_0"], "46440": ["link_0"], "46443": ["link_0"], "46452": ["link_0"], "46462": ["link_0"], "46466": ["link_0"], "46537": ["link_0"], "46544": ["link_0"], "46556": ["link_0"], "46598": ["link_0"], "46653": ["link_0"], "46741": ["link_0"], "46762": ["link_0"], "46768": ["link_0"], "46839": ["link_0"], "46856": ["link_0"], "47024": ["link_0"], "47089": ["link_0"], "47168": ["link_0"], "47178": ["link_0"], "47183": ["link_0"], "47185": ["link_0"], "47233": ["link_0"], "47252": ["link_0"], "47254": ["link_0"], "47296": ["link_0"], "47391": ["link_0"], "47438": ["link_0"], "47565": ["link_0"], "47570": ["link_0"], "47711": ["link_0"], "47926": ["link_0"], "47963": ["link_0"], "48010": ["link_0"], "48051": ["link_0"], "48063": ["link_0"], "48169": ["link_0"], "48253": ["link_0"], "48258": ["link_0"], "48263": ["link_0"], "48491": ["link_0"], "48513": ["link_0"], "48517": ["link_0"], "48740": ["link_0"], "48855": ["link_0"], "48876": ["link_0"], "48878": ["link_0"], "11818": ["link_0"], "102996": ["link_0"], "11229": ["link_0"], "11259": ["link_0"], "100520": ["link_0"], "100523": ["link_0"], "100526": ["link_0"], "100531": ["link_0"], "100532": ["link_0"], "100568": ["link_0"], "100579": ["link_0"], "100586": ["link_0"], "100600": ["link_0"], "100609": ["link_0"], "100611": ["link_0"], "100616": ["link_0"], "102255": ["link_0"], "102263": ["link_0"], "102314": ["link_0"], "102333": ["link_0"], "9748": ["link_0"], "9960": ["link_0"], "9992": ["link_0"], "9996": ["link_0"], "10098": ["link_0"], "10101": ["link_0"], "10125": ["link_0"], "10213": ["link_0"], "10238": ["link_0"], "10239": ["link_0"], "10243": ["link_0"], "10248": ["link_0"], "10269": ["link_0"], "10289": ["link_0"], "10305": ["link_0"], "10306": ["link_0"], "10383": ["link_0"], "10707": ["link_0"], "11030": ["link_0"], "11141": ["link_0"], "11156": ["link_0"], "11395": ["link_0"], "11405": ["link_0"], "11406": ["link_0"], "11778": ["link_0"], "11429": ["link_0"], "11477": ["link_0"], "11888": ["link_0"], "10885": ["link_0"], "11854": ["link_0"], "10915": ["link_0"], "11586": ["link_0"], "11581": ["link_0"], "11876": ["link_0"], "10036": ["link_0"], "10068": ["link_0"], "10144": ["link_0"], "10347": ["link_0"], "10373": ["link_0"], "10489": ["link_0"], "10612": ["link_0"], "10620": ["link_0"], "10638": ["link_0"], "10627": ["link_0"], "10655": ["link_0"], "10685": ["link_0"], "10751": ["link_0"], "10797": ["link_0"], "10849": ["link_0"], "10867": ["link_0"], "10900": ["link_0"], "10905": ["link_0"], "10944": ["link_0"], "11211": ["link_0"], "11299": ["link_0"], "11550": ["link_0"], "11709": ["link_0"], "12036": ["link_0"], "12042": ["link_0"], "12043": ["link_0"], "12054": ["link_0"], "12059": ["link_0"], "12066": ["link_0"], "12249": ["link_0"], "12250": ["link_0"], "12252": ["link_0"], "102620": ["link_0"], "102621": ["link_0"], "102622": ["link_0"], "102630": ["link_0"], "102645": ["link_0"], "102648": ["link_0"], "102651": ["link_0"], "102652": ["link_0"], "102654": ["link_0"], "102658": ["link_0"], "102663": ["link_0"], "102666": ["link_0"], "102668": ["link_0"], "102669": ["link_0"], "102670": ["link_0"], "102676": ["link_0"], "102677": ["link_0"], "102687": ["link_0"], "102689": ["link_0"], "102692": ["link_0"], "102694": ["link_0"], "102699": ["link_0"], "102701": ["link_0"], "102703": ["link_0"], "102708": ["link_0"], "103646": ["link_0"], "102252": ["link_0"], "103007": ["link_0"], "103012": ["link_0"], "102158": ["link_0"], "101384": ["link_0"], "102163": ["link_0"], "103634": ["link_0"], "103647": ["link_0"], "102219": ["link_0"], "102992": ["link_0"], "103633": ["link_0"], "102229": ["link_0"], "103639": ["link_0"], "10584": ["link_0"], "11124": ["link_0"], "11279": ["link_0"], "11361": ["link_0"], "12447": ["link_0"], "12483": ["link_0"], "101378": ["link_0"], "102153": ["link_0"], "102154": ["link_0"], "102155": ["link_0"], "102156": ["link_0"], "102173": ["link_0"], "102181": ["link_0"], "102182": ["link_0"], "102186": ["link_0"], "102189": ["link_0"], "102192": ["link_0"], "102259": ["link_0"], "7236": ["link_0"], "7263": ["link_0"], "7292": ["link_0"], "7310": ["link_0"], "7366": ["link_0"], "7167": ["link_0"], "7128": ["link_0"], "7349": ["link_0"], "102990": ["link_0"], "103095": ["link_0"], "103099": ["link_0"], "103100": ["link_0"], "103104": ["link_0"], "103111": ["link_0"], "103113": ["link_0"], "103271": ["link_0"], "103273": ["link_0"], "103280": ["link_0"], "103283": ["link_0"], "103292": ["link_0"], "103293": ["link_0"], "103301": ["link_0"], "103303": ["link_0"], "103305": ["link_0"], "103792": ["link_0"], "100849": ["link_0"], "100965": ["link_0"], "100980": ["link_0"], "103040": ["link_0"], "103135": ["link_0"], "35059": ["link_0"], "38516": ["link_0"], "41003": ["link_0"], "41004": ["link_0"], "41452": ["link_0"], "41529": ["link_0"], "45001": ["link_0"], "45007": ["link_0"], "45087": ["link_0"], "45091": ["link_0"], "45134": ["link_0"], "45164": ["link_0"], "45166": ["link_0"], "45173": ["link_0"], "45177": ["link_0"], "45189": ["link_0"], "45203": ["link_0"], "45212": ["link_0"], "45244": ["link_0"], "45247": ["link_0"], "45249": ["link_0"], "45267": ["link_0"], "45297": ["link_0"], "45305": ["link_0"], "45372": ["link_0"], "45384": ["link_0"], "45385": ["link_0"], "45387": ["link_0"], "45397": ["link_0"], "45415": ["link_0"], "45420": ["link_0"], "45423": ["link_0"], "45443": ["link_0"], "45444": ["link_0"], "45448": ["link_0"], "45463": ["link_0"], "45504": ["link_0"], "45505": ["link_0"], "45516": ["link_0"], "45523": ["link_0"], "45524": ["link_0"], "45526": ["link_0"], "45600": ["link_0"], "45606": ["link_0"], "45621": ["link_0"], "45633": ["link_0"], "45638": ["link_0"], "45645": ["link_0"], "45662": ["link_0"], "45667": ["link_0"], "45670": ["link_0"], "45671": ["link_0"], "45690": ["link_0"], "45691": ["link_0"], "45693": ["link_0"], "45696": ["link_0"], "45699": ["link_0"], "45717": ["link_0"], "45747": ["link_0"], "45749": ["link_0"], "45767": ["link_0"], "45776": ["link_0"], "45780": ["link_0"], "45783": ["link_0"], "45850": ["link_0"], "45908": ["link_0"], "45915": ["link_0"], "45916": ["link_0"], "45936": ["link_0"], "45950": ["link_0"], "45961": ["link_0"], "45963": ["link_0"], "45964": ["link_0"], "46002": ["link_0"], "46019": ["link_0"], "46029": ["link_0"], "46033": ["link_0"], "46037": ["link_0"], "46044": ["link_0"], "46057": ["link_0"], "46092": ["link_0"], "46108": ["link_0"], "46117": ["link_0"], "46166": ["link_0"], "46197": ["link_0"], "46277": ["link_0"], "46401": ["link_0"], "46417": ["link_0"], "46427": ["link_0"], "46430": ["link_0"], "46437": ["link_0"], "46456": ["link_0"], "46480": ["link_0"], "46481": ["link_0"], "46490": ["link_0"], "46563": ["link_0"], "46700": ["link_0"], "46732": ["link_0"], "46744": ["link_0"], "46787": ["link_0"], "46801": ["link_0"], "46889": ["link_0"], "46906": ["link_0"], "46922": ["link_0"], "46944": ["link_0"], "46955": ["link_0"], "47099": ["link_0"], "47133": ["link_0"], "47182": ["link_0"], "47187": ["link_0"], "47227": ["link_0"], "47278": ["link_0"], "47290": ["link_0"], "47388": ["link_0"], "47419": ["link_0"], "47514": ["link_0"], "47577": ["link_0"], "47595": ["link_0"], "47601": ["link_0"], "47613": ["link_0"], "47632": ["link_0"], "47669": ["link_0"], "47686": ["link_0"], "47701": ["link_0"], "47729": ["link_0"], "47742": ["link_0"], "47747": ["link_0"], "47976": ["link_0"], "48018": ["link_0"], "48023": ["link_0"], "48036": ["link_0"], "48167": ["link_0"], "48177": ["link_0"], "48243": ["link_0"], "48271": ["link_0"], "48356": ["link_0"], "48381": ["link_0"], "48413": ["link_0"], "48452": ["link_0"], "48467": ["link_0"], "48490": ["link_0"], "48519": ["link_0"], "48623": ["link_0"], "48721": ["link_0"], "49025": ["link_0"], "49042": ["link_0"], "49132": ["link_0"], "49133": ["link_0"], "49182": ["link_0"], "49188": ["link_0"], "45403": ["link_0"], "45419": ["link_0"], "45725": ["link_0"], "47088": ["link_0"]}
diff --git a/src/flowbothd/__init__.py b/src/flowbothd/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/src/flowbothd/datasets/__init__.py b/src/flowbothd/datasets/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/src/flowbothd/datasets/flow_history_dataset.py b/src/flowbothd/datasets/flow_history_dataset.py
new file mode 100644
index 0000000..eae090c
--- /dev/null
+++ b/src/flowbothd/datasets/flow_history_dataset.py
@@ -0,0 +1,386 @@
+import math
+import random
+from typing import List, Optional, Protocol, Union, cast
+
+import flowbot3d.datasets.flow_dataset as f3dd
+import numpy as np
+import pybullet as p
+import rpad.partnet_mobility_utils.dataset as pmd
+import torch
+import torch_geometric.data as tgd
+from rpad.partnet_mobility_utils.render.pybullet import PybulletRenderer
+from torch_geometric.data import Data
+
+"""
+Variable length history dataset
+- Generated by taking the joint limits of a randomly selected joint
+- Rendering small change in movement for 100 steps
+- Randomly selecting a variable length (K) from the history
+"""
+############################################################
+
+# Joints helper functions
+
+
+def get_random_joint(obj_id, client_id, seed=None, raw_data_obj=None):
+    rng = np.random.default_rng(seed)
+    n_joints = p.getNumJoints(obj_id, client_id)
+    articulation_ixs = []
+    for joint_ix in range(n_joints):
+        jinfo = p.getJointInfo(obj_id, joint_ix, client_id)
+        if jinfo[2] == p.JOINT_REVOLUTE or jinfo[2] == p.JOINT_PRISMATIC:
+            joint_name = jinfo[1].decode("UTF-8")
+            joint_type = int(jinfo[2])
+            start_end = raw_data_obj.get_joint(joint_name).limit
+            if start_end is not None:
+                start, end = start_end
+                if start >= end:
+                    continue
+            articulation_ixs.append((joint_name, joint_type, joint_ix))
+    selected_ix = articulation_ixs[rng.choice(len(articulation_ixs))]
+    joint_name, joint_type, joint_ix = selected_ix
+    return joint_name, joint_type, joint_ix
+
+
+def get_joint_type(obj_id, client_id, joint_ix):
+    jinfo = p.getJointInfo(obj_id, joint_ix, client_id)
+    if jinfo[2] == p.JOINT_REVOLUTE:
+        return "R"
+    elif jinfo[2] == p.JOINT_PRISMATIC:
+        return "P"
+
+
+def get_joints(obj_id, client_id):
+    joints = []
+    for i in range(p.getNumJoints(obj_id, client_id)):
+        jinfo = p.getJointInfo(obj_id, i, client_id)
+        joints.append(jinfo)
+    return joints
+
+
+def get_joint_angles(obj_id, client_id):
+    angles = {}
+    for i in range(p.getNumJoints(obj_id, client_id)):
+        jinfo = p.getJointInfo(obj_id, i, client_id)
+        jstate = p.getJointState(obj_id, i, client_id)
+        angles[jinfo[12].decode("UTF-8")] = jstate[0]
+    return angles
+
+
+############################################################
+
+
+class FlowHistory(Protocol):
+    id: str  # Object ID.
+    # curr_pos: torch.Tensor  # Points in the point cloud.
+    pos: torch.Tensor
+    point: torch.Tensor
+    delta: torch.Tensor  # instantaneous positive 3D flow.
+    mask: torch.Tensor  # Mask of the part of interest.
+    K: int  # Size of history window (i.e. number of point clouds in the history)
+    history: torch.Tensor  # Array of K point clouds which form the history of the observation
+
+
+class FlowHistoryDataset(tgd.Dataset):
+    def __init__(
+        self,
+        root: str,
+        split: Union[pmd.AVAILABLE_DATASET, List[str]],
+        randomize_joints: bool = True,
+        randomize_camera: bool = True,
+        randomize_size: bool = False,
+        augmentation: bool = False,
+        trajectory_len: int = 1,
+        special_req: str = None,
+        n_points: Optional[int] = 1200,
+        seed: int = 42,
+    ):
+        super().__init__()
+
+        self.seed = seed
+        self._dataset = pmd.PCDataset(root=root, split=split, renderer="pybullet")
+
+        self.randomize_joints = randomize_joints
+        self.randomize_camera = randomize_camera
+        self.randomize_size = randomize_size
+        self.augmentation = augmentation
+
+        self.trajectory_len = trajectory_len
+        self.special_req = special_req
+        self.n_points = n_points
+
+    def len(self) -> int:
+        return len(self._dataset)
+
+    def get(self, index) -> tgd.Data:
+        return self.get_data(self._dataset._ids[index])
+
+    @staticmethod
+    def get_processed_dir(
+        randomize_joints,
+        randomize_camera,
+        trajectory_len,
+        special_req=None,
+        toy_dataset_id=None,
+        randomize_size=False,
+        augmentation=False,
+    ):
+        joint_chunk = "rj" if randomize_joints else "sj"
+        camera_chunk = "rc" if randomize_camera else "sc"
+        random_size_str = "" if not randomize_size else "_rsz"
+        augmentation_str = "" if not augmentation else "_aug"
+        if special_req is None and toy_dataset_id is None:
+            return f"processed_history_{trajectory_len}_{joint_chunk}_{camera_chunk}_random{random_size_str}{augmentation_str}"
+        elif special_req is not None and toy_dataset_id is None:
+            # fully_closed
+            # half_half
+            return f"processed_history_{trajectory_len}_{joint_chunk}_{camera_chunk}_{special_req}{random_size_str}{augmentation_str}"
+        elif special_req is None and toy_dataset_id is not None:
+            # fully_closed
+            # half_half
+            return f"processed_history_{trajectory_len}_{joint_chunk}_{camera_chunk}_toy{toy_dataset_id}_random{random_size_str}{augmentation_str}"
+        else:
+            return f"processed_history_{trajectory_len}_{joint_chunk}_{camera_chunk}_{special_req}_toy{toy_dataset_id}{random_size_str}{augmentation_str}"
+
+    def get_data(self, obj_id: str, seed=None) -> FlowHistory:
+        # Initial randomization parameters.
+        # Select the camera.
+        this_sample_open = True
+        if self.special_req is None:
+            joints = "random" if self.randomize_joints else None
+        elif self.special_req == "half-half":
+            this_sample_open = random.randint(0, 99) < 50  # 50 open + 50 closed
+            # print(this_sample_open)
+            if this_sample_open:  # Open this sample - with history
+                joints = "random"
+            else:  # Close this sample - without history
+                joints = "fully-closed"
+        elif self.special_req == "half-half-01":
+            this_sample_open = (
+                random.randint(0, 199) >= 50
+            )  # 100 open + 50 open(no history) + 50 closed
+            # print(this_sample_open)
+            if this_sample_open:  # Open this sample - with history
+                has_history = random.randint(0, 149) < 100
+                joints = "random"
+            else:  # Close this sample - without history
+                has_history = False
+                joints = "fully-closed"
+            this_sample_open = has_history
+        elif self.special_req == "fully-closed":
+            this_sample_open = False
+            joints = "fully-closed"
+        else:
+            assert True, f"{self.special_req} mode not supported in history dataset."
+        camera_xyz = "random" if self.randomize_camera else None
+
+        rng = np.random.default_rng(seed)
+        seed1, seed2, seed3, seed4 = rng.bit_generator._seed_seq.spawn(4)  # type: ignore
+        _ = self._dataset.get(  # Just to create a renderer
+            obj_id=obj_id,
+            joints=joints,
+            camera_xyz=camera_xyz,
+            seed=seed1,
+        )
+
+        raw_data_obj = self._dataset.pm_objs[obj_id].obj
+        # Randomly select a joint to modify by poking through the guts.
+        renderer: PybulletRenderer = self._dataset.renderers[obj_id]  # type: ignore
+        (
+            joint_name,
+            joint_type,
+            joint_ix,
+        ) = get_random_joint(  # Choose a joint to manipulate with
+            obj_id=renderer._render_env.obj_id,
+            client_id=renderer._render_env.client_id,
+            seed=seed2,
+            raw_data_obj=raw_data_obj,
+        )
+        # print(joint_name, "open" if this_sample_open else "close")
+        data_t0 = (
+            self._dataset.get(  # Re-render the object with one specific random joint!
+                obj_id=obj_id,
+                joints=joints,
+                camera_xyz=camera_xyz,
+                seed=seed1,
+                random_joint_id=joint_name,
+            )
+        )
+        pos_t0 = data_t0["pos"]
+
+        # Compute the flow + mask at that time.
+        # target_point_t0, _, flow_t0 = compute_normalized_flow(
+        flow_t0 = f3dd.compute_normalized_flow(
+            P_world=pos_t0,
+            T_world_base=data_t0["T_world_base"],
+            current_jas=data_t0["angles"],
+            pc_seg=data_t0["seg"],
+            labelmap=data_t0["labelmap"],
+            pm_raw_data=self._dataset.pm_objs[obj_id],
+            linknames="all",
+        )
+        mask_t0 = (~(flow_t0 == 0.0).all(axis=-1)).astype(int)
+
+        # Compute the states for the camera and joints at t1.
+        # Camera should be the same.
+        camera_xyz_t1 = data_t0["T_world_cam"][:3, 3]
+        joints_t0 = data_t0["angles"]
+
+        # print("t0:", joints_t0)
+
+        ###################################################################
+
+        K = (
+            0 if not this_sample_open else 1
+        )  # If fully closed - then return None history
+        # print(K)
+
+        d_theta = 0
+        if this_sample_open:  # Pick a joint to open
+            joint = raw_data_obj.get_joint(joint_name)
+
+            if joint.limit == None:  # revolute free moving
+                min_theta, max_theta = 0, 2 * math.pi
+            else:
+                min_theta, max_theta = joint.limit
+
+            assert (
+                max_theta > min_theta
+            ), "Selected a joint with min theta >= max theta?"
+
+            interval_cnt = random.randint(5, 50)
+            d_theta = (max_theta - min_theta) / interval_cnt
+
+        # HACK HACK HACK we need to make sure that the joint is actually in the joint list.
+        # This is a bug in the underlying library, annoying.
+        joints_t1 = {
+            jn: jv
+            for jn, jv in joints_t0.items()
+            if jn in renderer._render_env.jn_to_ix
+        }
+
+        ###################################################################
+        # Render. and compute values.
+        lengths = []
+
+        # end_theta = min_theta + end_ix * d_theta
+
+        step_id = 0
+        while step_id <= K:
+            # print("t1:", joints_t1)
+            # Describe the action that was taken.
+            action = np.zeros(len(joints_t0))
+            action[joint_ix] = d_theta
+
+            # Get the second render.
+            data_t1 = self._dataset.get(
+                obj_id=obj_id, joints=joints_t1, camera_xyz=camera_xyz_t1, seed=seed3
+            )
+            pos_t1 = data_t1["pos"]
+
+            # Compute the flow + mask at that time.
+            flow_t1 = f3dd.compute_normalized_flow(
+                # target_point_t1, _, flow_t1 = compute_normalized_flow(
+                P_world=pos_t1,
+                T_world_base=data_t1["T_world_base"],
+                current_jas=data_t1["angles"],
+                pc_seg=data_t1["seg"],
+                labelmap=data_t1["labelmap"],
+                pm_raw_data=self._dataset.pm_objs[obj_id],
+                linknames="all",
+            )
+
+            mask_t1 = (~(flow_t1 == 0.0).all(axis=-1)).astype(int)
+
+            # Downsample.
+            if self.n_points:
+                rng = np.random.default_rng(seed4)
+
+                ixs_t0 = rng.permutation(range(len(pos_t0)))[: self.n_points]
+                pos_t0 = pos_t0[ixs_t0]
+                flow_t0 = flow_t0[ixs_t0]
+                # target_point_t0 = target_point_t0[ixs_t0]
+                mask_t0 = mask_t0[ixs_t0]
+
+                ixs_t1 = rng.permutation(range(len(pos_t1)))[: self.n_points]
+                pos_t1 = pos_t1[ixs_t1]
+                flow_t1 = flow_t1[ixs_t1]
+                # target_point_t1 = target_point_t1[ixs_t1]
+                mask_t1 = mask_t1[ixs_t1]
+
+            # Add to series of history
+            if step_id == 0:
+                # history = np.array([pos_t0])
+                # flow_history = np.array([flow_t0])
+                # target_point_history = np.array([target_point_t0])
+                # lengths.append(len(pos_t0))
+                history = np.array([pos_t1])
+                flow_history = np.array([flow_t1])
+                # target_point_history = np.array([target_point_t1])
+                lengths.append(len(pos_t1))
+            else:
+                history = np.append(history, [pos_t1], axis=0)
+                flow_history = np.append(flow_history, [flow_t1], axis=0)
+                # target_point_history = np.append(target_point_history, [target_point_t1], axis=0)
+                lengths.append(len(pos_t1))
+
+            step_id += 1
+
+            # Rotate the target joint for an angle
+            joints_t1[joint_name] += d_theta
+
+        # start_ix = random.randint(0, num_obs - 2) # min 1 observation
+        # temp_K = random.randint(1, 20)
+        # end_ix = min(start_ix + temp_K, num_obs - 1) # upperbound of num_obs-1
+        # K = end_ix - start_ix
+
+        # print(step_id)
+        curr_pos = history[-1]
+        flow = flow_history[-1]
+
+        history = (
+            history[:-1] if K >= 1 else history * 0
+        )  # No history, but the shape should be the same
+        flow_history = flow_history[:-1] if K >= 1 else flow_history * 0
+        lengths = lengths[:-1] if K >= 1 else lengths
+        # print(history.shape, flow_history.shape, lengths)
+
+        history = history.reshape(-1, history.shape[-1])  #
+        flow_history = flow_history.reshape(-1, flow_history.shape[-1])
+        # target_point_history = target_point_history.reshape(-1, target_point_history.shape[-1])
+
+        # random size
+        rsz = 1 if not self.randomize_size else np.random.uniform(0.1, 5)
+        # data augmentation
+        flip = 0 if not self.augmentation else np.random.randint(0, 4)  # 4 flip modes
+        flip_mat = torch.tensor(
+            [
+                [[1, 0, 0], [0, 1, 0], [0, 0, 1]],  # Normal
+                [[1, 0, 0], [0, -1, 0], [0, 0, 1]],  # Left, right
+                [[-1, 0, 0], [0, 1, 0], [0, 0, 1]],  # Front, back
+                [[-1, 0, 0], [0, -1, 0], [0, 0, 1]],  # Front back & left right
+            ]
+        ).float()
+
+        data = Data(
+            id=obj_id,
+            num_points=torch.tensor([curr_pos.shape[0]]),  # N: shape of point cloud
+            action=torch.from_numpy(action).float(),
+            pos=torch.matmul(torch.from_numpy(curr_pos).float() * rsz, flip_mat[flip]),
+            delta=torch.matmul(
+                torch.from_numpy(flow).unsqueeze(1).float(), flip_mat[flip]
+            ),
+            history=torch.matmul(
+                torch.from_numpy(history).float() * rsz, flip_mat[flip]
+            ),  # N*K, 3
+            flow_history=torch.matmul(
+                torch.from_numpy(flow_history).float(), flip_mat[flip]  # N*K, 3
+            ),  # Snapshot of flow history
+            # point=torch.from_numpy(target_point).unsqueeze(1).float(),
+            mask=torch.from_numpy(mask_t1).float(),
+            # link=joint.child,  # child of the joint gives you the link that the joint is connected to
+            K=K,  # length of history
+            lengths=torch.as_tensor(lengths).int(),  # size of point cloud
+        )
+
+        return cast(FlowHistory, data)
diff --git a/src/flowbothd/datasets/flow_trajectory.py b/src/flowbothd/datasets/flow_trajectory.py
new file mode 100644
index 0000000..67dfa5f
--- /dev/null
+++ b/src/flowbothd/datasets/flow_trajectory.py
@@ -0,0 +1,213 @@
+import os
+
+import lightning as L
+import rpad.partnet_mobility_utils.dataset as rpd
+import torch_geometric.loader as tgl
+from rpad.pyg.dataset import CachedByKeyDataset
+
+from flowbothd.datasets.flow_history_dataset import FlowHistoryDataset
+from flowbothd.datasets.flow_trajectory_dataset_pyg import (
+    FlowTrajectoryPyGDataset,
+)
+
+
+# Create FlowBot datamodule
+class FlowTrajectoryDataModule(L.LightningDataModule):
+    def __init__(
+        self,
+        root,
+        batch_size,
+        num_workers,
+        n_proc,
+        history=False,  ## With / without history
+        randomize_camera: bool = True,
+        randomize_size: bool = False,
+        augmentation: bool = False,
+        trajectory_len: int = 1,
+        seed: int = 42,
+        special_req: str = None,
+        toy_dataset: dict = None,
+        n_repeat: int = 100,  # By default, repeat training dataset by 100
+    ):
+        super().__init__()
+        self.batch_size = batch_size
+        self.seed = seed
+        self.dataset_cls = FlowHistoryDataset if history else FlowTrajectoryPyGDataset
+        if augmentation:  # Augmentation: 4 flip modes
+            n_repeat *= 4
+        print(
+            self.dataset_cls.get_processed_dir(
+                True,
+                randomize_camera,
+                trajectory_len,
+                special_req,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                toy_dataset_id=None if toy_dataset is None else toy_dataset["id"],
+            )
+        )
+        self.train_dset = CachedByKeyDataset(
+            dset_cls=self.dataset_cls,
+            dset_kwargs=dict(
+                root=os.path.join(root, "raw"),
+                split="umpnet-train-train"
+                if toy_dataset is None
+                else toy_dataset["train-train"],
+                randomize_camera=randomize_camera,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                trajectory_len=trajectory_len,
+                special_req=special_req,
+            ),
+            data_keys=rpd.UMPNET_TRAIN_TRAIN_OBJ_IDS
+            if toy_dataset is None
+            else toy_dataset["train-train"],
+            root=root,
+            processed_dirname=self.dataset_cls.get_processed_dir(
+                True,
+                randomize_camera,
+                trajectory_len,
+                special_req,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                toy_dataset_id=None if toy_dataset is None else toy_dataset["id"],
+            ),
+            n_repeat=n_repeat,
+            # n_repeat=1,
+            n_workers=num_workers,
+            n_proc_per_worker=n_proc,
+            seed=seed,
+        )
+
+        self.train_val_dset = CachedByKeyDataset(
+            dset_cls=self.dataset_cls,
+            dset_kwargs=dict(
+                root=os.path.join(root, "raw"),
+                split="umpnet-train-train"
+                if toy_dataset is None
+                else toy_dataset["train-train"],
+                randomize_camera=randomize_camera,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                trajectory_len=trajectory_len,
+                special_req=special_req,
+            ),
+            data_keys=rpd.UMPNET_TRAIN_TRAIN_OBJ_IDS
+            if toy_dataset is None
+            else toy_dataset["train-train"],
+            root=root,
+            processed_dirname=self.dataset_cls.get_processed_dir(
+                True,
+                randomize_camera,
+                trajectory_len,
+                special_req,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                toy_dataset_id=None if toy_dataset is None else toy_dataset["id"],
+            ),
+            n_repeat=1,
+            n_workers=num_workers,
+            n_proc_per_worker=n_proc,
+            seed=seed,
+        )
+
+        self.val_dset = CachedByKeyDataset(
+            dset_cls=self.dataset_cls,
+            dset_kwargs=dict(
+                root=os.path.join(root, "raw"),
+                split="umpnet-train-test"
+                if toy_dataset is None
+                else toy_dataset["train-test"],
+                randomize_camera=randomize_camera,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                trajectory_len=trajectory_len,
+                special_req=special_req,
+            ),
+            data_keys=rpd.UMPNET_TRAIN_TEST_OBJ_IDS
+            if toy_dataset is None
+            else toy_dataset["train-test"],
+            root=root,
+            processed_dirname=self.dataset_cls.get_processed_dir(
+                True,
+                randomize_camera,
+                trajectory_len,
+                special_req,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                toy_dataset_id=None if toy_dataset is None else toy_dataset["id"],
+            ),
+            n_repeat=1,
+            n_workers=num_workers,
+            n_proc_per_worker=n_proc,
+            seed=seed,
+        )
+
+        self.unseen_dset = CachedByKeyDataset(
+            dset_cls=self.dataset_cls,
+            dset_kwargs=dict(
+                root=os.path.join(root, "raw"),
+                split="umpnet-test" if toy_dataset is None else toy_dataset["test"],
+                randomize_camera=randomize_camera,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                trajectory_len=trajectory_len,
+                special_req=special_req,
+            ),
+            data_keys=rpd.UMPNET_TEST_OBJ_IDS
+            if toy_dataset is None
+            else toy_dataset["test"],
+            root=root,
+            processed_dirname=self.dataset_cls.get_processed_dir(
+                True,
+                randomize_camera,
+                trajectory_len,
+                special_req,
+                randomize_size=randomize_size,
+                augmentation=augmentation,
+                toy_dataset_id=None if toy_dataset is None else toy_dataset["id"],
+            ),
+            n_repeat=1,
+            n_workers=num_workers,
+            n_proc_per_worker=n_proc,
+            seed=seed,
+        )
+
+    def train_dataloader(self):
+        L.seed_everything(self.seed)
+        return tgl.DataLoader(
+            self.train_dset, self.batch_size, shuffle=True, num_workers=0
+        )
+
+    def train_val_dataloader(self, bsz=None):
+        bsz = self.batch_size if bsz is None else bsz
+        return tgl.DataLoader(
+            self.train_val_dset,
+            bsz,
+            shuffle=False,
+            num_workers=0
+            # self.train_val_dset, 1, shuffle=False, num_workers=0
+        )
+
+    def val_dataloader(self, bsz=None):
+        bsz = self.batch_size if bsz is None else bsz
+        return tgl.DataLoader(
+            self.val_dset,
+            bsz,
+            # 1,   # TODO: change back!
+            shuffle=False,
+            num_workers=0
+            # self.val_dset, 1, shuffle=False, num_workers=0
+        )
+
+    def unseen_dataloader(self, bsz=None):
+        bsz = self.batch_size if bsz is None else bsz
+        return tgl.DataLoader(
+            self.unseen_dset,
+            # self.batch_size,
+            bsz,
+            # 1,  # TODO: change back!
+            shuffle=False,
+            num_workers=0
+            # self.unseen_dset, self.batch_size, shuffle=False, num_workers=0
+        )
diff --git a/src/flowbothd/datasets/flow_trajectory_dataset.py b/src/flowbothd/datasets/flow_trajectory_dataset.py
new file mode 100644
index 0000000..93393a4
--- /dev/null
+++ b/src/flowbothd/datasets/flow_trajectory_dataset.py
@@ -0,0 +1,206 @@
+import copy
+from typing import Dict, List, Literal, Optional, Sequence, TypedDict, Union
+
+import numpy as np
+import numpy.typing as npt
+import rpad.partnet_mobility_utils.articulate as pma
+import rpad.partnet_mobility_utils.dataset as pmd
+from rpad.partnet_mobility_utils.data import PMObject
+
+
+class FlowTrajectoryData(TypedDict):
+    id: str
+    pos: npt.NDArray[np.float32]  # (N, 3): Point cloud observation.
+    delta: npt.NDArray[np.float32]  # (N, K, traj_len * 3): Ground-truth flow.
+    point: npt.NDArray[np.float32]  # (N, K, traj_len * 3): Ground-truth waypoints.
+    mask: npt.NDArray[np.bool_]  #  (N,): Mask the point of interest.
+
+
+"""
+Changes made:
+Apart from flow, also return new pos (P_world_new) and new joint angles (target_jas)
+"""
+
+
+def compute_normalized_flow(
+    P_world: npt.NDArray[np.float32],
+    T_world_base: npt.NDArray[np.float32],
+    current_jas: Dict[str, float],
+    pc_seg: npt.NDArray[np.uint8],
+    labelmap: Dict[str, int],
+    pm_raw_data: PMObject,
+    linknames: Union[Literal["all"], Sequence[str]] = "all",
+) -> npt.NDArray[np.float32]:
+    """Compute normalized flow for an object, based on its kinematics.
+
+    Args:
+        P_world (npt.NDArray[np.float32]): Point cloud render of the object in the world frame.
+        T_world_base (npt.NDArray[np.float32]): The pose of the base link in the world frame.
+        current_jas (Dict[str, float]): The current joint angles (easy to acquire from the render that created the points.)
+        pc_seg (npt.NDArray[np.uint8]): The segmentation labels of each point.
+        labelmap (Dict[str, int]): Map from the link name to segmentation name.
+        pm_raw_data (PMObject): The object description, essentially providing the kinematic structure of the object.
+        linknames (Union[Literal['all'], Sequence[str]], optional): The names of the links for which to
+            compute flow. Defaults to "all", which will articulate all of them.
+
+    Returns:
+        npt.NDArray[np.float32]: _description_
+    """
+
+    # We actuate all links.
+    if linknames == "all":
+        joints = pm_raw_data.semantics.by_type("slider")
+        joints += pm_raw_data.semantics.by_type("hinge")
+        linknames = [joint.name for joint in joints]
+
+    flow = np.zeros_like(P_world)
+    target_jas = copy.deepcopy(current_jas)
+
+    for linkname in linknames:
+        P_world_new = pma.articulate_joint(
+            pm_raw_data,
+            current_jas,
+            linkname,
+            0.01,  # Articulate by only a little bit.
+            P_world,
+            pc_seg,
+            labelmap,
+            T_world_base,
+        )
+        # Articulate the joint angles
+        target_jas[pm_raw_data.obj.get_joint_by_child(linkname).name] += 0.01
+
+        link_flow = P_world_new - P_world
+        # P_world = P_world_new
+        flow += link_flow
+
+    largest_mag: float = np.linalg.norm(flow, axis=-1).max()
+
+    normalized_flow = flow / (largest_mag + 1e-6)
+
+    # return P_world_new, target_jas, normalized_flow
+    return P_world + flow, target_jas, normalized_flow
+
+
+# Compute trajectories as K deltas & waypoints
+def compute_flow_trajectory(
+    K,
+    P_world,
+    T_world_base,
+    current_jas,
+    pc_seg,
+    labelmap,
+    pm_raw_data,
+    linknames="all",
+) -> npt.NDArray[np.float32]:
+    flow_trajectory = np.zeros((K, P_world.shape[0], 3), dtype=np.float32)
+    point_trajectory = np.zeros((K, P_world.shape[0], 3), dtype=np.float32)
+    for step in range(K):
+        # compute the delta / waypoint & rotate and then calculate another
+        P_world_new, current_jas, flow = compute_normalized_flow(
+            P_world,
+            T_world_base,
+            current_jas,
+            pc_seg,
+            labelmap,
+            pm_raw_data,
+            linknames,
+        )
+        flow_trajectory[step, :, :] = flow
+        point_trajectory[step, :, :] = P_world_new
+        # Update pos
+        P_world = P_world_new
+    return flow_trajectory.transpose(1, 0, 2), point_trajectory.transpose(
+        1, 0, 2
+    )  # Delta / Point * traj_len * 3
+
+
+class FlowTrajectoryDataset:
+    def __init__(
+        self,
+        root: str,
+        split: Union[pmd.AVAILABLE_DATASET, List[str]],
+        randomize_joints: bool = True,
+        randomize_camera: bool = True,
+        trajectory_len: int = 5,
+        special_req: str = None,
+        n_points: Optional[int] = None,
+    ) -> None:
+        """The FlowBot3D dataset. Set n_points depending if you can handle ragged batches or not.
+
+        Args:
+            root (str): The root directory of the downloaded partnet-mobility dataset.
+            split (Union[pmd.AVAILABLE_DATASET, List[str]]): Either an available split like "umpnet-train-train" or a list of object IDs from the PM dataset.
+            randomize_joints (bool): Whether or not to randomize the joints.
+            randomize_camera (bool): Whether or not to randomize the camera location (in a fixed range, see the underlying renderer...)
+            n_points (Optional[int], optional): Whether or not to downsample the number of points returned for each example. If
+                you want to use this datasets as a standard PyTorch dataset, you should set this to a non-None value (otherwise passing it into
+                a dataloader won't really work, since you'll have ragged batches. If you're using PyTorch-Geometric to handle batches, do whatever you want.
+                Defaults to None.
+        """
+        self._dataset = pmd.PCDataset(root=root, split=split, renderer="pybullet")
+        self._ids = self._dataset._ids
+        self.randomize_joints = randomize_joints
+        self.randomize_camera = randomize_camera
+        self.trajectory_len = trajectory_len
+        self.special_req = special_req
+        self.n_points = n_points
+
+    def get_data(self, obj_id: str, seed=None) -> FlowTrajectoryData:
+        # Select the camera.
+        if self.special_req is None:
+            joints = "random" if self.randomize_joints else None
+        else:
+            joints = (
+                self.special_req
+            )  # TODO: Set to random-oc as for multimodal experiments
+        # print(joints)
+        # joints = "random" if self.randomize_joints else None
+        camera_xyz = "random" if self.randomize_camera else None
+
+        rng = np.random.default_rng(seed)
+        seed1, seed2 = rng.bit_generator._seed_seq.spawn(2)  # type: ignore
+
+        data = self._dataset.get(
+            obj_id=obj_id, joints=joints, camera_xyz=camera_xyz, seed=seed1  # type: ignore
+        )
+        pos = data["pos"]
+
+        # Compute the flow trajectory
+        flow_trajectory, point_trajectory = compute_flow_trajectory(
+            K=self.trajectory_len,
+            P_world=pos,
+            T_world_base=data["T_world_base"],
+            current_jas=data["angles"],
+            pc_seg=data["seg"],
+            labelmap=data["labelmap"],
+            pm_raw_data=self._dataset.pm_objs[obj_id],
+            linknames="all",
+        )
+        # Compute the mask of any part which has flow.
+        mask = (
+            ~(
+                np.isclose(flow_trajectory.reshape(flow_trajectory.shape[0], -1), 0.0)
+            ).all(axis=-1)
+        ).astype(np.bool_)
+        if self.n_points:
+            rng = np.random.default_rng(seed2)
+            ixs = rng.permutation(range(len(pos)))[: self.n_points]
+            pos = pos[ixs]
+            flow_trajectory = flow_trajectory[ixs, :, :]
+            point_trajectory = point_trajectory[ixs, :, :]
+            mask = mask[ixs]
+        return {
+            "id": data["id"],
+            "pos": pos,
+            "delta": flow_trajectory,  #  N , traj_len, 3
+            "point": point_trajectory,  #  N , traj_len, 3
+            "mask": mask,
+        }
+
+    def __getitem__(self, item: int) -> FlowTrajectoryData:
+        obj_id = self._dataset._ids[item]
+        return self.get_data(obj_id)
+
+    def __len__(self):
+        return len(self._dataset)
diff --git a/src/flowbothd/datasets/flow_trajectory_dataset_pyg.py b/src/flowbothd/datasets/flow_trajectory_dataset_pyg.py
new file mode 100644
index 0000000..35faaaa
--- /dev/null
+++ b/src/flowbothd/datasets/flow_trajectory_dataset_pyg.py
@@ -0,0 +1,111 @@
+from typing import List, Optional, Protocol, Union, cast
+
+import rpad.partnet_mobility_utils.dataset as pmd
+import torch
+import torch_geometric.data as tgd
+
+from flowbothd.datasets.flow_trajectory_dataset import (
+    FlowTrajectoryDataset,
+)
+
+
+class FlowTrajectoryTGData(Protocol):
+    id: str  # Object ID.
+
+    pos: torch.Tensor  # Points in the point cloud.
+    delta: torch.Tensor  # instantaneous positive 3D flow trajectories.
+    point: torch.Tensor  # the trajectory waypoints
+    mask: torch.Tensor  # Mask of the part of interest.
+
+
+class FlowTrajectoryPyGDataset(tgd.Dataset):
+    def __init__(
+        self,
+        root: str,
+        split: Union[pmd.AVAILABLE_DATASET, List[str]],
+        randomize_joints: bool = True,  # TODO: set to True
+        randomize_camera: bool = True,
+        randomize_size: bool = False,
+        augmentation: bool = False,
+        trajectory_len: int = 5,
+        special_req: str = None,
+        n_points: Optional[int] = 1200,
+        seed: int = 42,  # Randomize everything
+    ) -> None:
+        super().__init__()
+        self.dataset = FlowTrajectoryDataset(
+            root,
+            split,
+            randomize_joints,
+            randomize_camera,
+            trajectory_len,
+            special_req,
+            n_points,
+        )
+        self.n_points = n_points
+        self.seed = seed
+        self.randomize_size = randomize_size
+        self.augmentation = augmentation
+
+    def len(self) -> int:
+        return len(self.dataset)
+
+    def get(self, index) -> tgd.Data:
+        return self.get_data(self.dataset._dataset._ids[index], seed=self.seed)
+
+    @staticmethod
+    def get_processed_dir(
+        randomize_joints,
+        randomize_camera,
+        trajectory_len,
+        special_req=None,
+        toy_dataset_id=None,
+        randomize_size: bool = False,
+        augmentation: bool = False,
+    ):
+        joint_chunk = "rj" if randomize_joints else "sj"
+        camera_chunk = "rc" if randomize_camera else "sc"
+        random_size_str = "" if not randomize_size else "_rsz"
+        augmentation_str = "" if not augmentation else "_aug"
+        if special_req is None and toy_dataset_id is None:
+            return f"processed_{trajectory_len}_{joint_chunk}_{camera_chunk}_random{random_size_str}{augmentation_str}"
+        elif special_req is not None and toy_dataset_id is None:
+            # fully_closed
+            # half_half
+            return f"processed_{trajectory_len}_{joint_chunk}_{camera_chunk}_{special_req}{random_size_str}{augmentation_str}"
+        elif special_req is None and toy_dataset_id is not None:
+            # fully_closed
+            # half_half
+            return f"processed_{trajectory_len}_{joint_chunk}_{camera_chunk}_toy{toy_dataset_id}_random{random_size_str}{augmentation_str}"
+        else:
+            return f"processed_{trajectory_len}_{joint_chunk}_{camera_chunk}_{special_req}_toy{toy_dataset_id}{random_size_str}{augmentation_str}"
+
+    def get_data(self, obj_id: str, seed) -> FlowTrajectoryTGData:
+        data_dict = self.dataset.get_data(obj_id, seed)
+        # random size
+        rsz = 1 if not self.randomize_size else np.random.uniform(0.1, 5)
+        # data augmentation
+        flip = 0 if not self.augmentation else np.random.randint(0, 4)  # 4 flip modes
+        flip_mat = torch.tensor(
+            [
+                [[1, 0, 0], [0, 1, 0], [0, 0, 1]],  # Normal
+                [[1, 0, 0], [0, -1, 0], [0, 0, 1]],  # Left, right
+                [[-1, 0, 0], [0, 1, 0], [0, 0, 1]],  # Front, back
+                [[-1, 0, 0], [0, -1, 0], [0, 0, 1]],  # Front back & left right
+            ]
+        ).float()
+
+        data = tgd.Data(
+            id=data_dict["id"],
+            pos=torch.matmul(
+                torch.from_numpy(data_dict["pos"]).float() * rsz, flip_mat[flip]
+            ),
+            delta=torch.matmul(
+                torch.from_numpy(data_dict["delta"]).float(), flip_mat[flip]
+            ),
+            point=torch.matmul(
+                torch.from_numpy(data_dict["point"]).float() * rsz, flip_mat[flip]
+            ),
+            mask=torch.from_numpy(data_dict["mask"]).float(),
+        )
+        return cast(FlowTrajectoryTGData, data)
diff --git a/src/flowbothd/datasets/flowbot.py b/src/flowbothd/datasets/flowbot.py
new file mode 100644
index 0000000..3a17761
--- /dev/null
+++ b/src/flowbothd/datasets/flowbot.py
@@ -0,0 +1,122 @@
+import os
+
+import lightning as L
+import rpad.partnet_mobility_utils.dataset as rpd
+import torch_geometric.loader as tgl
+from flowbot3d.datasets.flow_dataset_pyg import Flowbot3DPyGDataset
+from rpad.pyg.dataset import CachedByKeyDataset
+
+
+class FlowBotDataModule(L.LightningDataModule):
+    def __init__(
+        self,
+        root,
+        batch_size,
+        num_workers,
+        n_proc,
+        randomize_camera: bool = True,
+        seed=42,
+        **kwargs,
+    ):
+        super().__init__()
+        self.batch_size = batch_size
+        self.seed = seed
+
+        self.train_dset = CachedByKeyDataset(
+            dset_cls=Flowbot3DPyGDataset,
+            dset_kwargs=dict(
+                root=os.path.join(root, "raw"),
+                split="umpnet-train-train",
+                randomize_camera=randomize_camera,
+            ),
+            data_keys=rpd.UMPNET_TRAIN_TRAIN_OBJ_IDS,
+            root=root,
+            processed_dirname=Flowbot3DPyGDataset.get_processed_dir(
+                True,
+                randomize_camera,
+            ),
+            n_repeat=100,
+            n_workers=num_workers,
+            n_proc_per_worker=n_proc,
+            seed=seed,
+        )
+
+        self.train_val_dset = CachedByKeyDataset(
+            dset_cls=Flowbot3DPyGDataset,
+            dset_kwargs=dict(
+                root=os.path.join(root, "raw"),
+                split="umpnet-train-train",
+                randomize_camera=randomize_camera,
+            ),
+            data_keys=rpd.UMPNET_TRAIN_TRAIN_OBJ_IDS,
+            root=root,
+            processed_dirname=Flowbot3DPyGDataset.get_processed_dir(
+                True,
+                randomize_camera,
+            ),
+            n_repeat=1,
+            n_workers=num_workers,
+            n_proc_per_worker=n_proc,
+            seed=seed,
+        )
+
+        self.val_dset = CachedByKeyDataset(
+            dset_cls=Flowbot3DPyGDataset,
+            dset_kwargs=dict(
+                root=os.path.join(root, "raw"),
+                split="umpnet-train-test",
+                randomize_camera=randomize_camera,
+            ),
+            data_keys=rpd.UMPNET_TRAIN_TEST_OBJ_IDS,
+            root=root,
+            processed_dirname=Flowbot3DPyGDataset.get_processed_dir(
+                True,
+                randomize_camera,
+            ),
+            n_repeat=1,
+            n_workers=num_workers,
+            n_proc_per_worker=n_proc,
+            seed=seed,
+        )
+
+        self.unseen_dset = CachedByKeyDataset(
+            dset_cls=Flowbot3DPyGDataset,
+            dset_kwargs=dict(
+                root=os.path.join(root, "raw"),
+                split="umpnet-test",
+                randomize_camera=randomize_camera,
+            ),
+            data_keys=rpd.UMPNET_TEST_OBJ_IDS,
+            root=root,
+            processed_dirname=Flowbot3DPyGDataset.get_processed_dir(
+                True,
+                randomize_camera,
+            ),
+            n_repeat=1,
+            n_workers=num_workers,
+            n_proc_per_worker=n_proc,
+            seed=seed,
+        )
+
+    def train_dataloader(self, shuffle=True):
+        if shuffle:
+            L.seed_everything(self.seed)
+        return tgl.DataLoader(
+            self.train_dset, self.batch_size, shuffle=shuffle, num_workers=0
+        )
+
+    def train_val_dataloader(self):
+        L.seed_everything(self.seed)
+        return tgl.DataLoader(
+            self.train_val_dset, self.batch_size, shuffle=False, num_workers=0
+        )
+
+    def val_dataloader(self):
+        return tgl.DataLoader(
+            self.val_dset, self.batch_size, shuffle=False, num_workers=0
+        )
+
+    def unseen_dataloader(self):
+        return tgl.DataLoader(
+            self.unseen_dset, self.batch_size, shuffle=False, num_workers=0
+        )
diff --git a/src/flowbothd/metrics/__init__.py b/src/flowbothd/metrics/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/src/flowbothd/metrics/trajectory.py b/src/flowbothd/metrics/trajectory.py
new file mode 100644
index 0000000..cdaa960
--- /dev/null
+++ b/src/flowbothd/metrics/trajectory.py
@@ -0,0 +1,62 @@
+import torch
+
+
+def normalize_trajectory(pred):  # pred: bs * 1200, traj_len, 3
+    pred = pred.reshape(-1, 1200, pred.shape[1], pred.shape[2])
+    norm = pred.norm(p=2, dim=-1)
+    norm = torch.max(norm, dim=1).values + 1e-6
+    pred = pred / norm[:, None, :, None]
+    return torch.flatten(pred, start_dim=0, end_dim=1)  # bs * 1200, traj_len, 3
+
+
+def flow_metrics(
+    pred_flow, gt_flow, reduce=True
+):  # if reduce = True, return mean, else return everything
+    with torch.no_grad():
+        # pred_flow = normalize_trajectory(pred_flow)
+
+        # RMSE
+        rmse = (pred_flow - gt_flow).norm(p=2, dim=-1)  # .mean()
+
+        # Cosine similarity, normalized.
+        nonzero_gt_flowixs = torch.where(gt_flow.norm(dim=-1) != 0.0)
+        gt_flow_nz = gt_flow[nonzero_gt_flowixs]
+        pred_flow_nz = pred_flow[nonzero_gt_flowixs]
+
+        cos_dist = torch.cosine_similarity(pred_flow_nz, gt_flow_nz, dim=-1)  # .mean()
+        # print(pred_flow_nz, gt_flow_nz, cos_dist)
+
+        # Magnitude
+        mag_error = (
+            pred_flow.norm(p=2, dim=-1) - gt_flow.norm(p=2, dim=-1)
+        ).abs()  # .mean()
+
+    if reduce:
+        # print(rmse, cos_dist, mag_error)
+        return rmse.mean(), cos_dist.mean(), mag_error.mean()
+    else:
+        return rmse, cos_dist, mag_error
+
+
+def artflownet_loss(
+    f_pred: torch.Tensor,
+    f_target: torch.Tensor,
+    n_nodes: torch.Tensor,
+    reduce: bool = True,
+) -> torch.Tensor:  # if reduce = True, return mean, else return everything
+    # f_pred = normalize_trajectory(f_pred)
+
+    # Flow loss, per-point.
+    raw_se = ((f_pred - f_target) ** 2).sum(dim=-1)
+
+    if reduce:
+        weights = (1 / n_nodes).repeat_interleave(n_nodes)
+        l_se = (raw_se * weights[:, None]).sum() / f_pred.shape[1]  # Trajectory length
+
+        # Full loss, averaged across the batch.
+        loss: torch.Tensor = l_se / len(n_nodes)
+
+        return loss
+
+    else:
+        return raw_se
diff --git a/src/flowbothd/models/__init__.py b/src/flowbothd/models/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/src/flowbothd/models/dit_utils/__init__.py b/src/flowbothd/models/dit_utils/__init__.py
new file mode 100644
index 0000000..fdc4902
--- /dev/null
+++ b/src/flowbothd/models/dit_utils/__init__.py
@@ -0,0 +1,46 @@
+# Modified from OpenAI's diffusion repos
+#     GLIDE: https://github.com/openai/glide-text2im/blob/main/glide_text2im/gaussian_diffusion.py
+#     ADM:   https://github.com/openai/guided-diffusion/blob/main/guided_diffusion
+#     IDDPM: https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py
+
+from . import gaussian_diffusion as gd
+from .respace import SpacedDiffusion, space_timesteps
+
+
+def create_diffusion(
+    timestep_respacing,
+    noise_schedule="linear",
+    use_kl=False,
+    sigma_small=False,
+    predict_xstart=False,
+    learn_sigma=True,
+    rescale_learned_sigmas=False,
+    diffusion_steps=1000,
+):
+    betas = gd.get_named_beta_schedule(noise_schedule, diffusion_steps)
+    if use_kl:
+        loss_type = gd.LossType.RESCALED_KL
+    elif rescale_learned_sigmas:
+        loss_type = gd.LossType.RESCALED_MSE
+    else:
+        loss_type = gd.LossType.MSE
+    if timestep_respacing is None or timestep_respacing == "":
+        timestep_respacing = [diffusion_steps]
+    return SpacedDiffusion(
+        use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),
+        betas=betas,
+        model_mean_type=(
+            gd.ModelMeanType.EPSILON if not predict_xstart else gd.ModelMeanType.START_X
+        ),
+        model_var_type=(
+            (
+                gd.ModelVarType.FIXED_LARGE
+                if not sigma_small
+                else gd.ModelVarType.FIXED_SMALL
+            )
+            if not learn_sigma
+            else gd.ModelVarType.LEARNED_RANGE
+        ),
+        loss_type=loss_type
+        # rescale_timesteps=rescale_timesteps,
+    )
diff --git a/src/flowbothd/models/dit_utils/diffusion_utils.py b/src/flowbothd/models/dit_utils/diffusion_utils.py
new file mode 100644
index 0000000..786d194
--- /dev/null
+++ b/src/flowbothd/models/dit_utils/diffusion_utils.py
@@ -0,0 +1,90 @@
+# Modified from OpenAI's diffusion repos
+#     GLIDE: https://github.com/openai/glide-text2im/blob/main/glide_text2im/gaussian_diffusion.py
+#     ADM:   https://github.com/openai/guided-diffusion/blob/main/guided_diffusion
+#     IDDPM: https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py
+
+import numpy as np
+import torch as th
+
+
+def normal_kl(mean1, logvar1, mean2, logvar2):
+    """
+    Compute the KL divergence between two gaussians.
+    Shapes are automatically broadcasted, so batches can be compared to
+    scalars, among other use cases.
+    """
+    tensor = None
+    for obj in (mean1, logvar1, mean2, logvar2):
+        if isinstance(obj, th.Tensor):
+            tensor = obj
+            break
+    assert tensor is not None, "at least one argument must be a Tensor"
+
+    # Force variances to be Tensors. Broadcasting helps convert scalars to
+    # Tensors, but it does not work for th.exp().
+    logvar1, logvar2 = [
+        x if isinstance(x, th.Tensor) else th.tensor(x).to(tensor)
+        for x in (logvar1, logvar2)
+    ]
+
+    return 0.5 * (
+        -1.0
+        + logvar2
+        - logvar1
+        + th.exp(logvar1 - logvar2)
+        + ((mean1 - mean2) ** 2) * th.exp(-logvar2)
+    )
+
+
+def approx_standard_normal_cdf(x):
+    """
+    A fast approximation of the cumulative distribution function of the
+    standard normal.
+    """
+    return 0.5 * (1.0 + th.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * th.pow(x, 3))))
+
+
+def continuous_gaussian_log_likelihood(x, *, means, log_scales):
+    """
+    Compute the log-likelihood of a continuous Gaussian distribution.
+    :param x: the targets
+    :param means: the Gaussian mean Tensor.
+    :param log_scales: the Gaussian log stddev Tensor.
+    :return: a tensor like x of log probabilities (in nats).
+    """
+    centered_x = x - means
+    inv_stdv = th.exp(-log_scales)
+    normalized_x = centered_x * inv_stdv
+    log_probs = th.distributions.Normal(th.zeros_like(x), th.ones_like(x)).log_prob(
+        normalized_x
+    )
+    return log_probs
+
+
+def discretized_gaussian_log_likelihood(x, *, means, log_scales):
+    """
+    Compute the log-likelihood of a Gaussian distribution discretizing to a
+    given image.
+    :param x: the target images. It is assumed that this was uint8 values,
+              rescaled to the range [-1, 1].
+    :param means: the Gaussian mean Tensor.
+    :param log_scales: the Gaussian log stddev Tensor.
+    :return: a tensor like x of log probabilities (in nats).
+    """
+    assert x.shape == means.shape == log_scales.shape
+    centered_x = x - means
+    inv_stdv = th.exp(-log_scales)
+    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)
+    cdf_plus = approx_standard_normal_cdf(plus_in)
+    min_in = inv_stdv * (centered_x - 1.0 / 255.0)
+    cdf_min = approx_standard_normal_cdf(min_in)
+    log_cdf_plus = th.log(cdf_plus.clamp(min=1e-12))
+    log_one_minus_cdf_min = th.log((1.0 - cdf_min).clamp(min=1e-12))
+    cdf_delta = cdf_plus - cdf_min
+    log_probs = th.where(
+        x < -0.999,
+        log_cdf_plus,
+        th.where(x > 0.999, log_one_minus_cdf_min, th.log(cdf_delta.clamp(min=1e-12))),
+    )
+    assert log_probs.shape == x.shape
+    return log_probs
diff --git a/src/flowbothd/models/dit_utils/gaussian_diffusion.py b/src/flowbothd/models/dit_utils/gaussian_diffusion.py
new file mode 100644
index 0000000..8831782
--- /dev/null
+++ b/src/flowbothd/models/dit_utils/gaussian_diffusion.py
@@ -0,0 +1,902 @@
+# Modified from OpenAI's diffusion repos
+#     GLIDE: https://github.com/openai/glide-text2im/blob/main/glide_text2im/gaussian_diffusion.py
+#     ADM:   https://github.com/openai/guided-diffusion/blob/main/guided_diffusion
+#     IDDPM: https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py
+
+
+import enum
+import math
+
+import numpy as np
+import torch as th
+
+from .diffusion_utils import discretized_gaussian_log_likelihood, normal_kl
+
+
+def mean_flat(tensor):
+    """
+    Take the mean over all non-batch dimensions.
+    """
+    return tensor.mean(dim=list(range(1, len(tensor.shape))))
+
+
+class ModelMeanType(enum.Enum):
+    """
+    Which type of output the model predicts.
+    """
+
+    PREVIOUS_X = enum.auto()  # the model predicts x_{t-1}
+    START_X = enum.auto()  # the model predicts x_0
+    EPSILON = enum.auto()  # the model predicts epsilon
+
+
+class ModelVarType(enum.Enum):
+    """
+    What is used as the model's output variance.
+    The LEARNED_RANGE option has been added to allow the model to predict
+    values between FIXED_SMALL and FIXED_LARGE, making its job easier.
+    """
+
+    LEARNED = enum.auto()
+    FIXED_SMALL = enum.auto()
+    FIXED_LARGE = enum.auto()
+    LEARNED_RANGE = enum.auto()
+
+
+class LossType(enum.Enum):
+    MSE = enum.auto()  # use raw MSE loss (and KL when learning variances)
+    RESCALED_MSE = (
+        enum.auto()
+    )  # use raw MSE loss (with RESCALED_KL when learning variances)
+    KL = enum.auto()  # use the variational lower-bound
+    RESCALED_KL = enum.auto()  # like KL, but rescale to estimate the full VLB
+
+    def is_vb(self):
+        return self == LossType.KL or self == LossType.RESCALED_KL
+
+
+def _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, warmup_frac):
+    betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)
+    warmup_time = int(num_diffusion_timesteps * warmup_frac)
+    betas[:warmup_time] = np.linspace(
+        beta_start, beta_end, warmup_time, dtype=np.float64
+    )
+    return betas
+
+
+def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):
+    """
+    This is the deprecated API for creating beta schedules.
+    See get_named_beta_schedule() for the new library of schedules.
+    """
+    if beta_schedule == "quad":
+        betas = (
+            np.linspace(
+                beta_start**0.5,
+                beta_end**0.5,
+                num_diffusion_timesteps,
+                dtype=np.float64,
+            )
+            ** 2
+        )
+    elif beta_schedule == "linear":
+        betas = np.linspace(
+            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64
+        )
+    elif beta_schedule == "warmup10":
+        betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)
+    elif beta_schedule == "warmup50":
+        betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)
+    elif beta_schedule == "const":
+        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)
+    elif beta_schedule == "jsd":  # 1/T, 1/(T-1), 1/(T-2), ..., 1
+        betas = 1.0 / np.linspace(
+            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64
+        )
+    else:
+        raise NotImplementedError(beta_schedule)
+    assert betas.shape == (num_diffusion_timesteps,)
+    return betas
+
+
+def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):
+    """
+    Get a pre-defined beta schedule for the given name.
+    The beta schedule library consists of beta schedules which remain similar
+    in the limit of num_diffusion_timesteps.
+    Beta schedules may be added, but should not be removed or changed once
+    they are committed to maintain backwards compatibility.
+    """
+    if schedule_name == "linear":
+        # Linear schedule from Ho et al, extended to work for any number of
+        # diffusion steps.
+        scale = 1000 / num_diffusion_timesteps
+        return get_beta_schedule(
+            "linear",
+            beta_start=scale * 0.0001,
+            beta_end=scale * 0.02,
+            num_diffusion_timesteps=num_diffusion_timesteps,
+        )
+    elif schedule_name == "squaredcos_cap_v2":
+        return betas_for_alpha_bar(
+            num_diffusion_timesteps,
+            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,
+        )
+    else:
+        raise NotImplementedError(f"unknown beta schedule: {schedule_name}")
+
+
+def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):
+    """
+    Create a beta schedule that discretizes the given alpha_t_bar function,
+    which defines the cumulative product of (1-beta) over time from t = [0,1].
+    :param num_diffusion_timesteps: the number of betas to produce.
+    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and
+                      produces the cumulative product of (1-beta) up to that
+                      part of the diffusion process.
+    :param max_beta: the maximum beta to use; use values lower than 1 to
+                     prevent singularities.
+    """
+    betas = []
+    for i in range(num_diffusion_timesteps):
+        t1 = i / num_diffusion_timesteps
+        t2 = (i + 1) / num_diffusion_timesteps
+        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))
+    return np.array(betas)
+
+
+class GaussianDiffusion:
+    """
+    Utilities for training and sampling diffusion models.
+    Original ported from this codebase:
+    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42
+    :param betas: a 1-D numpy array of betas for each diffusion timestep,
+                  starting at T and going to 1.
+    """
+
+    def __init__(self, *, betas, model_mean_type, model_var_type, loss_type):
+        self.model_mean_type = model_mean_type
+        self.model_var_type = model_var_type
+        self.loss_type = loss_type
+
+        # Use float64 for accuracy.
+        betas = np.array(betas, dtype=np.float64)
+        self.betas = betas
+        assert len(betas.shape) == 1, "betas must be 1-D"
+        assert (betas > 0).all() and (betas <= 1).all()
+
+        self.num_timesteps = int(betas.shape[0])
+
+        alphas = 1.0 - betas
+        self.alphas_cumprod = np.cumprod(alphas, axis=0)
+        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])
+        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)
+        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)
+
+        # calculations for diffusion q(x_t | x_{t-1}) and others
+        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)
+        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)
+        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)
+        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)
+        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)
+
+        # calculations for posterior q(x_{t-1} | x_t, x_0)
+        self.posterior_variance = (
+            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
+        )
+        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain
+        self.posterior_log_variance_clipped = (
+            np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:]))
+            if len(self.posterior_variance) > 1
+            else np.array([])
+        )
+
+        self.posterior_mean_coef1 = (
+            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
+        )
+        self.posterior_mean_coef2 = (
+            (1.0 - self.alphas_cumprod_prev)
+            * np.sqrt(alphas)
+            / (1.0 - self.alphas_cumprod)
+        )
+
+    def q_mean_variance(self, x_start, t):
+        """
+        Get the distribution q(x_t | x_0).
+        :param x_start: the [N x C x ...] tensor of noiseless inputs.
+        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.
+        :return: A tuple (mean, variance, log_variance), all of x_start's shape.
+        """
+        mean = (
+            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start
+        )
+        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)
+        log_variance = _extract_into_tensor(
+            self.log_one_minus_alphas_cumprod, t, x_start.shape
+        )
+        return mean, variance, log_variance
+
+    def q_sample(self, x_start, t, noise=None):
+        """
+        Diffuse the data for a given number of diffusion steps.
+        In other words, sample from q(x_t | x_0).
+        :param x_start: the initial data batch.
+        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.
+        :param noise: if specified, the split-out normal noise.
+        :return: A noisy version of x_start.
+        """
+        if noise is None:
+            noise = th.randn_like(x_start)
+        assert noise.shape == x_start.shape
+        return (
+            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start
+            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)
+            * noise
+        )
+
+    def q_posterior_mean_variance(self, x_start, x_t, t):
+        """
+        Compute the mean and variance of the diffusion posterior:
+            q(x_{t-1} | x_t, x_0)
+        """
+        assert x_start.shape == x_t.shape
+        posterior_mean = (
+            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start
+            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t
+        )
+        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)
+        posterior_log_variance_clipped = _extract_into_tensor(
+            self.posterior_log_variance_clipped, t, x_t.shape
+        )
+        assert (
+            posterior_mean.shape[0]
+            == posterior_variance.shape[0]
+            == posterior_log_variance_clipped.shape[0]
+            == x_start.shape[0]
+        )
+        return posterior_mean, posterior_variance, posterior_log_variance_clipped
+
+    def p_mean_variance(
+        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None
+    ):
+        """
+        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of
+        the initial x, x_0.
+        :param model: the model, which takes a signal and a batch of timesteps
+                      as input.
+        :param x: the [N x C x ...] tensor at time t.
+        :param t: a 1-D Tensor of timesteps.
+        :param clip_denoised: if True, clip the denoised signal into [-1, 1].
+        :param denoised_fn: if not None, a function which applies to the
+            x_start prediction before it is used to sample. Applies before
+            clip_denoised.
+        :param model_kwargs: if not None, a dict of extra keyword arguments to
+            pass to the model. This can be used for conditioning.
+        :return: a dict with the following keys:
+                 - 'mean': the model mean output.
+                 - 'variance': the model variance output.
+                 - 'log_variance': the log of 'variance'.
+                 - 'pred_xstart': the prediction for x_0.
+        """
+        if model_kwargs is None:
+            model_kwargs = {}
+
+        B, C = x.shape[:2]
+        assert t.shape == (B,)
+        model_output = model(x, t, **model_kwargs)
+        if isinstance(model_output, tuple):
+            model_output, extra = model_output
+        else:
+            extra = None
+
+        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:
+            assert model_output.shape == (B, C * 2, *x.shape[2:])
+            model_output, model_var_values = th.split(model_output, C, dim=1)
+            min_log = _extract_into_tensor(
+                self.posterior_log_variance_clipped, t, x.shape
+            )
+            max_log = _extract_into_tensor(np.log(self.betas), t, x.shape)
+            # The model_var_values is [-1, 1] for [min_var, max_var].
+            frac = (model_var_values + 1) / 2
+            model_log_variance = frac * max_log + (1 - frac) * min_log
+            model_variance = th.exp(model_log_variance)
+        else:
+            model_variance, model_log_variance = {
+                # for fixedlarge, we set the initial (log-)variance like so
+                # to get a better decoder log likelihood.
+                ModelVarType.FIXED_LARGE: (
+                    np.append(self.posterior_variance[1], self.betas[1:]),
+                    np.log(np.append(self.posterior_variance[1], self.betas[1:])),
+                ),
+                ModelVarType.FIXED_SMALL: (
+                    self.posterior_variance,
+                    self.posterior_log_variance_clipped,
+                ),
+            }[self.model_var_type]
+            model_variance = _extract_into_tensor(model_variance, t, x.shape)
+            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)
+
+        def process_xstart(x):
+            if denoised_fn is not None:
+                x = denoised_fn(x)
+            if clip_denoised:
+                return x.clamp(-1, 1)
+            return x
+
+        if self.model_mean_type == ModelMeanType.START_X:
+            pred_xstart = process_xstart(model_output)
+        else:
+            pred_xstart = process_xstart(
+                self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)
+            )
+        model_mean, _, _ = self.q_posterior_mean_variance(
+            x_start=pred_xstart, x_t=x, t=t
+        )
+
+        assert (
+            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape
+        )
+        return {
+            "mean": model_mean,
+            "variance": model_variance,
+            "log_variance": model_log_variance,
+            "pred_xstart": pred_xstart,
+            "extra": extra,
+        }
+
+    def _predict_xstart_from_eps(self, x_t, t, eps):
+        assert x_t.shape == eps.shape
+        return (
+            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t
+            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps
+        )
+
+    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):
+        return (
+            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t
+            - pred_xstart
+        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)
+
+    def condition_mean(self, cond_fn, p_mean_var, x, t, model_kwargs=None):
+        """
+        Compute the mean for the previous step, given a function cond_fn that
+        computes the gradient of a conditional log probability with respect to
+        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to
+        condition on y.
+        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).
+        """
+        gradient = cond_fn(x, t, **model_kwargs)
+        new_mean = (
+            p_mean_var["mean"].float() + p_mean_var["variance"] * gradient.float()
+        )
+        return new_mean
+
+    def condition_score(self, cond_fn, p_mean_var, x, t, model_kwargs=None):
+        """
+        Compute what the p_mean_variance output would have been, should the
+        model's score function be conditioned by cond_fn.
+        See condition_mean() for details on cond_fn.
+        Unlike condition_mean(), this instead uses the conditioning strategy
+        from Song et al (2020).
+        """
+        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)
+
+        eps = self._predict_eps_from_xstart(x, t, p_mean_var["pred_xstart"])
+        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)
+
+        out = p_mean_var.copy()
+        out["pred_xstart"] = self._predict_xstart_from_eps(x, t, eps)
+        out["mean"], _, _ = self.q_posterior_mean_variance(
+            x_start=out["pred_xstart"], x_t=x, t=t
+        )
+        return out
+
+    def p_sample(
+        self,
+        model,
+        x,
+        t,
+        clip_denoised=True,
+        denoised_fn=None,
+        cond_fn=None,
+        model_kwargs=None,
+    ):
+        """
+        Sample x_{t-1} from the model at the given timestep.
+        :param model: the model to sample from.
+        :param x: the current tensor at x_{t-1}.
+        :param t: the value of t, starting at 0 for the first diffusion step.
+        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].
+        :param denoised_fn: if not None, a function which applies to the
+            x_start prediction before it is used to sample.
+        :param cond_fn: if not None, this is a gradient function that acts
+                        similarly to the model.
+        :param model_kwargs: if not None, a dict of extra keyword arguments to
+            pass to the model. This can be used for conditioning.
+        :return: a dict containing the following keys:
+                 - 'sample': a random sample from the model.
+                 - 'pred_xstart': a prediction of x_0.
+        """
+        out = self.p_mean_variance(
+            model,
+            x,
+            t,
+            clip_denoised=clip_denoised,
+            denoised_fn=denoised_fn,
+            model_kwargs=model_kwargs,
+        )
+        noise = th.randn_like(x)
+        nonzero_mask = (
+            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))
+        )  # no noise when t == 0
+        if cond_fn is not None:
+            out["mean"] = self.condition_mean(
+                cond_fn, out, x, t, model_kwargs=model_kwargs
+            )
+        sample = out["mean"] + nonzero_mask * th.exp(0.5 * out["log_variance"]) * noise
+        return {"sample": sample, "pred_xstart": out["pred_xstart"]}
+
+    def p_sample_loop(
+        self,
+        model,
+        shape,
+        noise=None,
+        clip_denoised=True,
+        denoised_fn=None,
+        cond_fn=None,
+        model_kwargs=None,
+        device=None,
+        progress=False,
+    ):
+        """
+        Generate samples from the model.
+        :param model: the model module.
+        :param shape: the shape of the samples, (N, C, H, W).
+        :param noise: if specified, the noise from the encoder to sample.
+                      Should be of the same shape as `shape`.
+        :param clip_denoised: if True, clip x_start predictions to [-1, 1].
+        :param denoised_fn: if not None, a function which applies to the
+            x_start prediction before it is used to sample.
+        :param cond_fn: if not None, this is a gradient function that acts
+                        similarly to the model.
+        :param model_kwargs: if not None, a dict of extra keyword arguments to
+            pass to the model. This can be used for conditioning.
+        :param device: if specified, the device to create the samples on.
+                       If not specified, use a model parameter's device.
+        :param progress: if True, show a tqdm progress bar.
+        :return: a non-differentiable batch of samples.
+        """
+        final = None
+        results = [noise]
+        for sample in self.p_sample_loop_progressive(
+            model,
+            shape,
+            noise=noise,
+            clip_denoised=clip_denoised,
+            denoised_fn=denoised_fn,
+            cond_fn=cond_fn,
+            model_kwargs=model_kwargs,
+            device=device,
+            progress=progress,
+        ):
+            final = sample
+            results.append(final["sample"])
+
+        return final["sample"], results
+
+    def p_sample_loop_progressive(
+        self,
+        model,
+        shape,
+        noise=None,
+        clip_denoised=True,
+        denoised_fn=None,
+        cond_fn=None,
+        model_kwargs=None,
+        device=None,
+        progress=False,
+    ):
+        """
+        Generate samples from the model and yield intermediate samples from
+        each timestep of diffusion.
+        Arguments are the same as p_sample_loop().
+        Returns a generator over dicts, where each dict is the return value of
+        p_sample().
+        """
+        if device is None:
+            device = next(model.parameters()).device
+        assert isinstance(shape, (tuple, list))
+        if noise is not None:
+            img = noise
+        else:
+            img = th.randn(*shape, device=device)
+        indices = list(range(self.num_timesteps))[::-1]
+
+        if progress:
+            # Lazy import so that we don't depend on tqdm.
+            pass
+
+            # indices = tqdm(indices)
+
+        for i in indices:
+            t = th.tensor([i] * shape[0], device=device)
+            with th.no_grad():
+                out = self.p_sample(
+                    model,
+                    img,
+                    t,
+                    clip_denoised=clip_denoised,
+                    denoised_fn=denoised_fn,
+                    cond_fn=cond_fn,
+                    model_kwargs=model_kwargs,
+                )
+                yield out
+                img = out["sample"]
+
+        # return results
+
+    def ddim_sample(
+        self,
+        model,
+        x,
+        t,
+        clip_denoised=True,
+        denoised_fn=None,
+        cond_fn=None,
+        model_kwargs=None,
+        eta=0.0,
+    ):
+        """
+        Sample x_{t-1} from the model using DDIM.
+        Same usage as p_sample().
+        """
+        out = self.p_mean_variance(
+            model,
+            x,
+            t,
+            clip_denoised=clip_denoised,
+            denoised_fn=denoised_fn,
+            model_kwargs=model_kwargs,
+        )
+        if cond_fn is not None:
+            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)
+
+        # Usually our model outputs epsilon, but we re-derive it
+        # in case we used x_start or x_prev prediction.
+        eps = self._predict_eps_from_xstart(x, t, out["pred_xstart"])
+
+        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)
+        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)
+        sigma = (
+            eta
+            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))
+            * th.sqrt(1 - alpha_bar / alpha_bar_prev)
+        )
+        # Equation 12.
+        noise = th.randn_like(x)
+        mean_pred = (
+            out["pred_xstart"] * th.sqrt(alpha_bar_prev)
+            + th.sqrt(1 - alpha_bar_prev - sigma**2) * eps
+        )
+        nonzero_mask = (
+            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))
+        )  # no noise when t == 0
+        sample = mean_pred + nonzero_mask * sigma * noise
+        return {"sample": sample, "pred_xstart": out["pred_xstart"]}
+
+    def ddim_reverse_sample(
+        self,
+        model,
+        x,
+        t,
+        clip_denoised=True,
+        denoised_fn=None,
+        cond_fn=None,
+        model_kwargs=None,
+        eta=0.0,
+    ):
+        """
+        Sample x_{t+1} from the model using DDIM reverse ODE.
+        """
+        assert eta == 0.0, "Reverse ODE only for deterministic path"
+        out = self.p_mean_variance(
+            model,
+            x,
+            t,
+            clip_denoised=clip_denoised,
+            denoised_fn=denoised_fn,
+            model_kwargs=model_kwargs,
+        )
+        if cond_fn is not None:
+            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)
+        # Usually our model outputs epsilon, but we re-derive it
+        # in case we used x_start or x_prev prediction.
+        eps = (
+            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x
+            - out["pred_xstart"]
+        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)
+        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)
+
+        # Equation 12. reversed
+        mean_pred = (
+            out["pred_xstart"] * th.sqrt(alpha_bar_next)
+            + th.sqrt(1 - alpha_bar_next) * eps
+        )
+
+        return {"sample": mean_pred, "pred_xstart": out["pred_xstart"]}
+
+    def ddim_sample_loop(
+        self,
+        model,
+        shape,
+        noise=None,
+        clip_denoised=True,
+        denoised_fn=None,
+        cond_fn=None,
+        model_kwargs=None,
+        device=None,
+        progress=False,
+        eta=0.0,
+    ):
+        """
+        Generate samples from the model using DDIM.
+        Same usage as p_sample_loop().
+        """
+        final = None
+        for sample in self.ddim_sample_loop_progressive(
+            model,
+            shape,
+            noise=noise,
+            clip_denoised=clip_denoised,
+            denoised_fn=denoised_fn,
+            cond_fn=cond_fn,
+            model_kwargs=model_kwargs,
+            device=device,
+            progress=progress,
+            eta=eta,
+        ):
+            final = sample
+        return final["sample"]
+
+    def ddim_sample_loop_progressive(
+        self,
+        model,
+        shape,
+        noise=None,
+        clip_denoised=True,
+        denoised_fn=None,
+        cond_fn=None,
+        model_kwargs=None,
+        device=None,
+        progress=False,
+        eta=0.0,
+    ):
+        """
+        Use DDIM to sample from the model and yield intermediate samples from
+        each timestep of DDIM.
+        Same usage as p_sample_loop_progressive().
+        """
+        if device is None:
+            device = next(model.parameters()).device
+        assert isinstance(shape, (tuple, list))
+        if noise is not None:
+            img = noise
+        else:
+            img = th.randn(*shape, device=device)
+        indices = list(range(self.num_timesteps))[::-1]
+
+        if progress:
+            # Lazy import so that we don't depend on tqdm.
+            pass
+
+            # indices = tqdm(indices)
+
+        for i in indices:
+            t = th.tensor([i] * shape[0], device=device)
+            with th.no_grad():
+                out = self.ddim_sample(
+                    model,
+                    img,
+                    t,
+                    clip_denoised=clip_denoised,
+                    denoised_fn=denoised_fn,
+                    cond_fn=cond_fn,
+                    model_kwargs=model_kwargs,
+                    eta=eta,
+                )
+                yield out
+                img = out["sample"]
+
+    def _vb_terms_bpd(
+        self, model, x_start, x_t, t, clip_denoised=True, model_kwargs=None
+    ):
+        """
+        Get a term for the variational lower-bound.
+        The resulting units are bits (rather than nats, as one might expect).
+        This allows for comparison to other papers.
+        :return: a dict with the following keys:
+                 - 'output': a shape [N] tensor of NLLs or KLs.
+                 - 'pred_xstart': the x_0 predictions.
+        """
+        true_mean, _, true_log_variance_clipped = self.q_posterior_mean_variance(
+            x_start=x_start, x_t=x_t, t=t
+        )
+        out = self.p_mean_variance(
+            model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs
+        )
+        kl = normal_kl(
+            true_mean, true_log_variance_clipped, out["mean"], out["log_variance"]
+        )
+        kl = mean_flat(kl) / np.log(2.0)
+
+        decoder_nll = -discretized_gaussian_log_likelihood(
+            x_start, means=out["mean"], log_scales=0.5 * out["log_variance"]
+        )
+        assert decoder_nll.shape == x_start.shape
+        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)
+
+        # At the first timestep return the decoder NLL,
+        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))
+        output = th.where((t == 0), decoder_nll, kl)
+        return {"output": output, "pred_xstart": out["pred_xstart"]}
+
+    def training_losses(self, model, x_start, t, model_kwargs=None, noise=None):
+        """
+        Compute training losses for a single timestep.
+        :param model: the model to evaluate loss on.
+        :param x_start: the [N x C x ...] tensor of inputs.
+        :param t: a batch of timestep indices.
+        :param model_kwargs: if not None, a dict of extra keyword arguments to
+            pass to the model. This can be used for conditioning.
+        :param noise: if specified, the specific Gaussian noise to try to remove.
+        :return: a dict with the key "loss" containing a tensor of shape [N].
+                 Some mean or variance settings may also have other keys.
+        """
+        if model_kwargs is None:
+            model_kwargs = {}
+        if noise is None:
+            noise = th.randn_like(x_start)
+        x_t = self.q_sample(x_start, t, noise=noise)
+
+        terms = {}
+
+        if self.loss_type == LossType.KL or self.loss_type == LossType.RESCALED_KL:
+            terms["loss"] = self._vb_terms_bpd(
+                model=model,
+                x_start=x_start,
+                x_t=x_t,
+                t=t,
+                clip_denoised=False,
+                model_kwargs=model_kwargs,
+            )["output"]
+            if self.loss_type == LossType.RESCALED_KL:
+                terms["loss"] *= self.num_timesteps
+        elif self.loss_type == LossType.MSE or self.loss_type == LossType.RESCALED_MSE:
+            model_output = model(x_t, t, **model_kwargs)
+
+            if self.model_var_type in [
+                ModelVarType.LEARNED,
+                ModelVarType.LEARNED_RANGE,
+            ]:
+                B, C = x_t.shape[:2]
+                assert model_output.shape == (B, C * 2, *x_t.shape[2:])
+                model_output, model_var_values = th.split(model_output, C, dim=1)
+                # Learn the variance using the variational bound, but don't let
+                # it affect our mean prediction.
+                frozen_out = th.cat([model_output.detach(), model_var_values], dim=1)
+                terms["vb"] = self._vb_terms_bpd(
+                    model=lambda *args, r=frozen_out: r,
+                    x_start=x_start,
+                    x_t=x_t,
+                    t=t,
+                    clip_denoised=False,
+                )["output"]
+                if self.loss_type == LossType.RESCALED_MSE:
+                    # Divide by 1000 for equivalence with initial implementation.
+                    # Without a factor of 1/1000, the VB term hurts the MSE term.
+                    terms["vb"] *= self.num_timesteps / 1000.0
+
+            target = {
+                ModelMeanType.PREVIOUS_X: self.q_posterior_mean_variance(
+                    x_start=x_start, x_t=x_t, t=t
+                )[0],
+                ModelMeanType.START_X: x_start,
+                ModelMeanType.EPSILON: noise,
+            }[self.model_mean_type]
+            assert model_output.shape == target.shape == x_start.shape
+            # print(target[0, :, 0, 0], model_output[0, :, 0, 0])
+            terms["mse"] = mean_flat((target - model_output) ** 2)
+            if "vb" in terms:
+                terms["loss"] = terms["mse"] + terms["vb"]
+            else:
+                terms["loss"] = terms["mse"]
+        else:
+            raise NotImplementedError(self.loss_type)
+
+        return terms
+
+    def _prior_bpd(self, x_start):
+        """
+        Get the prior KL term for the variational lower-bound, measured in
+        bits-per-dim.
+        This term can't be optimized, as it only depends on the encoder.
+        :param x_start: the [N x C x ...] tensor of inputs.
+        :return: a batch of [N] KL values (in bits), one per batch element.
+        """
+        batch_size = x_start.shape[0]
+        t = th.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)
+        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)
+        kl_prior = normal_kl(
+            mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0
+        )
+        return mean_flat(kl_prior) / np.log(2.0)
+
+    def calc_bpd_loop(self, model, x_start, clip_denoised=True, model_kwargs=None):
+        """
+        Compute the entire variational lower-bound, measured in bits-per-dim,
+        as well as other related quantities.
+        :param model: the model to evaluate loss on.
+        :param x_start: the [N x C x ...] tensor of inputs.
+        :param clip_denoised: if True, clip denoised samples.
+        :param model_kwargs: if not None, a dict of extra keyword arguments to
+            pass to the model. This can be used for conditioning.
+        :return: a dict containing the following keys:
+                 - total_bpd: the total variational lower-bound, per batch element.
+                 - prior_bpd: the prior term in the lower-bound.
+                 - vb: an [N x T] tensor of terms in the lower-bound.
+                 - xstart_mse: an [N x T] tensor of x_0 MSEs for each timestep.
+                 - mse: an [N x T] tensor of epsilon MSEs for each timestep.
+        """
+        device = x_start.device
+        batch_size = x_start.shape[0]
+
+        vb = []
+        xstart_mse = []
+        mse = []
+        for t in list(range(self.num_timesteps))[::-1]:
+            t_batch = th.tensor([t] * batch_size, device=device)
+            noise = th.randn_like(x_start)
+            x_t = self.q_sample(x_start=x_start, t=t_batch, noise=noise)
+            # Calculate VLB term at the current timestep
+            with th.no_grad():
+                out = self._vb_terms_bpd(
+                    model,
+                    x_start=x_start,
+                    x_t=x_t,
+                    t=t_batch,
+                    clip_denoised=clip_denoised,
+                    model_kwargs=model_kwargs,
+                )
+            vb.append(out["output"])
+            xstart_mse.append(mean_flat((out["pred_xstart"] - x_start) ** 2))
+            eps = self._predict_eps_from_xstart(x_t, t_batch, out["pred_xstart"])
+            mse.append(mean_flat((eps - noise) ** 2))
+
+        vb = th.stack(vb, dim=1)
+        xstart_mse = th.stack(xstart_mse, dim=1)
+        mse = th.stack(mse, dim=1)
+
+        prior_bpd = self._prior_bpd(x_start)
+        total_bpd = vb.sum(dim=1) + prior_bpd
+        return {
+            "total_bpd": total_bpd,
+            "prior_bpd": prior_bpd,
+            "vb": vb,
+            "xstart_mse": xstart_mse,
+            "mse": mse,
+        }
+
+
+def _extract_into_tensor(arr, timesteps, broadcast_shape):
+    """
+    Extract values from a 1-D numpy array for a batch of indices.
+    :param arr: the 1-D numpy array.
+    :param timesteps: a tensor of indices into the array to extract.
+    :param broadcast_shape: a larger shape of K dimensions with the batch
+                            dimension equal to the length of timesteps.
+    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.
+    """
+    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()
+    while len(res.shape) < len(broadcast_shape):
+        res = res[..., None]
+    return res + th.zeros(broadcast_shape, device=timesteps.device)
diff --git a/src/flowbothd/models/dit_utils/respace.py b/src/flowbothd/models/dit_utils/respace.py
new file mode 100644
index 0000000..252206b
--- /dev/null
+++ b/src/flowbothd/models/dit_utils/respace.py
@@ -0,0 +1,127 @@
+# Modified from OpenAI's diffusion repos
+#     GLIDE: https://github.com/openai/glide-text2im/blob/main/glide_text2im/gaussian_diffusion.py
+#     ADM:   https://github.com/openai/guided-diffusion/blob/main/guided_diffusion
+#     IDDPM: https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py
+
+import numpy as np
+import torch as th
+
+from .gaussian_diffusion import GaussianDiffusion
+
+
+def space_timesteps(num_timesteps, section_counts):
+    """
+    Create a list of timesteps to use from an original diffusion process,
+    given the number of timesteps we want to take from equally-sized portions
+    of the original process.
+    For example, if there's 300 timesteps and the section counts are [10,15,20]
+    then the first 100 timesteps are strided to be 10 timesteps, the second 100
+    are strided to be 15 timesteps, and the final 100 are strided to be 20.
+    If the stride is a string starting with "ddim", then the fixed striding
+    from the DDIM paper is used, and only one section is allowed.
+    :param num_timesteps: the number of diffusion steps in the original
+                          process to divide up.
+    :param section_counts: either a list of numbers, or a string containing
+                           comma-separated numbers, indicating the step count
+                           per section. As a special case, use "ddimN" where N
+                           is a number of steps to use the striding from the
+                           DDIM paper.
+    :return: a set of diffusion steps from the original process to use.
+    """
+    if isinstance(section_counts, str):
+        if section_counts.startswith("ddim"):
+            desired_count = int(section_counts[len("ddim") :])
+            for i in range(1, num_timesteps):
+                if len(range(0, num_timesteps, i)) == desired_count:
+                    return set(range(0, num_timesteps, i))
+            raise ValueError(
+                f"cannot create exactly {num_timesteps} steps with an integer stride"
+            )
+        section_counts = [int(x) for x in section_counts.split(",")]
+    size_per = num_timesteps // len(section_counts)
+    extra = num_timesteps % len(section_counts)
+    start_idx = 0
+    all_steps = []
+    for i, section_count in enumerate(section_counts):
+        size = size_per + (1 if i < extra else 0)
+        if size < section_count:
+            raise ValueError(
+                f"cannot divide section of {size} steps into {section_count}"
+            )
+        if section_count <= 1:
+            frac_stride = 1
+        else:
+            frac_stride = (size - 1) / (section_count - 1)
+        cur_idx = 0.0
+        taken_steps = []
+        for _ in range(section_count):
+            taken_steps.append(start_idx + round(cur_idx))
+            cur_idx += frac_stride
+        all_steps += taken_steps
+        start_idx += size
+    return set(all_steps)
+
+
+class SpacedDiffusion(GaussianDiffusion):
+    """
+    A diffusion process which can skip steps in a base diffusion process.
+    :param use_timesteps: a collection (sequence or set) of timesteps from the
+                          original diffusion process to retain.
+    :param kwargs: the kwargs to create the base diffusion process.
+    """
+
+    def __init__(self, use_timesteps, **kwargs):
+        self.use_timesteps = set(use_timesteps)
+        self.timestep_map = []
+        self.original_num_steps = len(kwargs["betas"])
+
+        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa
+        last_alpha_cumprod = 1.0
+        new_betas = []
+        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):
+            if i in self.use_timesteps:
+                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)
+                last_alpha_cumprod = alpha_cumprod
+                self.timestep_map.append(i)
+        kwargs["betas"] = np.array(new_betas)
+        super().__init__(**kwargs)
+
+    def p_mean_variance(
+        self, model, *args, **kwargs
+    ):  # pylint: disable=signature-differs
+        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)
+
+    def training_losses(
+        self, model, *args, **kwargs
+    ):  # pylint: disable=signature-differs
+        return super().training_losses(self._wrap_model(model), *args, **kwargs)
+
+    def condition_mean(self, cond_fn, *args, **kwargs):
+        return super().condition_mean(self._wrap_model(cond_fn), *args, **kwargs)
+
+    def condition_score(self, cond_fn, *args, **kwargs):
+        return super().condition_score(self._wrap_model(cond_fn), *args, **kwargs)
+
+    def _wrap_model(self, model):
+        if isinstance(model, _WrappedModel):
+            return model
+        return _WrappedModel(model, self.timestep_map, self.original_num_steps)
+
+    def _scale_timesteps(self, t):
+        # Scaling is done by the wrapped model.
+        return t
+
+
+class _WrappedModel:
+    def __init__(self, model, timestep_map, original_num_steps):
+        self.model = model
+        self.timestep_map = timestep_map
+        # self.rescale_timesteps = rescale_timesteps
+        self.original_num_steps = original_num_steps
+
+    def __call__(self, x, ts, **kwargs):
+        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)
+        new_ts = map_tensor[ts]
+        # if self.rescale_timesteps:
+        #     new_ts = new_ts.float() * (1000.0 / self.original_num_steps)
+        return self.model(x, new_ts, **kwargs)
diff --git a/src/flowbothd/models/flow_diffuser_dgdit.py b/src/flowbothd/models/flow_diffuser_dgdit.py
new file mode 100644
index 0000000..693a26a
--- /dev/null
+++ b/src/flowbothd/models/flow_diffuser_dgdit.py
@@ -0,0 +1,590 @@
+from typing import Any, Dict
+
+import lightning as L
+import plotly.express as px
+import plotly.graph_objects as go
+import rpad.visualize_3d.plots as v3p
+import torch
+import torch_geometric.data as tgd
+import tqdm
+from diffusers.optimization import get_cosine_schedule_with_warmup
+from plotly.subplots import make_subplots
+from torch import optim
+
+# from flowbothd.models.modules.dit_models import DiT
+from flowbothd.metrics.trajectory import (
+    artflownet_loss,
+    flow_metrics,
+    normalize_trajectory,
+)
+from flowbothd.models.dit_utils import create_diffusion
+
+
+# Flow predictor with DGCNN + DiT
+class FlowTrajectoryDiffusionModule_DGDiT(L.LightningModule):
+    def __init__(self, network, training_cfg, model_cfg) -> None:
+        super().__init__()
+
+        # Training params
+        self.batch_size = training_cfg.batch_size
+        self.lr = training_cfg.lr
+        self.mode = training_cfg.mode
+        self.traj_len = training_cfg.trajectory_len
+        self.epochs = training_cfg.epochs
+        self.train_sample_number = training_cfg.train_sample_number
+
+        # Diffuser training param
+        self.wta = training_cfg.wta
+        self.lr_warmup_steps = training_cfg.lr_warmup_steps
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.wta_trial_times = training_cfg.wta_trial_times
+
+        self.backbone = network
+        self.num_train_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_train_timesteps
+        )
+
+        self.cosine_distribution_cache = {"x": [], "y": [], "colors": []}
+
+    def forward(self, batch: tgd.Batch, mode):
+        x = (
+            batch.delta.squeeze(2)
+            .reshape(-1, 30, 40, 3 * self.traj_len)
+            .permute(0, 3, 1, 2)
+            .float()
+            .cuda()
+        )
+        pos = batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len).float().cuda()
+
+        model_kwargs = dict(pos=pos, context=batch)
+        loss_dict = self.diffusion.training_losses(
+            self.backbone, x, batch.timesteps, model_kwargs
+        )
+        loss = loss_dict["loss"].mean()
+
+        self.log_dict(
+            {
+                f"{mode}/loss": loss,
+                # The other metrics will be tested in validation
+                # f"{mode}/rmse": rmse,
+                # f"{mode}/cosine_similarity": cos_dist,
+                # f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return None, loss
+
+    # @torch.inference_mode()
+    def predict(self, batch: tgd.Batch, mode):
+        # torch.eval()
+        bs = batch.delta.shape[0] // self.sample_size
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)  # .float()
+
+        pos = batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len).float().cuda()
+        model_kwargs = dict(pos=pos, context=batch)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        # mask = batch.mask == 1
+        # mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+        if torch.sum(f_ix) == 0:  # No point
+            return f_pred, loss
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+
+        self.log_dict(
+            {
+                f"{mode}/flow_loss": loss,
+                f"{mode}/rmse": rmse,
+                f"{mode}/cosine_similarity": cos_dist,
+                f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return f_pred, loss
+
+    def predict_wta(self, orig_batch: tgd.Batch, mode):
+        bs = orig_batch.delta.shape[0] // self.sample_size
+        assert bs == 1, "Only support bsz = 1 for winner take all evaluation"
+        bs = self.wta_trial_times
+        data_list = orig_batch.to_data_list() * bs
+        batch = tgd.Batch.from_data_list(data_list)
+
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)  # .float()
+
+        pos = batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len).float().cuda()
+        model_kwargs = dict(pos=pos, context=batch)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        mask = batch.mask == 1
+        mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+        flow_loss = loss.reshape(bs, -1).mean(-1)
+        chosen_id = torch.min(flow_loss, 0)[1]  # index
+
+        if torch.sum(f_ix) == 0:  # No point
+            return (
+                f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+                loss[chosen_id],
+                [],
+            )
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(
+            f_pred[f_ix], f_target[f_ix], reduce=False
+        )
+
+        # Aggregate the results
+        # Choose the one with smallest flow loss
+        rmse = rmse.reshape(bs, -1).mean(-1)
+        cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+        mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+        pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+        neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+        multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+        self.log_dict(
+            {
+                f"{mode}_wta/flow_loss": flow_loss[chosen_id].item(),
+                f"{mode}_wta/rmse": rmse[chosen_id].item(),
+                f"{mode}_wta/cosine_similarity": cos_dist[chosen_id].item(),
+                f"{mode}_wta/mag_error": mag_error[chosen_id].item(),
+                f"{mode}_wta/multimodal": multimodal,
+                f"{mode}_wta/pos@0.7": pos_cosine.item(),
+                f"{mode}_wta/neg@0.7": neg_cosine.item(),
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return (
+            f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+            loss[chosen_id],
+            cos_dist.tolist(),
+        )
+
+    def configure_optimizers(self):
+        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-5)
+        lr_scheduler = get_cosine_schedule_with_warmup(
+            optimizer=optimizer,
+            num_warmup_steps=self.lr_warmup_steps,
+            # num_training_steps=(len(train_dataloader) * config.num_epochs),
+            num_training_steps=(
+                (self.train_sample_number // self.batch_size) * self.epochs
+            ),
+        )
+        return [optimizer], [lr_scheduler]
+
+    def training_step(self, batch: tgd.Batch, batch_id):  # type: ignore
+        self.train()
+        bs = batch.delta.shape[0] // self.sample_size
+
+        # batch.delta = normalize_trajectory(batch.delta)
+        batch.timesteps = torch.randint(
+            0,
+            self.num_train_timesteps,
+            (bs,),
+            device=self.device,
+        ).long()
+
+        _, loss = self(batch, "train")
+        return loss
+
+    def validation_step(self, batch: tgd.Batch, batch_id, dataloader_idx=0):  # type: ignore
+        self.eval()
+
+        # Clean cache for a new eval dataloader
+        if batch_id == 0:
+            self.cosine_distribution_cache["x"] = []
+            self.cosine_distribution_cache["y"] = []
+            self.cosine_distribution_cache["colors"] = []
+
+        dataloader_names = ["val", "train", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        with torch.no_grad():
+            f_pred, loss = self.predict(batch, name)
+            # print("predict:", f_pred.shape)
+            if self.wta:
+                f_pred, loss, cosines = self.predict_wta(batch, name)
+                self.cosine_distribution_cache["x"] += [batch_id] * len(cosines)
+                self.cosine_distribution_cache["y"] += cosines
+                self.cosine_distribution_cache["colors"] += [
+                    "blue" if batch_id % 2 == 0 else "red"
+                ] * len(cosines)
+        # breakpoint()
+        return {
+            "preds": f_pred,
+            "loss": loss,
+            "cosine_cache": self.cosine_distribution_cache,
+        }
+
+    @staticmethod
+    def make_plots(preds, batch: tgd.Batch, cosine_cache=None) -> Dict[str, go.Figure]:
+        # 1) Make the flow visualization plots
+        obj_id = batch.id
+        pos = (
+            batch.point[:, -2, :].numpy() if batch.point.shape[1] >= 2 else batch.pos
+        )  # The last step's beinning pos
+        mask = batch.mask.numpy()
+        f_target = batch.delta[:, -1, :]
+        f_pred = preds.reshape(preds.shape[0], -1, 3)[:, -1, :]
+
+        fig = make_subplots(
+            rows=2,
+            cols=2,
+            specs=[
+                [{"type": "scene", "colspan": 2}, None],
+                [{"type": "scene"}, {"type": "scene"}],
+            ],
+            subplot_titles=(
+                "input data",
+                "target flow",
+                "pred flow",
+            ),
+            vertical_spacing=0.05,
+        )
+
+        # Parent/child plot.
+        labelmap = {0: "unselected", 1: "part"}
+        labels = torch.zeros(len(pos)).int()
+        labels[mask == 1.0] = 1
+        fig.add_traces(v3p._segmentation_traces(pos, labels, labelmap, "scene1"))
+
+        fig.update_layout(
+            scene1=v3p._3d_scene(pos),
+            showlegend=True,
+            margin=dict(l=0, r=0, b=0, t=40),
+            legend=dict(x=1.0, y=0.75),
+        )
+
+        # normalize the flow for visualization.
+        n_f_gt = (f_target / f_target.norm(dim=1).max()).numpy()
+        n_f_pred = (f_pred / f_target.norm(dim=1).max()).numpy()
+
+        # GT flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene2", name="pts"), row=2, col=1)
+        f_gt_traces = v3p._flow_traces(
+            pos, n_f_gt, scene="scene2", name="f_gt", legendgroup="1"
+        )
+        fig.add_traces(f_gt_traces, rows=2, cols=1)
+        fig.update_layout(scene2=v3p._3d_scene(pos))
+
+        # Predicted flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene3", name="pts"), row=2, col=2)
+        f_pred_traces = v3p._flow_traces(
+            pos, n_f_pred, scene="scene3", name="f_pred", legendgroup="2"
+        )
+        fig.add_traces(f_pred_traces, rows=2, cols=2)
+        fig.update_layout(scene3=v3p._3d_scene(pos))
+
+        fig.update_layout(title=f"Object {obj_id}")
+
+        # 2) Make the cosine distribution plots
+        # table = wandb.Table(data=data, columns=["class_x", "class_y"])
+        # wandb.log({"my_custom_id": wandb.plot.scatter(table, "class_x", "class_y")})
+        cos_fig = None
+        if (
+            cosine_cache is not None and len(cosine_cache["x"]) != 0
+        ):  # Does wta, and needs plots
+            # The following Matplotlib code won't work because some matplotlib version issue (3.4.3 would work, but the version is old)
+            # cos_fig = plt.figure()
+            # ax = cos_fig.add_axes([0.1, 0.1, 0.8, 0.8])
+            # plt.ylim((-1, 1))
+            # ax.axhline(y=0)
+            # ax.axhline(y=0.7)
+            # ax.axhline(y=-0.7)
+            # plt.scatter(cosine_cache["x"], cosine_cache["y"], s=5, c=cosine_cache["colors"])
+            cos_fig = px.scatter(
+                x=cosine_cache["x"], y=cosine_cache["y"], color=cosine_cache["colors"]
+            )
+            cos_fig.update_layout(yaxis_range=[-1, 1])
+
+        return {"diffuser_plot": fig, "cosine_distribution_plot": cos_fig}
+
+
+class FlowTrajectoryDiffuserInferenceModule_DGDiT(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        # Inference params
+        self.batch_size = inference_cfg.batch_size
+        self.traj_len = inference_cfg.trajectory_len
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.backbone = network
+        self.num_inference_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_inference_timesteps
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        ckpt = torch.load(ckpt_file)
+        self.load_state_dict(ckpt["state_dict"])
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        print(
+            "Don't call this, it's not implemented. You should call predict_step or predict_wta"
+        )
+        return None
+
+    def predict(self, P_world) -> torch.Tensor:  # From pure point cloud
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
+
+    @torch.no_grad()
+    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:  # type: ignore
+        # torch.eval()
+        self.eval()
+        bs = batch.pos.shape[0] // self.sample_size
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)  # .float()
+
+        pos = batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len).float().cuda()
+        model_kwargs = dict(pos=pos, context=batch)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+        return f_pred
+
+    # For winner takes it all evaluation
+    @torch.inference_mode()
+    def predict_wta(self, dataloader, mode="delta", trial_times=50):
+        all_rmse = 0
+        all_cos_dist = 0
+        all_mag_error = 0
+        all_flow_loss = 0
+        all_multimodal = 0
+        all_pos_cosine = 0
+        all_neg_cosine = 0
+        valid_sample_cnt = 0
+
+        for id, orig_sample in tqdm.tqdm(enumerate(dataloader)):
+            bs = orig_sample.delta.shape[0] // self.sample_size
+            assert bs == 1, f"batch size should be 1, now is {bs}"
+
+            # batch every sample into bsz of trial_times
+            bs = trial_times
+            data_list = orig_sample.to_data_list() * bs
+            batch = tgd.Batch.from_data_list(data_list)
+
+            z = torch.randn(
+                bs, 3 * self.traj_len, 30, 40, device=self.device
+            )  # .float()
+
+            pos = (
+                batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len)
+                .float()
+                .to(self.device)
+            )
+            model_kwargs = dict(pos=pos, context=batch)
+
+            samples, results = self.diffusion.p_sample_loop(
+                self.backbone,
+                z.shape,
+                z,
+                clip_denoised=False,
+                model_kwargs=model_kwargs,
+                progress=True,
+                device=self.device,
+            )
+
+            f_pred = (
+                torch.flatten(samples, start_dim=2, end_dim=3)
+                .permute(0, 2, 1)
+                .reshape(-1, 3 * self.traj_len)
+                .unsqueeze(1)
+            )
+            f_pred = normalize_trajectory(f_pred)
+
+            # Compute the loss.
+            mask = batch.mask == 1
+            mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+            n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+            f_ix = batch.mask.bool().to(self.device)
+            if torch.sum(f_ix) == 0:
+                continue
+
+            valid_sample_cnt += 1
+            if mode == "delta":
+                f_target = batch.delta.to(self.device)
+            elif mode == "point":
+                f_target = batch.point.to(self.device)
+
+            f_target = f_target  # .float()
+            f_target = normalize_trajectory(f_target)
+
+            # print(f_pred[f_ix], batch.delta[f_ix])
+            # print(f_pred.device, f_target.device)
+            flow_loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+
+            # Compute some metrics on flow-only regions.
+            rmse, cos_dist, mag_error = flow_metrics(
+                f_pred[f_ix], f_target[f_ix], reduce=False
+            )
+
+            # Aggregate the results
+            # Choose the one with smallest flow loss
+            flow_loss = flow_loss.reshape(bs, -1).mean(-1)
+            rmse = rmse.reshape(bs, -1).mean(-1)
+            cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+            mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+            chosen_id = torch.min(flow_loss, 0)[1]  # index
+            pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+            neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+            multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+            print(
+                multimodal,
+                rmse[chosen_id],
+                cos_dist[chosen_id],
+                mag_error[chosen_id],
+                flow_loss[chosen_id],
+            )
+
+            all_multimodal += multimodal
+            all_pos_cosine += pos_cosine.item()
+            all_neg_cosine += neg_cosine.item()
+            all_rmse += rmse[chosen_id].item()
+            all_cos_dist += cos_dist[chosen_id].item()
+            all_mag_error += mag_error[chosen_id].item()
+            all_flow_loss += flow_loss[chosen_id].item()
+
+        metric_dict = {
+            f"flow_loss": all_flow_loss / valid_sample_cnt,  # / len(dataloader),
+            f"rmse": all_rmse / valid_sample_cnt,  # / len(dataloader),
+            f"cosine_similarity": all_cos_dist / valid_sample_cnt,  # / len(dataloader),
+            f"mag_error": all_mag_error / valid_sample_cnt,  # / len(dataloader),
+            f"multimodal": all_multimodal / valid_sample_cnt,  # / len(dataloader),
+            f"pos@0.7": all_pos_cosine / valid_sample_cnt,  # / len(dataloader),
+            f"neg@0.7": all_neg_cosine / valid_sample_cnt,  # / len(dataloader),
+        }
+
+        self.log_dict(
+            metric_dict,
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return metric_dict, cos_dist.tolist()  # dataloader * trial_times
+
+
+class FlowTrajectoryDiffuserSimulationModule_DGDiT(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        self.model = FlowTrajectoryDiffuserInferenceModule_DGDiT(
+            network, inference_cfg, model_cfg
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        self.model.load_from_ckpt(ckpt_file)
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
+
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.model.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
diff --git a/src/flowbothd/models/flow_diffuser_dit.py b/src/flowbothd/models/flow_diffuser_dit.py
new file mode 100644
index 0000000..5ce6a62
--- /dev/null
+++ b/src/flowbothd/models/flow_diffuser_dit.py
@@ -0,0 +1,599 @@
+from typing import Any, Dict
+
+import lightning as L
+import plotly.express as px
+import plotly.graph_objects as go
+import rpad.visualize_3d.plots as v3p
+import torch
+import torch_geometric.data as tgd
+import tqdm
+from diffusers.optimization import get_cosine_schedule_with_warmup
+from plotly.subplots import make_subplots
+from torch import optim
+
+# from flowbothd.models.modules.dit_models import DiT
+from flowbothd.metrics.trajectory import (
+    artflownet_loss,
+    flow_metrics,
+    normalize_trajectory,
+)
+from flowbothd.models.dit_utils import create_diffusion
+
+
+# Flow predictor with DiT
+class FlowTrajectoryDiffusionModule_DiT(L.LightningModule):
+    def __init__(self, network, training_cfg, model_cfg) -> None:
+        super().__init__()
+
+        # Training params
+        self.batch_size = training_cfg.batch_size
+        self.lr = training_cfg.lr
+        self.mode = training_cfg.mode
+        self.traj_len = training_cfg.trajectory_len
+        self.epochs = training_cfg.epochs
+        self.train_sample_number = training_cfg.train_sample_number
+
+        # Diffuser training param
+        self.wta = training_cfg.wta
+        self.lr_warmup_steps = training_cfg.lr_warmup_steps
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.wta_trial_times = training_cfg.wta_trial_times
+
+        self.backbone = network
+        self.num_train_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_train_timesteps
+        )
+
+        self.cosine_distribution_cache = {"x": [], "y": [], "colors": []}
+
+    def forward(self, batch: tgd.Batch, mode):
+        x = (
+            batch.delta.reshape(-1, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+        pos = (
+            batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+
+        model_kwargs = dict(pos=pos)
+        loss_dict = self.diffusion.training_losses(
+            self.backbone, x, batch.timesteps, model_kwargs
+        )
+        loss = loss_dict["loss"].mean()
+
+        self.log_dict(
+            {
+                f"{mode}/loss": loss,
+                # The other metrics will be tested in validation
+                # f"{mode}/rmse": rmse,
+                # f"{mode}/cosine_similarity": cos_dist,
+                # f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return None, loss
+
+    # @torch.inference_mode()
+    def predict(self, batch: tgd.Batch, mode):
+        # torch.eval()
+        bs = batch.delta.shape[0] // self.sample_size
+        z = torch.randn(
+            bs, 3 * self.traj_len, self.sample_size, device=self.device
+        )  # .float()
+
+        pos = (
+            batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+        model_kwargs = dict(pos=pos)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = samples.permute(0, 2, 1).reshape(-1, 3 * self.traj_len).unsqueeze(1)
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        mask = batch.mask == 1
+        mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+        if torch.sum(f_ix) == 0:  # No point
+            return f_pred, loss
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+        if torch.isnan(cos_dist):
+            breakpoint()
+
+        self.log_dict(
+            {
+                f"{mode}/flow_loss": loss,
+                f"{mode}/rmse": rmse,
+                f"{mode}/cosine_similarity": cos_dist,
+                f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return f_pred, loss
+
+    def predict_wta(self, orig_batch: tgd.Batch, mode):
+        bs = orig_batch.delta.shape[0] // self.sample_size
+        assert bs == 1, "Only support bsz = 1 for winner take all evaluation"
+        bs = self.wta_trial_times
+        data_list = orig_batch.to_data_list() * bs
+        batch = tgd.Batch.from_data_list(data_list)
+
+        z = torch.randn(
+            bs, 3 * self.traj_len, self.sample_size, device=self.device
+        )  # .float()
+
+        pos = (
+            batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+        model_kwargs = dict(pos=pos)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = samples.permute(0, 2, 1).reshape(-1, 3 * self.traj_len).unsqueeze(1)
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        # mask = batch.mask == 1
+        # mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+        flow_loss = loss.reshape(bs, -1).mean(-1)
+        chosen_id = torch.min(flow_loss, 0)[1]  # index
+
+        if torch.sum(f_ix) == 0:  # No point
+            return (
+                f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+                loss[chosen_id],
+                [],
+            )
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(
+            f_pred[f_ix], f_target[f_ix], reduce=False
+        )
+
+        # Aggregate the results
+        # Choose the one with smallest flow loss
+
+        rmse = rmse.reshape(bs, -1).mean(-1)
+        cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+        mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+        pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+        neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+        multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+        self.log_dict(
+            {
+                f"{mode}_wta/flow_loss": flow_loss[chosen_id].item(),
+                f"{mode}_wta/rmse": rmse[chosen_id].item(),
+                f"{mode}_wta/cosine_similarity": cos_dist[chosen_id].item(),
+                f"{mode}_wta/mag_error": mag_error[chosen_id].item(),
+                f"{mode}_wta/multimodal": multimodal,
+                f"{mode}_wta/pos@0.7": pos_cosine.item(),
+                f"{mode}_wta/neg@0.7": neg_cosine.item(),
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return (
+            f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+            loss[chosen_id],
+            cos_dist.tolist(),
+        )
+
+    def configure_optimizers(self):
+        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-5)
+        lr_scheduler = get_cosine_schedule_with_warmup(
+            optimizer=optimizer,
+            num_warmup_steps=self.lr_warmup_steps,
+            # num_training_steps=(len(train_dataloader) * config.num_epochs),
+            num_training_steps=(
+                (self.train_sample_number // self.batch_size) * self.epochs
+            ),
+        )
+        return [optimizer], [lr_scheduler]
+
+    def training_step(self, batch: tgd.Batch, batch_id):  # type: ignore
+        self.train()
+        bs = batch.delta.shape[0] // self.sample_size
+
+        batch.delta = normalize_trajectory(batch.delta)
+        batch.timesteps = torch.randint(
+            0,
+            self.num_train_timesteps,
+            (bs,),
+            device=self.device,
+        ).long()
+
+        _, loss = self(batch, "train")
+        return loss
+
+    def validation_step(self, batch: tgd.Batch, batch_id, dataloader_idx=0):  # type: ignore
+        self.eval()
+
+        # Clean cache for a new eval dataloader
+        if batch_id == 0:
+            self.cosine_distribution_cache["x"] = []
+            self.cosine_distribution_cache["y"] = []
+            self.cosine_distribution_cache["colors"] = []
+
+        dataloader_names = ["val", "train", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        with torch.no_grad():
+            f_pred, loss = self.predict(batch, name)
+            if self.wta:
+                f_pred, loss, cosines = self.predict_wta(batch, name)
+                self.cosine_distribution_cache["x"] += [batch_id] * len(cosines)
+                self.cosine_distribution_cache["y"] += cosines
+                self.cosine_distribution_cache["colors"] += [
+                    "blue" if batch_id % 2 == 0 else "red"
+                ] * len(cosines)
+        # breakpoint()
+        return {
+            "preds": f_pred,
+            "loss": loss,
+            "cosine_cache": self.cosine_distribution_cache,
+        }
+
+    @staticmethod
+    def make_plots(preds, batch: tgd.Batch, cosine_cache=None) -> Dict[str, go.Figure]:
+        # 1) Make the flow visualization plots
+        obj_id = batch.id
+        pos = (
+            batch.point[:, -2, :].numpy() if batch.point.shape[1] >= 2 else batch.pos
+        )  # The last step's beinning pos
+        mask = batch.mask.numpy()
+        f_target = batch.delta[:, -1, :]
+        f_pred = preds.reshape(preds.shape[0], -1, 3)[:, -1, :]
+
+        fig = make_subplots(
+            rows=2,
+            cols=2,
+            specs=[
+                [{"type": "scene", "colspan": 2}, None],
+                [{"type": "scene"}, {"type": "scene"}],
+            ],
+            subplot_titles=(
+                "input data",
+                "target flow",
+                "pred flow",
+            ),
+            vertical_spacing=0.05,
+        )
+
+        # Parent/child plot.
+        labelmap = {0: "unselected", 1: "part"}
+        labels = torch.zeros(len(pos)).int()
+        labels[mask == 1.0] = 1
+        fig.add_traces(v3p._segmentation_traces(pos, labels, labelmap, "scene1"))
+
+        fig.update_layout(
+            scene1=v3p._3d_scene(pos),
+            showlegend=True,
+            margin=dict(l=0, r=0, b=0, t=40),
+            legend=dict(x=1.0, y=0.75),
+        )
+
+        # normalize the flow for visualization.
+        n_f_gt = (f_target / f_target.norm(dim=1).max()).numpy()
+        n_f_pred = (f_pred / f_target.norm(dim=1).max()).numpy()
+
+        # GT flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene2", name="pts"), row=2, col=1)
+        f_gt_traces = v3p._flow_traces(
+            pos, n_f_gt, scene="scene2", name="f_gt", legendgroup="1"
+        )
+        fig.add_traces(f_gt_traces, rows=2, cols=1)
+        fig.update_layout(scene2=v3p._3d_scene(pos))
+
+        # Predicted flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene3", name="pts"), row=2, col=2)
+        f_pred_traces = v3p._flow_traces(
+            pos, n_f_pred, scene="scene3", name="f_pred", legendgroup="2"
+        )
+        fig.add_traces(f_pred_traces, rows=2, cols=2)
+        fig.update_layout(scene3=v3p._3d_scene(pos))
+
+        fig.update_layout(title=f"Object {obj_id}")
+
+        # 2) Make the cosine distribution plots
+        # table = wandb.Table(data=data, columns=["class_x", "class_y"])
+        # wandb.log({"my_custom_id": wandb.plot.scatter(table, "class_x", "class_y")})
+        cos_fig = None
+        if (
+            cosine_cache is not None and len(cosine_cache["x"]) != 0
+        ):  # Does wta, and needs plots
+            # The following Matplotlib code won't work because some matplotlib version issue (3.4.3 would work, but the version is old)
+            # cos_fig = plt.figure()
+            # ax = cos_fig.add_axes([0.1, 0.1, 0.8, 0.8])
+            # plt.ylim((-1, 1))
+            # ax.axhline(y=0)
+            # ax.axhline(y=0.7)
+            # ax.axhline(y=-0.7)
+            # plt.scatter(cosine_cache["x"], cosine_cache["y"], s=5, c=cosine_cache["colors"])
+            cos_fig = px.scatter(
+                x=cosine_cache["x"], y=cosine_cache["y"], color=cosine_cache["colors"]
+            )
+            cos_fig.update_layout(yaxis_range=[-1, 1])
+
+        return {"diffuser_plot": fig, "cosine_distribution_plot": cos_fig}
+
+
+class FlowTrajectoryDiffuserInferenceModule_DiT(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        # Inference params
+        self.batch_size = inference_cfg.batch_size
+        self.traj_len = inference_cfg.trajectory_len
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.backbone = network
+        self.num_inference_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_inference_timesteps
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        ckpt = torch.load(ckpt_file)
+        self.load_state_dict(ckpt["state_dict"])
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        print(
+            "Don't call this, it's not implemented. You should call predict_step or predict_wta"
+        )
+        return None
+
+    def predict(self, P_world) -> torch.Tensor:  # From pure point cloud
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
+
+    @torch.no_grad()
+    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:  # type: ignore
+        # torch.eval()
+        self.eval()
+        bs = batch.pos.shape[0] // self.sample_size
+        z = torch.randn(
+            bs, 3 * self.traj_len, self.sample_size, device=self.device
+        )  # .float()
+
+        pos = (
+            batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+        model_kwargs = dict(pos=pos)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = samples.permute(0, 2, 1).reshape(-1, 3 * self.traj_len).unsqueeze(1)
+        # print(f_pred.shape)
+        f_pred = normalize_trajectory(f_pred)
+        return f_pred
+
+    # For winner takes it all evaluation
+    @torch.inference_mode()
+    def predict_wta(self, dataloader, mode="delta", trial_times=50):
+        all_rmse = 0
+        all_cos_dist = 0
+        all_mag_error = 0
+        all_flow_loss = 0
+        all_multimodal = 0
+        all_pos_cosine = 0
+        all_neg_cosine = 0
+        valid_sample_cnt = 0
+
+        for id, orig_sample in tqdm.tqdm(enumerate(dataloader)):
+            bs = orig_sample.delta.shape[0] // self.sample_size
+            assert bs == 1, f"batch size should be 1, now is {bs}"
+
+            # batch every sample into bsz of trial_times
+            bs = trial_times
+            data_list = orig_sample.to_data_list() * bs
+            batch = tgd.Batch.from_data_list(data_list)
+
+            z = torch.randn(
+                bs, 3 * self.traj_len, self.sample_size, device=self.device
+            )  # .float()
+
+            pos = (
+                batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len)
+                .permute(0, 2, 1)
+                .float()
+                .to(self.device)
+            )
+            model_kwargs = dict(pos=pos)
+
+            samples, results = self.diffusion.p_sample_loop(
+                self.backbone,
+                z.shape,
+                z,
+                clip_denoised=False,
+                model_kwargs=model_kwargs,
+                progress=True,
+                device=self.device,
+            )
+
+            f_pred = (
+                samples.permute(0, 2, 1).reshape(-1, 3 * self.traj_len).unsqueeze(1)
+            )
+            f_pred = normalize_trajectory(f_pred)
+
+            # Compute the loss.
+            mask = batch.mask == 1
+            mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+            n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+            f_ix = batch.mask.bool().to(self.device)
+            if torch.sum(f_ix) == 0:
+                continue
+            valid_sample_cnt += 1
+            if mode == "delta":
+                f_target = batch.delta.to(self.device)
+            elif mode == "point":
+                f_target = batch.point.to(self.device)
+
+            f_target = f_target  # .float()
+            f_target = normalize_trajectory(f_target)
+
+            # print(f_pred[f_ix], batch.delta[f_ix])
+            flow_loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+
+            # Compute some metrics on flow-only regions.
+            rmse, cos_dist, mag_error = flow_metrics(
+                f_pred[f_ix], f_target[f_ix], reduce=False
+            )
+
+            # Aggregate the results
+            # Choose the one with smallest flow loss
+            flow_loss = flow_loss.reshape(bs, -1).mean(-1)
+            rmse = rmse.reshape(bs, -1).mean(-1)
+            cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+            mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+            chosen_id = torch.min(flow_loss, 0)[1]  # index
+            pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+            neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+            multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+            print(
+                multimodal,
+                rmse[chosen_id],
+                cos_dist[chosen_id],
+                mag_error[chosen_id],
+                flow_loss[chosen_id],
+            )
+
+            all_multimodal += multimodal
+            all_pos_cosine += pos_cosine.item()
+            all_neg_cosine += neg_cosine.item()
+            all_rmse += rmse[chosen_id].item()
+            all_cos_dist += cos_dist[chosen_id].item()
+            all_mag_error += mag_error[chosen_id].item()
+            all_flow_loss += flow_loss[chosen_id].item()
+
+        metric_dict = {
+            f"flow_loss": all_flow_loss / valid_sample_cnt,
+            f"rmse": all_rmse / valid_sample_cnt,
+            f"cosine_similarity": all_cos_dist / valid_sample_cnt,
+            f"mag_error": all_mag_error / valid_sample_cnt,
+            f"multimodal": all_multimodal / valid_sample_cnt,
+            f"pos@0.7": all_pos_cosine / valid_sample_cnt,
+            f"neg@0.7": all_neg_cosine / valid_sample_cnt,
+        }
+
+        self.log_dict(
+            metric_dict,
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return metric_dict, cos_dist.tolist()  # dataloader * trial_times
+
+
+class FlowTrajectoryDiffuserSimulationModule_DiT(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        self.model = FlowTrajectoryDiffuserInferenceModule_DiT(
+            network, inference_cfg, model_cfg
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        self.model.load_from_ckpt(ckpt_file)
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
+
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.model.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
diff --git a/src/flowbothd/models/flow_diffuser_hisdit.py b/src/flowbothd/models/flow_diffuser_hisdit.py
new file mode 100644
index 0000000..628cb4b
--- /dev/null
+++ b/src/flowbothd/models/flow_diffuser_hisdit.py
@@ -0,0 +1,664 @@
+from typing import Any, Dict
+
+import lightning as L
+import numpy as np
+import plotly.express as px
+import plotly.graph_objects as go
+import rpad.visualize_3d.plots as v3p
+import torch
+import torch_geometric.data as tgd
+import tqdm
+from diffusers.optimization import get_cosine_schedule_with_warmup
+from plotly.subplots import make_subplots
+from torch import optim
+
+# from flowbothd.models.modules.dit_models import DiT
+from flowbothd.metrics.trajectory import (
+    artflownet_loss,
+    flow_metrics,
+    normalize_trajectory,
+)
+from flowbothd.models.dit_utils import create_diffusion
+
+
+# Flow predictor with DiT
+class FlowTrajectoryDiffusionModule_HisDiT(L.LightningModule):
+    def __init__(self, networks, training_cfg, model_cfg) -> None:
+        super().__init__()
+        network = networks["DiT"]
+        history_encoder = networks["History"]
+
+        # Training params
+        self.batch_size = training_cfg.batch_size
+        self.lr = training_cfg.lr
+        self.mode = training_cfg.mode
+        self.traj_len = training_cfg.trajectory_len
+        self.epochs = training_cfg.epochs
+        self.train_sample_number = training_cfg.train_sample_number
+
+        # Diffuser training param
+        self.wta = training_cfg.wta
+        self.lr_warmup_steps = training_cfg.lr_warmup_steps
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.wta_trial_times = training_cfg.wta_trial_times
+
+        # self.history_encoder = HistoryEncoder(history_dim=model_cfg.history_embed_dim)
+        self.history_encoder = history_encoder
+        self.backbone = network
+        self.history_len = history_encoder.history_len
+        self.num_train_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_train_timesteps
+        )
+
+        self.cosine_distribution_cache = {"x": [], "y": [], "colors": []}
+
+    def forward(self, batch: tgd.Batch, mode):
+        x = (
+            batch.delta.reshape(-1, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )  # Ground truth, for loss calculation
+        history_embed = self.history_encoder(batch).permute(
+            0, 2, 1
+        )  # History embedding
+        pos = torch.concat(
+            [
+                batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+                .permute(0, 2, 1)
+                .float()
+                .cuda(),
+                history_embed,  # Concat history embedding
+            ],
+            dim=1,
+        )
+        model_kwargs = dict(pos=pos)
+        loss_dict = self.diffusion.training_losses(
+            self.backbone, x, batch.timesteps, model_kwargs
+        )
+        loss = loss_dict["loss"].mean()
+
+        self.log_dict(
+            {
+                f"{mode}/loss": loss,
+                # The other metrics will be tested in validation
+                # f"{mode}/rmse": rmse,
+                # f"{mode}/cosine_similarity": cos_dist,
+                # f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return None, loss
+
+    # @torch.inference_mode()
+    def predict(self, batch: tgd.Batch, mode):
+        # torch.eval()
+        bs = batch.delta.shape[0] // self.sample_size
+        z = torch.randn(
+            bs, 3 * self.traj_len, self.sample_size, device=self.device
+        )  # .float()
+
+        history_embed = self.history_encoder(batch).permute(0, 2, 1)
+        pos = torch.concat(
+            [
+                batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+                .permute(0, 2, 1)
+                .float()
+                .cuda(),
+                history_embed,  # Concat history embedding
+            ],
+            dim=1,
+        )
+        model_kwargs = dict(pos=pos)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = samples.permute(0, 2, 1).reshape(-1, 3 * self.traj_len).unsqueeze(1)
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        mask = batch.mask == 1
+        mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            assert True, "point supervision not implemented"
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+        if torch.sum(f_ix) == 0:  # No point
+            return f_pred, loss
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+        if torch.isnan(cos_dist):
+            breakpoint()
+
+        self.log_dict(
+            {
+                f"{mode}/flow_loss": loss,
+                f"{mode}/rmse": rmse,
+                f"{mode}/cosine_similarity": cos_dist,
+                f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return f_pred, loss
+
+    def predict_wta(self, orig_batch: tgd.Batch, mode):
+        bs = orig_batch.delta.shape[0] // self.sample_size
+        assert bs == 1, "Only support bsz = 1 for winner take all evaluation"
+        bs = self.wta_trial_times
+        data_list = orig_batch.to_data_list() * bs
+        batch = tgd.Batch.from_data_list(data_list)
+
+        z = torch.randn(
+            bs, 3 * self.traj_len, self.sample_size, device=self.device
+        )  # .float()
+
+        history_embed = self.history_encoder(batch).permute(0, 2, 1)
+        pos = torch.concat(
+            [
+                batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+                .permute(0, 2, 1)
+                .float()
+                .cuda(),
+                history_embed,  # Concat history embedding
+            ],
+            dim=1,
+        )
+        model_kwargs = dict(pos=pos)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = samples.permute(0, 2, 1).reshape(-1, 3 * self.traj_len).unsqueeze(1)
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        # mask = batch.mask == 1
+        # mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            assert True, "point supervision not implemented"
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+        flow_loss = loss.reshape(bs, -1).mean(-1)
+        chosen_id = torch.min(flow_loss, 0)[1]  # index
+
+        if torch.sum(f_ix) == 0:  # No point
+            return (
+                f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+                loss[chosen_id],
+                [],
+            )
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(
+            f_pred[f_ix], f_target[f_ix], reduce=False
+        )
+
+        # Aggregate the results
+        # Choose the one with smallest flow loss
+
+        rmse = rmse.reshape(bs, -1).mean(-1)
+        cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+        mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+        pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+        neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+        multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+        self.log_dict(
+            {
+                f"{mode}_wta/flow_loss": flow_loss[chosen_id].item(),
+                f"{mode}_wta/rmse": rmse[chosen_id].item(),
+                f"{mode}_wta/cosine_similarity": cos_dist[chosen_id].item(),
+                f"{mode}_wta/mag_error": mag_error[chosen_id].item(),
+                f"{mode}_wta/multimodal": multimodal,
+                f"{mode}_wta/pos@0.7": pos_cosine.item(),
+                f"{mode}_wta/neg@0.7": neg_cosine.item(),
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return (
+            f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+            loss[chosen_id],
+            cos_dist.tolist(),
+        )
+
+    def configure_optimizers(self):
+        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-5)
+        lr_scheduler = get_cosine_schedule_with_warmup(
+            optimizer=optimizer,
+            num_warmup_steps=self.lr_warmup_steps,
+            # num_training_steps=(len(train_dataloader) * config.num_epochs),
+            num_training_steps=(
+                (self.train_sample_number // self.batch_size) * self.epochs
+            ),
+        )
+        return [optimizer], [lr_scheduler]
+
+    def training_step(self, batch: tgd.Batch, batch_id):  # type: ignore
+        self.train()
+        bs = batch.delta.shape[0] // self.sample_size
+
+        batch.delta = normalize_trajectory(batch.delta)
+        batch.timesteps = torch.randint(
+            0,
+            self.num_train_timesteps,
+            (bs,),
+            device=self.device,
+        ).long()
+
+        _, loss = self(batch, "train")
+        return loss
+
+    def validation_step(self, batch: tgd.Batch, batch_id, dataloader_idx=0):  # type: ignore
+        self.eval()
+
+        # Clean cache for a new eval dataloader
+        if batch_id == 0:
+            self.cosine_distribution_cache["x"] = []
+            self.cosine_distribution_cache["y"] = []
+            self.cosine_distribution_cache["colors"] = []
+
+        # dataloader_names = ["val", "train", "unseen"]
+        dataloader_names = ["val", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        with torch.no_grad():
+            f_pred, loss = self.predict(batch, name)
+            if self.wta:
+                f_pred, loss, cosines = self.predict_wta(batch, name)
+                self.cosine_distribution_cache["x"] += [batch_id] * len(cosines)
+                self.cosine_distribution_cache["y"] += cosines
+                self.cosine_distribution_cache["colors"] += [
+                    "blue" if batch_id % 2 == 0 else "red"
+                ] * len(cosines)
+        # breakpoint()
+        return {
+            "preds": f_pred,
+            "loss": loss,
+            "cosine_cache": self.cosine_distribution_cache,
+        }
+
+    @staticmethod
+    def make_plots(preds, batch: tgd.Batch, cosine_cache=None) -> Dict[str, go.Figure]:
+        # 1) Make the flow visualization plots
+        obj_id = batch.id
+        pos = batch.pos  # The last step's beinning pos
+        mask = batch.mask.numpy()
+        f_target = batch.delta[:, -1, :]
+        f_pred = preds.reshape(preds.shape[0], -1, 3)[:, -1, :]
+
+        fig = make_subplots(
+            rows=2,
+            cols=2,
+            specs=[
+                [{"type": "scene", "colspan": 2}, None],
+                [{"type": "scene"}, {"type": "scene"}],
+            ],
+            subplot_titles=(
+                "input data",
+                "target flow",
+                "pred flow",
+            ),
+            vertical_spacing=0.05,
+        )
+
+        # Parent/child plot.
+        labelmap = {0: "unselected", 1: "part"}
+        labels = torch.zeros(len(pos)).int()
+        labels[mask == 1.0] = 1
+        fig.add_traces(v3p._segmentation_traces(pos, labels, labelmap, "scene1"))
+
+        fig.update_layout(
+            scene1=v3p._3d_scene(pos),
+            showlegend=True,
+            margin=dict(l=0, r=0, b=0, t=40),
+            legend=dict(x=1.0, y=0.75),
+        )
+
+        # normalize the flow for visualization.
+        n_f_gt = (f_target / f_target.norm(dim=1).max()).numpy()
+        n_f_pred = (f_pred / f_target.norm(dim=1).max()).numpy()
+
+        # GT flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene2", name="pts"), row=2, col=1)
+        f_gt_traces = v3p._flow_traces(
+            pos, n_f_gt, scene="scene2", name="f_gt", legendgroup="1"
+        )
+        fig.add_traces(f_gt_traces, rows=2, cols=1)
+        fig.update_layout(scene2=v3p._3d_scene(pos))
+
+        # Predicted flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene3", name="pts"), row=2, col=2)
+        f_pred_traces = v3p._flow_traces(
+            pos, n_f_pred, scene="scene3", name="f_pred", legendgroup="2"
+        )
+        fig.add_traces(f_pred_traces, rows=2, cols=2)
+        fig.update_layout(scene3=v3p._3d_scene(pos))
+
+        fig.update_layout(title=f"Object {obj_id}")
+
+        # 2) Make the cosine distribution plots
+        # table = wandb.Table(data=data, columns=["class_x", "class_y"])
+        # wandb.log({"my_custom_id": wandb.plot.scatter(table, "class_x", "class_y")})
+        cos_fig = None
+        if (
+            cosine_cache is not None and len(cosine_cache["x"]) != 0
+        ):  # Does wta, and needs plots
+            # The following Matplotlib code won't work because some matplotlib version issue (3.4.3 would work, but the version is old)
+            # cos_fig = plt.figure()
+            # ax = cos_fig.add_axes([0.1, 0.1, 0.8, 0.8])
+            # plt.ylim((-1, 1))
+            # ax.axhline(y=0)
+            # ax.axhline(y=0.7)
+            # ax.axhline(y=-0.7)
+            # plt.scatter(cosine_cache["x"], cosine_cache["y"], s=5, c=cosine_cache["colors"])
+            cos_fig = px.scatter(
+                x=cosine_cache["x"], y=cosine_cache["y"], color=cosine_cache["colors"]
+            )
+            cos_fig.update_layout(yaxis_range=[-1, 1])
+
+        return {"diffuser_plot": fig, "cosine_distribution_plot": cos_fig}
+
+
+class FlowTrajectoryDiffuserInferenceModule_HisDiT(L.LightningModule):
+    def __init__(self, networks, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        network = networks["DiT"]
+        history_encoder = networks["History"]
+
+        # Inference params
+        self.batch_size = inference_cfg.batch_size
+        self.traj_len = inference_cfg.trajectory_len
+
+        # Diffuser params
+        self.sample_size = 1200
+
+        self.history_encoder = history_encoder
+        self.history_len = self.history_encoder.history_len
+        self.backbone = network
+        self.num_inference_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_inference_timesteps
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        ckpt = torch.load(ckpt_file)
+        self.load_state_dict(ckpt["state_dict"])
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        print(
+            "Don't call this, it's not implemented. You should call predict_step or predict_wta"
+        )
+        return None
+
+    def predict(
+        self, P_world, history_pcd=None, history_flow=None
+    ) -> torch.Tensor:  # From pure point cloud
+        K = self.history_len
+        if history_pcd is None:
+            history_pcd = np.zeros_like(P_world)
+            history_flow = np.zeros_like(P_world)
+            K = 0
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            history=torch.from_numpy(history_pcd).float().cuda(),
+            flow_history=torch.from_numpy(history_flow).float().cuda(),
+            K=K,
+            lengths=self.sample_size
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
+
+    @torch.no_grad()
+    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:  # type: ignore
+        # torch.eval()
+        self.eval()
+        bs = batch.pos.shape[0] // self.sample_size
+        z = torch.randn(
+            bs, 3 * self.traj_len, self.sample_size, device=self.device
+        )  # .float()
+
+        history_embed = self.history_encoder(batch).permute(
+            0, 2, 1
+        )  # History embedding
+        pos = torch.concat(
+            [
+                batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len)
+                .permute(0, 2, 1)
+                .float()
+                .cuda(),
+                history_embed,  # Concat history embedding
+            ],
+            dim=1,
+        )
+        model_kwargs = dict(pos=pos)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = samples.permute(0, 2, 1).reshape(-1, 3 * self.traj_len).unsqueeze(1)
+        f_pred = normalize_trajectory(f_pred)
+        return f_pred
+
+    # For winner takes it all evaluation
+    @torch.inference_mode()
+    def predict_wta(self, dataloader, mode="delta", trial_times=50):
+        all_rmse = 0
+        all_cos_dist = 0
+        all_mag_error = 0
+        all_flow_loss = 0
+        all_multimodal = 0
+        all_pos_cosine = 0
+        all_neg_cosine = 0
+        valid_sample_cnt = 0
+
+        for id, orig_sample in tqdm.tqdm(enumerate(dataloader)):
+            bs = orig_sample.delta.shape[0] // self.sample_size
+            assert bs == 1, f"batch size should be 1, now is {bs}"
+
+            # batch every sample into bsz of trial_times
+            bs = trial_times
+            data_list = orig_sample.to_data_list() * bs
+            batch = tgd.Batch.from_data_list(data_list)
+
+            z = torch.randn(
+                bs, 3 * self.traj_len, self.sample_size, device=self.device
+            )  # .float()
+
+            history_embed = self.history_encoder(batch).permute(
+                0, 2, 1
+            )  # History embedding
+            pos = torch.concat(
+                [
+                    batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len)
+                    .permute(0, 2, 1)
+                    .float()
+                    .cuda(),
+                    history_embed,  # Concat history embedding
+                ],
+                dim=1,
+            )
+            model_kwargs = dict(pos=pos)
+
+            samples, results = self.diffusion.p_sample_loop(
+                self.backbone,
+                z.shape,
+                z,
+                clip_denoised=False,
+                model_kwargs=model_kwargs,
+                progress=True,
+                device=self.device,
+            )
+
+            f_pred = (
+                samples.permute(0, 2, 1).reshape(-1, 3 * self.traj_len).unsqueeze(1)
+            )
+            f_pred = normalize_trajectory(f_pred)
+
+            # Compute the loss.
+            mask = batch.mask == 1
+            mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+            n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+            f_ix = batch.mask.bool().to(self.device)
+            if torch.sum(f_ix) == 0:
+                continue
+            valid_sample_cnt += 1
+            if mode == "delta":
+                f_target = batch.delta.to(self.device)
+            elif mode == "point":
+                assert True, "point supervision not implemented"
+                f_target = batch.point.to(self.device)
+
+            f_target = f_target  # .float()
+            f_target = normalize_trajectory(f_target)
+
+            # print(f_pred[f_ix], batch.delta[f_ix])
+            flow_loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+
+            # Compute some metrics on flow-only regions.
+            rmse, cos_dist, mag_error = flow_metrics(
+                f_pred[f_ix], f_target[f_ix], reduce=False
+            )
+
+            # Aggregate the results
+            # Choose the one with smallest flow loss
+            flow_loss = flow_loss.reshape(bs, -1).mean(-1)
+            rmse = rmse.reshape(bs, -1).mean(-1)
+            cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+            mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+            chosen_id = torch.min(flow_loss, 0)[1]  # index
+            pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+            neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+            multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+            print(
+                multimodal,
+                rmse[chosen_id],
+                cos_dist[chosen_id],
+                mag_error[chosen_id],
+                flow_loss[chosen_id],
+            )
+
+            all_multimodal += multimodal
+            all_pos_cosine += pos_cosine.item()
+            all_neg_cosine += neg_cosine.item()
+            all_rmse += rmse[chosen_id].item()
+            all_cos_dist += cos_dist[chosen_id].item()
+            all_mag_error += mag_error[chosen_id].item()
+            all_flow_loss += flow_loss[chosen_id].item()
+
+        metric_dict = {
+            f"flow_loss": all_flow_loss / valid_sample_cnt,
+            f"rmse": all_rmse / valid_sample_cnt,
+            f"cosine_similarity": all_cos_dist / valid_sample_cnt,
+            f"mag_error": all_mag_error / valid_sample_cnt,
+            f"multimodal": all_multimodal / valid_sample_cnt,
+            f"pos@0.7": all_pos_cosine / valid_sample_cnt,
+            f"neg@0.7": all_neg_cosine / valid_sample_cnt,
+        }
+
+        self.log_dict(
+            metric_dict,
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return metric_dict, cos_dist.tolist()  # dataloader * trial_times
+
+
+class FlowTrajectoryDiffuserSimulationModule_HisDiT(L.LightningModule):
+    def __init__(self, networks, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        self.model = FlowTrajectoryDiffuserInferenceModule_HisDiT(
+            networks, inference_cfg, model_cfg
+        )
+        self.history_len = self.model.history_len
+        self.sample_size = self.model.sample_size
+
+    def load_from_ckpt(self, ckpt_file):
+        self.model.load_from_ckpt(ckpt_file)
+
+    def forward(self, data, history_pcd=None, history_flow=None) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
+        K = self.history_len
+        if history_pcd is None:
+            history_pcd = np.zeros_like(P_world)
+            history_flow = np.zeros_like(P_world)
+            K = 0
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            history=torch.from_numpy(history_pcd).float().cuda(),
+            flow_history=torch.from_numpy(history_flow).float().cuda(),
+            K=K,
+            lengths=self.sample_size
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        # breakpoint()
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.model.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
diff --git a/src/flowbothd/models/flow_diffuser_hispndit.py b/src/flowbothd/models/flow_diffuser_hispndit.py
new file mode 100644
index 0000000..4ecbdda
--- /dev/null
+++ b/src/flowbothd/models/flow_diffuser_hispndit.py
@@ -0,0 +1,671 @@
+from typing import Any, Dict
+
+import lightning as L
+import numpy as np
+import plotly.express as px
+import plotly.graph_objects as go
+import rpad.visualize_3d.plots as v3p
+import torch
+import torch_geometric.data as tgd
+import tqdm
+from diffusers.optimization import get_cosine_schedule_with_warmup
+from plotly.subplots import make_subplots
+from torch import optim
+
+# from flowbothd.models.modules.dit_models import DiT
+from flowbothd.metrics.trajectory import (
+    artflownet_loss,
+    flow_metrics,
+    normalize_trajectory,
+)
+from flowbothd.models.dit_utils import create_diffusion
+
+
+# Flow predictor with DiT
+class FlowTrajectoryDiffusionModule_HisPNDiT(L.LightningModule):
+    def __init__(self, networks, training_cfg, model_cfg) -> None:
+        super().__init__()
+        network = networks["DiT"]
+        history_encoder = networks["History"]
+
+        # Training params
+        self.batch_size = training_cfg.batch_size
+        self.lr = training_cfg.lr
+        self.mode = training_cfg.mode
+        self.traj_len = training_cfg.trajectory_len
+        self.epochs = training_cfg.epochs
+        self.train_sample_number = training_cfg.train_sample_number
+
+        # Diffuser training param
+        self.wta = training_cfg.wta
+        self.lr_warmup_steps = training_cfg.lr_warmup_steps
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.wta_trial_times = training_cfg.wta_trial_times
+
+        # self.history_encoder = HistoryEncoder(history_dim=model_cfg.history_embed_dim)
+        self.history_encoder = history_encoder
+        self.backbone = network
+        self.history_len = history_encoder.history_len
+        self.num_train_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_train_timesteps
+        )
+
+        self.cosine_distribution_cache = {"x": [], "y": [], "colors": []}
+
+    def forward(self, batch: tgd.Batch, mode):
+        x = (
+            batch.delta.squeeze(2)
+            .reshape(-1, 30, 40, 3 * self.traj_len)
+            .permute(0, 3, 1, 2)
+            .float()
+            .cuda()
+        )  # Ground truth, for loss calculation
+        history_embed = (
+            self.history_encoder(batch).permute(0, 2, 1).squeeze(-1)
+        )  # History embedding
+        batch.history_embed = history_embed
+        pos = (
+            batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+        model_kwargs = dict(pos=pos, context=batch.cuda())
+        loss_dict = self.diffusion.training_losses(
+            self.backbone, x, batch.timesteps, model_kwargs
+        )
+        loss = loss_dict["loss"].mean()
+
+        self.log_dict(
+            {
+                f"{mode}/loss": loss,
+                # The other metrics will be tested in validation
+                # f"{mode}/rmse": rmse,
+                # f"{mode}/cosine_similarity": cos_dist,
+                # f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return None, loss
+
+    # @torch.inference_mode()
+    def predict(self, batch: tgd.Batch, mode):
+        # torch.eval()
+        bs = batch.delta.shape[0] // self.sample_size
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)
+
+        history_embed = self.history_encoder(batch).permute(0, 2, 1).squeeze(-1)
+        batch.history_embed = history_embed
+        pos = (
+            batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+        model_kwargs = dict(pos=pos, context=batch.cuda())
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        mask = batch.mask == 1
+        mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            assert True, "point supervision not implemented"
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+        if torch.sum(f_ix) == 0:  # No point
+            return f_pred, loss
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+        if torch.isnan(cos_dist):
+            breakpoint()
+
+        self.log_dict(
+            {
+                f"{mode}/flow_loss": loss,
+                f"{mode}/rmse": rmse,
+                f"{mode}/cosine_similarity": cos_dist,
+                f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return f_pred, loss
+
+    def predict_wta(self, orig_batch: tgd.Batch, mode):
+        bs = orig_batch.delta.shape[0] // self.sample_size
+        assert bs == 1, "Only support bsz = 1 for winner take all evaluation"
+        bs = self.wta_trial_times
+        data_list = orig_batch.to_data_list() * bs
+        batch = tgd.Batch.from_data_list(data_list)
+
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)  # .float()
+
+        history_embed = self.history_encoder(batch).permute(0, 2, 1).squeeze(-1)
+        batch.history_embed = history_embed
+        pos = (
+            batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+        model_kwargs = dict(pos=pos, context=batch.cuda())
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        # mask = batch.mask == 1
+        # mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            assert True, "point supervision not implemented"
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+        flow_loss = loss.reshape(bs, -1).mean(-1)
+        chosen_id = torch.min(flow_loss, 0)[1]  # index
+
+        if torch.sum(f_ix) == 0:  # No point
+            return (
+                f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+                loss[chosen_id],
+                [],
+            )
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(
+            f_pred[f_ix], f_target[f_ix], reduce=False
+        )
+
+        # Aggregate the results
+        # Choose the one with smallest flow loss
+
+        rmse = rmse.reshape(bs, -1).mean(-1)
+        cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+        mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+        pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+        neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+        multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+        self.log_dict(
+            {
+                f"{mode}_wta/flow_loss": flow_loss[chosen_id].item(),
+                f"{mode}_wta/rmse": rmse[chosen_id].item(),
+                f"{mode}_wta/cosine_similarity": cos_dist[chosen_id].item(),
+                f"{mode}_wta/mag_error": mag_error[chosen_id].item(),
+                f"{mode}_wta/multimodal": multimodal,
+                f"{mode}_wta/pos@0.7": pos_cosine.item(),
+                f"{mode}_wta/neg@0.7": neg_cosine.item(),
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return (
+            f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+            loss[chosen_id],
+            cos_dist.tolist(),
+        )
+
+    def configure_optimizers(self):
+        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-5)
+        lr_scheduler = get_cosine_schedule_with_warmup(
+            optimizer=optimizer,
+            num_warmup_steps=self.lr_warmup_steps,
+            # num_training_steps=(len(train_dataloader) * config.num_epochs),
+            num_training_steps=(
+                (self.train_sample_number // self.batch_size) * self.epochs
+            ),
+        )
+        return [optimizer], [lr_scheduler]
+
+    def training_step(self, batch: tgd.Batch, batch_id):  # type: ignore
+        self.train()
+        bs = batch.delta.shape[0] // self.sample_size
+
+        batch.delta = normalize_trajectory(batch.delta)
+        batch.timesteps = torch.randint(
+            0,
+            self.num_train_timesteps,
+            (bs,),
+            device=self.device,
+        ).long()
+
+        _, loss = self(batch, "train")
+        return loss
+
+    def validation_step(self, batch: tgd.Batch, batch_id, dataloader_idx=0):  # type: ignore
+        self.eval()
+
+        # Clean cache for a new eval dataloader
+        if batch_id == 0:
+            self.cosine_distribution_cache["x"] = []
+            self.cosine_distribution_cache["y"] = []
+            self.cosine_distribution_cache["colors"] = []
+
+        # dataloader_names = ["val", "train", "unseen"]
+        dataloader_names = ["val", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        with torch.no_grad():
+            f_pred, loss = self.predict(batch, name)
+            if self.wta:
+                f_pred, loss, cosines = self.predict_wta(batch, name)
+                self.cosine_distribution_cache["x"] += [batch_id] * len(cosines)
+                self.cosine_distribution_cache["y"] += cosines
+                self.cosine_distribution_cache["colors"] += [
+                    "blue" if batch_id % 2 == 0 else "red"
+                ] * len(cosines)
+        # breakpoint()
+        return {
+            "preds": f_pred,
+            "loss": loss,
+            "cosine_cache": self.cosine_distribution_cache,
+        }
+
+    @staticmethod
+    def make_plots(preds, batch: tgd.Batch, cosine_cache=None) -> Dict[str, go.Figure]:
+        # 1) Make the flow visualization plots
+        obj_id = batch.id
+        pos = batch.pos  # The last step's beinning pos
+        mask = batch.mask.numpy()
+        f_target = batch.delta[:, -1, :]
+        f_pred = preds.reshape(preds.shape[0], -1, 3)[:, -1, :]
+
+        fig = make_subplots(
+            rows=2,
+            cols=2,
+            specs=[
+                [{"type": "scene", "colspan": 2}, None],
+                [{"type": "scene"}, {"type": "scene"}],
+            ],
+            subplot_titles=(
+                "input data",
+                "target flow",
+                "pred flow",
+            ),
+            vertical_spacing=0.05,
+        )
+
+        # Parent/child plot.
+        labelmap = {0: "unselected", 1: "part"}
+        labels = torch.zeros(len(pos)).int()
+        labels[mask == 1.0] = 1
+        fig.add_traces(v3p._segmentation_traces(pos, labels, labelmap, "scene1"))
+
+        fig.update_layout(
+            scene1=v3p._3d_scene(pos),
+            showlegend=True,
+            margin=dict(l=0, r=0, b=0, t=40),
+            legend=dict(x=1.0, y=0.75),
+        )
+
+        # normalize the flow for visualization.
+        n_f_gt = (f_target / f_target.norm(dim=1).max()).numpy()
+        n_f_pred = (f_pred / f_target.norm(dim=1).max()).numpy()
+
+        # GT flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene2", name="pts"), row=2, col=1)
+        f_gt_traces = v3p._flow_traces(
+            pos, n_f_gt, scene="scene2", name="f_gt", legendgroup="1"
+        )
+        fig.add_traces(f_gt_traces, rows=2, cols=1)
+        fig.update_layout(scene2=v3p._3d_scene(pos))
+
+        # Predicted flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene3", name="pts"), row=2, col=2)
+        f_pred_traces = v3p._flow_traces(
+            pos, n_f_pred, scene="scene3", name="f_pred", legendgroup="2"
+        )
+        fig.add_traces(f_pred_traces, rows=2, cols=2)
+        fig.update_layout(scene3=v3p._3d_scene(pos))
+
+        fig.update_layout(title=f"Object {obj_id}")
+
+        # 2) Make the cosine distribution plots
+        # table = wandb.Table(data=data, columns=["class_x", "class_y"])
+        # wandb.log({"my_custom_id": wandb.plot.scatter(table, "class_x", "class_y")})
+        cos_fig = None
+        if (
+            cosine_cache is not None and len(cosine_cache["x"]) != 0
+        ):  # Does wta, and needs plots
+            # The following Matplotlib code won't work because some matplotlib version issue (3.4.3 would work, but the version is old)
+            # cos_fig = plt.figure()
+            # ax = cos_fig.add_axes([0.1, 0.1, 0.8, 0.8])
+            # plt.ylim((-1, 1))
+            # ax.axhline(y=0)
+            # ax.axhline(y=0.7)
+            # ax.axhline(y=-0.7)
+            # plt.scatter(cosine_cache["x"], cosine_cache["y"], s=5, c=cosine_cache["colors"])
+            cos_fig = px.scatter(
+                x=cosine_cache["x"], y=cosine_cache["y"], color=cosine_cache["colors"]
+            )
+            cos_fig.update_layout(yaxis_range=[-1, 1])
+
+        return {"diffuser_plot": fig, "cosine_distribution_plot": cos_fig}
+
+
+class FlowTrajectoryDiffuserInferenceModule_HisPNDiT(L.LightningModule):
+    def __init__(self, networks, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        network = networks["DiT"]
+        history_encoder = networks["History"]
+
+        # Inference params
+        self.batch_size = inference_cfg.batch_size
+        self.traj_len = inference_cfg.trajectory_len
+
+        # Diffuser params
+        self.sample_size = 1200
+
+        self.history_encoder = history_encoder
+        self.history_len = self.history_encoder.history_len
+        self.backbone = network
+        self.num_inference_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_inference_timesteps
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        ckpt = torch.load(ckpt_file)
+        self.load_state_dict(ckpt["state_dict"])
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        print(
+            "Don't call this, it's not implemented. You should call predict_step or predict_wta"
+        )
+        return None
+
+    def predict(
+        self, P_world, history_pcd=None, history_flow=None
+    ) -> torch.Tensor:  # From pure point cloud
+        K = self.history_len
+        if history_pcd is None:
+            history_pcd = np.zeros_like(P_world)
+            history_flow = np.zeros_like(P_world)
+            K = 0
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            history=torch.from_numpy(history_pcd).float().cuda(),
+            flow_history=torch.from_numpy(history_flow).float().cuda(),
+            K=K,
+            lengths=self.sample_size
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
+
+    @torch.no_grad()
+    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0, return_intermediate: bool = False) -> torch.Tensor:  # type: ignore
+        # torch.eval()
+        self.eval()
+        bs = batch.pos.shape[0] // self.sample_size
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)  # .float()
+
+        history_embed = (
+            self.history_encoder(batch).permute(0, 2, 1).squeeze(-1)
+        )  # History embedding
+        batch.history_embed = history_embed
+        pos = (
+            batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+            .permute(0, 2, 1)
+            .float()
+            .cuda()
+        )
+        model_kwargs = dict(pos=pos, context=batch.cuda())
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+        if return_intermediate:
+            return f_pred, results
+        return f_pred
+
+    # For winner takes it all evaluation
+    @torch.inference_mode()
+    def predict_wta(self, dataloader, mode="delta", trial_times=50):
+        all_rmse = 0
+        all_cos_dist = 0
+        all_mag_error = 0
+        all_flow_loss = 0
+        all_multimodal = 0
+        all_pos_cosine = 0
+        all_neg_cosine = 0
+        valid_sample_cnt = 0
+
+        for id, orig_sample in tqdm.tqdm(enumerate(dataloader)):
+            bs = orig_sample.delta.shape[0] // self.sample_size
+            assert bs == 1, f"batch size should be 1, now is {bs}"
+
+            # batch every sample into bsz of trial_times
+            bs = trial_times
+            data_list = orig_sample.to_data_list() * bs
+            batch = tgd.Batch.from_data_list(data_list)
+
+            z = torch.randn(
+                bs, 3 * self.traj_len, 30, 40, device=self.device
+            )  # .float()
+
+            history_embed = (
+                self.history_encoder(batch).permute(0, 2, 1).squeeze(-1)
+            )  # History embedding
+            batch.history_embed = history_embed
+            pos = (
+                batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len)
+                .permute(0, 2, 1)
+                .float()
+                .cuda()
+            )
+            model_kwargs = dict(pos=pos, context=batch.cuda())
+
+            samples, results = self.diffusion.p_sample_loop(
+                self.backbone,
+                z.shape,
+                z,
+                clip_denoised=False,
+                model_kwargs=model_kwargs,
+                progress=True,
+                device=self.device,
+            )
+
+            f_pred = (
+                torch.flatten(samples, start_dim=2, end_dim=3)
+                .permute(0, 2, 1)
+                .reshape(-1, 3 * self.traj_len)
+                .unsqueeze(1)
+            )
+            f_pred = normalize_trajectory(f_pred)
+
+            # Compute the loss.
+            mask = batch.mask == 1
+            mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+            n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+            f_ix = batch.mask.bool().to(self.device)
+            if torch.sum(f_ix) == 0:
+                continue
+            valid_sample_cnt += 1
+            if mode == "delta":
+                f_target = batch.delta.to(self.device)
+            elif mode == "point":
+                assert True, "point supervision not implemented"
+                f_target = batch.point.to(self.device)
+
+            f_target = f_target  # .float()
+            f_target = normalize_trajectory(f_target)
+
+            # print(f_pred[f_ix], batch.delta[f_ix])
+            flow_loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+
+            # Compute some metrics on flow-only regions.
+            rmse, cos_dist, mag_error = flow_metrics(
+                f_pred[f_ix], f_target[f_ix], reduce=False
+            )
+
+            # Aggregate the results
+            # Choose the one with smallest flow loss
+            flow_loss = flow_loss.reshape(bs, -1).mean(-1)
+            rmse = rmse.reshape(bs, -1).mean(-1)
+            cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+            mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+            chosen_id = torch.min(flow_loss, 0)[1]  # index
+            pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+            neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+            multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+            print(
+                multimodal,
+                rmse[chosen_id],
+                cos_dist[chosen_id],
+                mag_error[chosen_id],
+                flow_loss[chosen_id],
+            )
+
+            all_multimodal += multimodal
+            all_pos_cosine += pos_cosine.item()
+            all_neg_cosine += neg_cosine.item()
+            all_rmse += rmse[chosen_id].item()
+            all_cos_dist += cos_dist[chosen_id].item()
+            all_mag_error += mag_error[chosen_id].item()
+            all_flow_loss += flow_loss[chosen_id].item()
+
+        metric_dict = {
+            f"flow_loss": all_flow_loss / valid_sample_cnt,
+            f"rmse": all_rmse / valid_sample_cnt,
+            f"cosine_similarity": all_cos_dist / valid_sample_cnt,
+            f"mag_error": all_mag_error / valid_sample_cnt,
+            f"multimodal": all_multimodal / valid_sample_cnt,
+            f"pos@0.7": all_pos_cosine / valid_sample_cnt,
+            f"neg@0.7": all_neg_cosine / valid_sample_cnt,
+        }
+
+        self.log_dict(
+            metric_dict,
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return metric_dict, cos_dist.tolist()  # dataloader * trial_times
+
+
+class FlowTrajectoryDiffuserSimulationModule_HisPNDiT(L.LightningModule):
+    def __init__(self, networks, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        self.model = FlowTrajectoryDiffuserInferenceModule_HisPNDiT(
+            networks, inference_cfg, model_cfg
+        )
+        self.history_len = self.model.history_len
+        self.sample_size = self.model.sample_size
+
+    def load_from_ckpt(self, ckpt_file):
+        self.model.load_from_ckpt(ckpt_file)
+
+    def forward(self, data, history_pcd=None, history_flow=None, return_intermediate=False) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
+        K = self.history_len
+        if history_pcd is None:
+            history_pcd = np.zeros_like(P_world)
+            history_flow = np.zeros_like(P_world)
+            K = 0
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            history=torch.from_numpy(history_pcd).float().cuda(),
+            flow_history=torch.from_numpy(history_flow).float().cuda(),
+            K=K,
+            lengths=self.sample_size
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        # breakpoint()
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            if return_intermediate:
+                trajectory, intermediates = self.model.predict_step(
+                    batch, 0, return_intermediate=True
+                )
+                return trajectory.cpu(), intermediates
+            else:
+                trajectory = self.model.predict_step(
+                    batch, 0, return_intermediate=False
+                )
+                return trajectory.cpu()
diff --git a/src/flowbothd/models/flow_diffuser_pndit.py b/src/flowbothd/models/flow_diffuser_pndit.py
new file mode 100644
index 0000000..86a22cc
--- /dev/null
+++ b/src/flowbothd/models/flow_diffuser_pndit.py
@@ -0,0 +1,590 @@
+from typing import Any, Dict
+
+import lightning as L
+import plotly.express as px
+import plotly.graph_objects as go
+import rpad.visualize_3d.plots as v3p
+import torch
+import torch_geometric.data as tgd
+import tqdm
+from diffusers.optimization import get_cosine_schedule_with_warmup
+from plotly.subplots import make_subplots
+from torch import optim
+
+# from flowbothd.models.modules.dit_models import DiT
+from flowbothd.metrics.trajectory import (
+    artflownet_loss,
+    flow_metrics,
+    normalize_trajectory,
+)
+from flowbothd.models.dit_utils import create_diffusion
+
+
+# Flow predictor with PN++ + DiT
+class FlowTrajectoryDiffusionModule_PNDiT(L.LightningModule):
+    def __init__(self, network, training_cfg, model_cfg) -> None:
+        super().__init__()
+
+        # Training params
+        self.batch_size = training_cfg.batch_size
+        self.lr = training_cfg.lr
+        self.mode = training_cfg.mode
+        self.traj_len = training_cfg.trajectory_len
+        self.epochs = training_cfg.epochs
+        self.train_sample_number = training_cfg.train_sample_number
+
+        # Diffuser training param
+        self.wta = training_cfg.wta
+        self.lr_warmup_steps = training_cfg.lr_warmup_steps
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.wta_trial_times = training_cfg.wta_trial_times
+
+        self.backbone = network
+        self.num_train_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_train_timesteps
+        )
+
+        self.cosine_distribution_cache = {"x": [], "y": [], "colors": []}
+
+    def forward(self, batch: tgd.Batch, mode):
+        x = (
+            batch.delta.squeeze(2)
+            .reshape(-1, 30, 40, 3 * self.traj_len)
+            .permute(0, 3, 1, 2)
+            .float()
+            .cuda()
+        )
+        pos = batch.pos.reshape(-1, self.sample_size, 3 * self.traj_len).float().cuda()
+
+        model_kwargs = dict(pos=pos, context=batch.cuda())
+        loss_dict = self.diffusion.training_losses(
+            self.backbone, x, batch.timesteps, model_kwargs
+        )
+        loss = loss_dict["loss"].mean()
+
+        self.log_dict(
+            {
+                f"{mode}/loss": loss,
+                # The other metrics will be tested in validation
+                # f"{mode}/rmse": rmse,
+                # f"{mode}/cosine_similarity": cos_dist,
+                # f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return None, loss
+
+    # @torch.inference_mode()
+    def predict(self, batch: tgd.Batch, mode):
+        # torch.eval()
+        bs = batch.delta.shape[0] // self.sample_size
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)  # .float()
+
+        pos = batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len).float().cuda()
+        model_kwargs = dict(pos=pos, context=batch)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        # mask = batch.mask == 1
+        # mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+        if torch.sum(f_ix) == 0:  # No point
+            return f_pred, loss
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+
+        self.log_dict(
+            {
+                f"{mode}/flow_loss": loss,
+                f"{mode}/rmse": rmse,
+                f"{mode}/cosine_similarity": cos_dist,
+                f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return f_pred, loss
+
+    def predict_wta(self, orig_batch: tgd.Batch, mode):
+        bs = orig_batch.delta.shape[0] // self.sample_size
+        assert bs == 1, "Only support bsz = 1 for winner take all evaluation"
+        bs = self.wta_trial_times
+        data_list = orig_batch.to_data_list() * bs
+        batch = tgd.Batch.from_data_list(data_list)
+
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)  # .float()
+
+        pos = batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len).float().cuda()
+        model_kwargs = dict(pos=pos, context=batch)
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        mask = batch.mask == 1
+        mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+        flow_loss = loss.reshape(bs, -1).mean(-1)
+        chosen_id = torch.min(flow_loss, 0)[1]  # index
+
+        if torch.sum(f_ix) == 0:  # No point
+            return (
+                f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+                loss[chosen_id],
+                [],
+            )
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(
+            f_pred[f_ix], f_target[f_ix], reduce=False
+        )
+
+        # Aggregate the results
+        # Choose the one with smallest flow loss
+        rmse = rmse.reshape(bs, -1).mean(-1)
+        cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+        mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+        pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+        neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+        multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+        self.log_dict(
+            {
+                f"{mode}_wta/flow_loss": flow_loss[chosen_id].item(),
+                f"{mode}_wta/rmse": rmse[chosen_id].item(),
+                f"{mode}_wta/cosine_similarity": cos_dist[chosen_id].item(),
+                f"{mode}_wta/mag_error": mag_error[chosen_id].item(),
+                f"{mode}_wta/multimodal": multimodal,
+                f"{mode}_wta/pos@0.7": pos_cosine.item(),
+                f"{mode}_wta/neg@0.7": neg_cosine.item(),
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return (
+            f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+            loss[chosen_id],
+            cos_dist.tolist(),
+        )
+
+    def configure_optimizers(self):
+        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-5)
+        lr_scheduler = get_cosine_schedule_with_warmup(
+            optimizer=optimizer,
+            num_warmup_steps=self.lr_warmup_steps,
+            # num_training_steps=(len(train_dataloader) * config.num_epochs),
+            num_training_steps=(
+                (self.train_sample_number // self.batch_size) * self.epochs
+            ),
+        )
+        return [optimizer], [lr_scheduler]
+
+    def training_step(self, batch: tgd.Batch, batch_id):  # type: ignore
+        self.train()
+        bs = batch.delta.shape[0] // self.sample_size
+
+        # batch.delta = normalize_trajectory(batch.delta)
+        batch.timesteps = torch.randint(
+            0,
+            self.num_train_timesteps,
+            (bs,),
+            device=self.device,
+        ).long()
+
+        _, loss = self(batch, "train")
+        return loss
+
+    def validation_step(self, batch: tgd.Batch, batch_id, dataloader_idx=0):  # type: ignore
+        self.eval()
+
+        # Clean cache for a new eval dataloader
+        if batch_id == 0:
+            self.cosine_distribution_cache["x"] = []
+            self.cosine_distribution_cache["y"] = []
+            self.cosine_distribution_cache["colors"] = []
+
+        dataloader_names = ["val", "train", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        with torch.no_grad():
+            f_pred, loss = self.predict(batch, name)
+            # print("predict:", f_pred.shape)
+            if self.wta:
+                f_pred, loss, cosines = self.predict_wta(batch, name)
+                self.cosine_distribution_cache["x"] += [batch_id] * len(cosines)
+                self.cosine_distribution_cache["y"] += cosines
+                self.cosine_distribution_cache["colors"] += [
+                    "blue" if batch_id % 2 == 0 else "red"
+                ] * len(cosines)
+        # breakpoint()
+        return {
+            "preds": f_pred,
+            "loss": loss,
+            "cosine_cache": self.cosine_distribution_cache,
+        }
+
+    @staticmethod
+    def make_plots(preds, batch: tgd.Batch, cosine_cache=None) -> Dict[str, go.Figure]:
+        # 1) Make the flow visualization plots
+        obj_id = batch.id
+        pos = (
+            batch.point[:, -2, :].numpy() if batch.point.shape[1] >= 2 else batch.pos
+        )  # The last step's beinning pos
+        mask = batch.mask.numpy()
+        f_target = batch.delta[:, -1, :]
+        f_pred = preds.reshape(preds.shape[0], -1, 3)[:, -1, :]
+
+        fig = make_subplots(
+            rows=2,
+            cols=2,
+            specs=[
+                [{"type": "scene", "colspan": 2}, None],
+                [{"type": "scene"}, {"type": "scene"}],
+            ],
+            subplot_titles=(
+                "input data",
+                "target flow",
+                "pred flow",
+            ),
+            vertical_spacing=0.05,
+        )
+
+        # Parent/child plot.
+        labelmap = {0: "unselected", 1: "part"}
+        labels = torch.zeros(len(pos)).int()
+        labels[mask == 1.0] = 1
+        fig.add_traces(v3p._segmentation_traces(pos, labels, labelmap, "scene1"))
+
+        fig.update_layout(
+            scene1=v3p._3d_scene(pos),
+            showlegend=True,
+            margin=dict(l=0, r=0, b=0, t=40),
+            legend=dict(x=1.0, y=0.75),
+        )
+
+        # normalize the flow for visualization.
+        n_f_gt = (f_target / f_target.norm(dim=1).max()).numpy()
+        n_f_pred = (f_pred / f_target.norm(dim=1).max()).numpy()
+
+        # GT flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene2", name="pts"), row=2, col=1)
+        f_gt_traces = v3p._flow_traces(
+            pos, n_f_gt, scene="scene2", name="f_gt", legendgroup="1"
+        )
+        fig.add_traces(f_gt_traces, rows=2, cols=1)
+        fig.update_layout(scene2=v3p._3d_scene(pos))
+
+        # Predicted flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene3", name="pts"), row=2, col=2)
+        f_pred_traces = v3p._flow_traces(
+            pos, n_f_pred, scene="scene3", name="f_pred", legendgroup="2"
+        )
+        fig.add_traces(f_pred_traces, rows=2, cols=2)
+        fig.update_layout(scene3=v3p._3d_scene(pos))
+
+        fig.update_layout(title=f"Object {obj_id}")
+
+        # 2) Make the cosine distribution plots
+        # table = wandb.Table(data=data, columns=["class_x", "class_y"])
+        # wandb.log({"my_custom_id": wandb.plot.scatter(table, "class_x", "class_y")})
+        cos_fig = None
+        if (
+            cosine_cache is not None and len(cosine_cache["x"]) != 0
+        ):  # Does wta, and needs plots
+            # The following Matplotlib code won't work because some matplotlib version issue (3.4.3 would work, but the version is old)
+            # cos_fig = plt.figure()
+            # ax = cos_fig.add_axes([0.1, 0.1, 0.8, 0.8])
+            # plt.ylim((-1, 1))
+            # ax.axhline(y=0)
+            # ax.axhline(y=0.7)
+            # ax.axhline(y=-0.7)
+            # plt.scatter(cosine_cache["x"], cosine_cache["y"], s=5, c=cosine_cache["colors"])
+            cos_fig = px.scatter(
+                x=cosine_cache["x"], y=cosine_cache["y"], color=cosine_cache["colors"]
+            )
+            cos_fig.update_layout(yaxis_range=[-1, 1])
+
+        return {"diffuser_plot": fig, "cosine_distribution_plot": cos_fig}
+
+
+class FlowTrajectoryDiffuserInferenceModule_PNDiT(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        # Inference params
+        self.batch_size = inference_cfg.batch_size
+        self.traj_len = inference_cfg.trajectory_len
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.backbone = network
+        self.num_inference_timesteps = model_cfg.num_train_timesteps
+        self.diffusion = create_diffusion(
+            timestep_respacing=None, diffusion_steps=self.num_inference_timesteps
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        ckpt = torch.load(ckpt_file)
+        self.load_state_dict(ckpt["state_dict"])
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        print(
+            "Don't call this, it's not implemented. You should call predict_step or predict_wta"
+        )
+        return None
+
+    def predict(self, P_world) -> torch.Tensor:  # From pure point cloud
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
+
+    @torch.no_grad()
+    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:  # type: ignore
+        # torch.eval()
+        self.eval()
+        bs = batch.pos.shape[0] // self.sample_size
+        z = torch.randn(bs, 3 * self.traj_len, 30, 40, device=self.device)  # .float()
+
+        pos = batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len).float().cuda()
+        model_kwargs = dict(pos=pos, context=batch.cuda())
+
+        samples, results = self.diffusion.p_sample_loop(
+            self.backbone,
+            z.shape,
+            z,
+            clip_denoised=False,
+            model_kwargs=model_kwargs,
+            progress=True,
+            device=self.device,
+        )
+
+        f_pred = (
+            torch.flatten(samples, start_dim=2, end_dim=3)
+            .permute(0, 2, 1)
+            .reshape(-1, 3 * self.traj_len)
+            .unsqueeze(1)
+        )
+        f_pred = normalize_trajectory(f_pred)
+        return f_pred
+
+    # For winner takes it all evaluation
+    @torch.inference_mode()
+    def predict_wta(self, dataloader, mode="delta", trial_times=50):
+        all_rmse = 0
+        all_cos_dist = 0
+        all_mag_error = 0
+        all_flow_loss = 0
+        all_multimodal = 0
+        all_pos_cosine = 0
+        all_neg_cosine = 0
+        valid_sample_cnt = 0
+
+        for id, orig_sample in tqdm.tqdm(enumerate(dataloader)):
+            bs = orig_sample.delta.shape[0] // self.sample_size
+            assert bs == 1, f"batch size should be 1, now is {bs}"
+
+            # batch every sample into bsz of trial_times
+            bs = trial_times
+            data_list = orig_sample.to_data_list() * bs
+            batch = tgd.Batch.from_data_list(data_list)
+
+            z = torch.randn(
+                bs, 3 * self.traj_len, 30, 40, device=self.device
+            )  # .float()
+
+            pos = (
+                batch.pos.reshape(bs, self.sample_size, 3 * self.traj_len)
+                .float()
+                .to(self.device)
+            )
+            model_kwargs = dict(pos=pos, context=batch)
+            # breakpoint()
+            samples, results = self.diffusion.p_sample_loop(
+                self.backbone,
+                z.shape,
+                z,
+                clip_denoised=False,
+                model_kwargs=model_kwargs,
+                progress=True,
+                device=self.device,
+            )
+
+            f_pred = (
+                torch.flatten(samples, start_dim=2, end_dim=3)
+                .permute(0, 2, 1)
+                .reshape(-1, 3 * self.traj_len)
+                .unsqueeze(1)
+            )
+            f_pred = normalize_trajectory(f_pred)
+
+            # Compute the loss.
+            mask = batch.mask == 1
+            mask = mask.reshape(-1, self.sample_size).to("cuda")
+
+            n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+            f_ix = batch.mask.bool().to(self.device)
+            if torch.sum(f_ix) == 0:
+                continue
+
+            valid_sample_cnt += 1
+            if mode == "delta":
+                f_target = batch.delta.to(self.device)
+            elif mode == "point":
+                f_target = batch.point.to(self.device)
+
+            f_target = f_target  # .float()
+            f_target = normalize_trajectory(f_target)
+
+            # print(f_pred[f_ix], batch.delta[f_ix])
+            # print(f_pred.device, f_target.device)
+            flow_loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+
+            # Compute some metrics on flow-only regions.
+            rmse, cos_dist, mag_error = flow_metrics(
+                f_pred[f_ix], f_target[f_ix], reduce=False
+            )
+
+            # Aggregate the results
+            # Choose the one with smallest flow loss
+            flow_loss = flow_loss.reshape(bs, -1).mean(-1)
+            rmse = rmse.reshape(bs, -1).mean(-1)
+            cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+            mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+            chosen_id = torch.min(flow_loss, 0)[1]  # index
+            pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+            neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+            multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+            print(
+                multimodal,
+                rmse[chosen_id],
+                cos_dist[chosen_id],
+                mag_error[chosen_id],
+                flow_loss[chosen_id],
+            )
+
+            all_multimodal += multimodal
+            all_pos_cosine += pos_cosine.item()
+            all_neg_cosine += neg_cosine.item()
+            all_rmse += rmse[chosen_id].item()
+            all_cos_dist += cos_dist[chosen_id].item()
+            all_mag_error += mag_error[chosen_id].item()
+            all_flow_loss += flow_loss[chosen_id].item()
+
+        metric_dict = {
+            f"flow_loss": all_flow_loss / valid_sample_cnt,  # / len(dataloader),
+            f"rmse": all_rmse / valid_sample_cnt,  # / len(dataloader),
+            f"cosine_similarity": all_cos_dist / valid_sample_cnt,  # / len(dataloader),
+            f"mag_error": all_mag_error / valid_sample_cnt,  # / len(dataloader),
+            f"multimodal": all_multimodal / valid_sample_cnt,  # / len(dataloader),
+            f"pos@0.7": all_pos_cosine / valid_sample_cnt,  # / len(dataloader),
+            f"neg@0.7": all_neg_cosine / valid_sample_cnt,  # / len(dataloader),
+        }
+
+        self.log_dict(
+            metric_dict,
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return metric_dict, cos_dist.tolist()  # dataloader * trial_times
+
+
+class FlowTrajectoryDiffuserSimulationModule_PNDiT(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        self.model = FlowTrajectoryDiffuserInferenceModule_PNDiT(
+            network, inference_cfg, model_cfg
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        self.model.load_from_ckpt(ckpt_file)
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
+
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.model.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
diff --git a/src/flowbothd/models/flow_predictor.py b/src/flowbothd/models/flow_predictor.py
new file mode 100644
index 0000000..8397764
--- /dev/null
+++ b/src/flowbothd/models/flow_predictor.py
@@ -0,0 +1,192 @@
+import typing
+from typing import Any, Dict
+
+import lightning as L
+import plotly.graph_objects as go
+import rpad.visualize_3d.plots as v3p
+import torch
+import torch_geometric.data as tgd
+from flowbot3d.models.artflownet import artflownet_loss, flow_metrics
+from plotly.subplots import make_subplots
+from torch import optim
+
+
+# Flow predictor
+class FlowPredictorTrainingModule(L.LightningModule):
+    def __init__(self, network, training_cfg, model_cfg=None) -> None:
+        super().__init__()
+        self.network = network
+        self.lr = training_cfg.lr
+        self.batch_size = training_cfg.batch_size
+        self.mask_input_channel = training_cfg.mask_input_channel
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        if self.mask_input_channel:
+            data.x = data.mask.reshape(len(data.mask), 1)
+
+        # Run the model.
+        flow = typing.cast(torch.Tensor, self.network(data))
+
+        return flow
+
+    def _step(self, batch: tgd.Batch, mode):
+        # Make a prediction.
+        f_pred = self(batch)
+
+        # Compute the loss.
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        f_target = batch.flow.float()
+        loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+        # import pdb
+        # pdb.set_trace()
+
+        if mode == "train_train":
+            self.log_dict(
+                {
+                    f"train/loss": loss,
+                },
+                add_dataloader_idx=False,
+                batch_size=len(batch),
+            )
+        else:
+            self.log_dict(
+                {
+                    f"{mode}/flow_loss": loss,
+                    f"{mode}/rmse": rmse,
+                    f"{mode}/cosine_similarity": cos_dist,
+                    f"{mode}/mag_error": mag_error,
+                },
+                add_dataloader_idx=False,
+                batch_size=len(batch),
+            )
+
+        return f_pred.reshape(len(batch), -1, 3), loss
+
+    def configure_optimizers(self):
+        optimizer = optim.AdamW(self.parameters(), lr=self.lr)
+        lr_scheduler = optim.lr_scheduler.MultiStepLR(
+            optimizer, milestones=[100, 150], gamma=0.1
+        )
+        return [optimizer], [lr_scheduler]
+
+    def training_step(self, batch: tgd.Batch, batch_id):  # type: ignore
+        self.train()
+        f_pred, loss = self._step(batch, "train")
+        return {"loss": loss, "preds": f_pred}
+
+    def validation_step(self, batch: tgd.Batch, batch_id, dataloader_idx=0):  # type: ignore
+        self.eval()
+        dataloader_names = ["val", "train", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        f_pred, loss = self._step(batch, name)
+        return {"preds": f_pred, "loss": loss, "cosine_cache": None}
+
+    @staticmethod
+    def make_plots(preds, batch: tgd.Batch, cosine_cache=None) -> Dict[str, go.Figure]:
+        obj_id = batch.id
+        pos = batch.pos.numpy()
+        mask = batch.mask.numpy()
+        f_target = batch.flow
+        f_pred = preds
+
+        fig = make_subplots(
+            rows=2,
+            cols=2,
+            specs=[
+                [{"type": "scene", "colspan": 2}, None],
+                [{"type": "scene"}, {"type": "scene"}],
+            ],
+            subplot_titles=(
+                "input data",
+                "target flow",
+                "pred flow",
+            ),
+            vertical_spacing=0.05,
+        )
+
+        # Parent/child plot.
+        labelmap = {0: "unselected", 1: "part"}
+        labels = torch.zeros(len(pos)).int()
+        labels[mask == 1.0] = 1
+        fig.add_traces(v3p._segmentation_traces(pos, labels, labelmap, "scene1"))
+
+        fig.update_layout(
+            scene1=v3p._3d_scene(pos),
+            showlegend=True,
+            margin=dict(l=0, r=0, b=0, t=40),
+            legend=dict(x=1.0, y=0.75),
+        )
+
+        # normalize the flow for visualization.
+        n_f_gt = (f_target / f_target.norm(dim=1).max()).numpy()
+        n_f_pred = (f_pred / f_target.norm(dim=1).max()).numpy()
+
+        # GT flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene2", name="pts"), row=2, col=1)
+        f_gt_traces = v3p._flow_traces(
+            pos, n_f_gt, scene="scene2", name="f_gt", legendgroup="1"
+        )
+        fig.add_traces(f_gt_traces, rows=2, cols=1)
+        fig.update_layout(scene2=v3p._3d_scene(pos))
+
+        # Predicted flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene3", name="pts"), row=2, col=2)
+        f_pred_traces = v3p._flow_traces(
+            pos, n_f_pred, scene="scene3", name="f_pred", legendgroup="2"
+        )
+        fig.add_traces(f_pred_traces, rows=2, cols=2)
+        fig.update_layout(scene3=v3p._3d_scene(pos))
+
+        fig.update_layout(title=f"Object {obj_id}")
+
+        return {"artflownet_plot": fig}
+
+
+class FlowPredictorInferenceModule(L.LightningModule):
+    def __init__(self, network, inference_config) -> None:
+        super().__init__()
+        self.network = network
+        self.mask_input_channel = inference_config.mask_input_channel
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        if self.mask_input_channel:
+            data.x = data.mask.reshape(len(data.mask), 1)
+
+        # Run the model.
+        flow = typing.cast(torch.Tensor, self.network(data))
+
+        return flow
+
+    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:  # type: ignore
+        return self.forward(batch)
+
+    # the predict step input is different now, pay attention
+    def predict(self, xyz: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
+        """Predict the flow for a single object. The point cloud should
+        come straight from the maniskill processed observation function.
+
+        Args:
+            xyz (torch.Tensor): Nx3 pointcloud
+            mask (torch.Tensor): Nx1 mask of the part that will move.
+
+        Returns:
+            torch.Tensor: Nx3 dense flow prediction
+        """
+        print(xyz, mask)
+        assert len(xyz) == len(mask)
+        assert len(xyz.shape) == 2
+        assert len(mask.shape) == 1
+
+        data = tgd.Data(pos=xyz, mask=mask)
+        batch = tgd.Batch.from_data_list([data])
+        batch = batch.to(self.device)
+        self.eval()
+        with torch.no_grad():
+            flow = self.forward(batch)
+        return flow
diff --git a/src/flowbothd/models/flow_trajectory_diffuser.py b/src/flowbothd/models/flow_trajectory_diffuser.py
new file mode 100644
index 0000000..3f70a47
--- /dev/null
+++ b/src/flowbothd/models/flow_trajectory_diffuser.py
@@ -0,0 +1,689 @@
+from typing import Any, Dict
+
+import lightning as L
+import plotly.express as px
+import plotly.graph_objects as go
+import rpad.visualize_3d.plots as v3p
+import torch
+import torch.nn.functional as F
+import torch_geometric.data as tgd
+import tqdm
+from diffusers import DDIMScheduler, DDPMScheduler
+from diffusers.models.embeddings import TimestepEmbedding, Timesteps
+from diffusers.optimization import get_cosine_schedule_with_warmup
+from plotly.subplots import make_subplots
+from torch import optim
+
+from flowbothd.metrics.trajectory import (
+    artflownet_loss,
+    flow_metrics,
+    normalize_trajectory,
+)
+
+
+# Flow diffuser with PN++
+class FlowTrajectoryDiffusionModule_PN2(L.LightningModule):
+    def __init__(self, network, training_cfg, model_cfg) -> None:
+        super().__init__()
+        # Training params
+        self.batch_size = training_cfg.batch_size
+        self.lr = training_cfg.lr
+        self.mode = training_cfg.mode
+        self.traj_len = training_cfg.trajectory_len
+        self.epochs = training_cfg.epochs
+        self.train_sample_number = training_cfg.train_sample_number
+
+        # Diffuser training param
+        self.wta = training_cfg.wta
+        self.wta_trial_times = training_cfg.wta_trial_times
+        self.lr_warmup_steps = training_cfg.lr_warmup_steps
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.time_embed_dim = model_cfg.time_embed_dim
+        self.in_channels = 3 * self.traj_len + self.time_embed_dim
+        assert (
+            network.in_ch == self.in_channels
+        ), "Network input channels doesn't match expectation"
+
+        # positional time embeddings
+        flip_sin_to_cos = model_cfg.flip_sin_to_cos
+        freq_shift = model_cfg.freq_shift
+        self.time_proj = Timesteps(64, flip_sin_to_cos, freq_shift)
+        timestep_input_dim = model_cfg.time_proj_dim
+        self.time_emb = TimestepEmbedding(timestep_input_dim, self.time_embed_dim)
+
+        self.backbone = network
+        self.num_train_timesteps = model_cfg.num_train_timesteps
+        self.noise_scheduler = DDPMScheduler(
+            num_train_timesteps=model_cfg.num_train_timesteps
+        )
+
+        self.cosine_distribution_cache = {"x": [], "y": [], "colors": []}
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        timesteps = data.timesteps
+        traj_noise = data.traj_noise  # bs * 1200, traj_len, 3
+        traj_noise = torch.flatten(traj_noise, start_dim=1, end_dim=2)
+
+        t_emb = self.time_emb(self.time_proj(timesteps))  # bs, 64
+        # Repeat the time embedding. MAKE SURE THAT EACH BATCH ITEM IS INDEPENDENT!
+        t_emb = t_emb.unsqueeze(1).repeat(1, self.sample_size, 1)  # bs, 1200, 64
+        t_emb = torch.flatten(t_emb, start_dim=0, end_dim=1)  # bs * 1200, 64
+
+        data.x = torch.cat([traj_noise, t_emb], dim=-1)  # bs * 1200, 64 + 3 * traj_len
+
+        # Run the model.
+        pred = self.backbone(data)
+
+        return pred
+
+    def _step(self, batch: tgd.Batch, mode):
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+        f_target = f_target  # .float()
+
+        # noisy_flow = batch.traj_noise
+        # Predict the noise.
+        noise = batch.pure_noise  # The added noise
+        noise_pred = self(batch)
+        noise_pred = noise_pred.reshape(
+            noise_pred.shape[0], -1, 3
+        )  # batch, traj_len, 3
+
+        # Compute the loss.
+        loss = F.mse_loss(noise_pred, noise)
+
+        self.log_dict(
+            {
+                f"{mode}/loss": loss,
+                # The other metrics will be tested in validation
+                # f"{mode}/rmse": rmse,
+                # f"{mode}/cosine_similarity": cos_dist,
+                # f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return None, loss
+
+    # @torch.inference_mode()
+    def predict(self, batch: tgd.Batch, mode):
+        # torch.eval()
+        bs = batch.delta.shape[0] // self.sample_size
+        batch.traj_noise = torch.randn_like(batch.delta, device=self.device)  # .float()
+        # batch.traj_noise = normalize_trajectory(batch.traj_noise)
+        # breakpoint()
+
+        # import time
+        # batch_time = 0
+        # model_time = 0
+        # noise_scheduler_time = 0
+
+        # self.noise_scheduler_inference.set_timesteps(self.num_inference_timesteps)
+        # print(self.noise_scheduler_inference.timesteps)
+        # for t in self.noise_scheduler_inference.timesteps:
+        for t in self.noise_scheduler.timesteps:
+            # tm = time.time()
+            batch.timesteps = torch.zeros(bs, device=self.device) + t  # Uniform t steps
+            batch.timesteps = batch.timesteps.long()
+            # batch_time += time.time() - tm
+
+            # tm = time.time()
+            model_output = self(batch)  # bs * 1200, traj_len * 3
+            model_output = model_output.reshape(
+                model_output.shape[0], -1, 3
+            )  # bs * 1200, traj_len, 3
+
+            batch.traj_noise = self.noise_scheduler.step(
+                # batch.traj_noise = self.noise_scheduler_inference.step(
+                model_output.reshape(
+                    -1, self.sample_size, model_output.shape[1], model_output.shape[2]
+                ),
+                t,
+                batch.traj_noise.reshape(
+                    -1, self.sample_size, model_output.shape[1], model_output.shape[2]
+                ),
+            ).prev_sample
+            batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)
+
+        f_pred = batch.traj_noise  # .float()
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+        # print(f_pred[f_ix], batch.delta[f_ix])
+        loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+
+        self.log_dict(
+            {
+                f"{mode}/flow_loss": loss,
+                f"{mode}/rmse": rmse,
+                f"{mode}/cosine_similarity": cos_dist,
+                f"{mode}/mag_error": mag_error,
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return f_pred, loss
+
+    def predict_wta(self, orig_batch: tgd.Batch, mode):
+        bs = orig_batch.delta.shape[0] // self.sample_size
+        assert bs == 1, f"batch size should be 1, now is {bs}"
+
+        # batch every sample into bsz of trial_times
+        bs = self.wta_trial_times
+        data_list = orig_batch.to_data_list() * self.wta_trial_times
+        batch = tgd.Batch.from_data_list(data_list)
+
+        batch.traj_noise = torch.randn_like(batch.delta, device=self.device)
+
+        for t in self.noise_scheduler.timesteps:
+            batch.timesteps = torch.zeros(bs, device=self.device) + t  # Uniform t steps
+            batch.timesteps = batch.timesteps.long()
+
+            model_output = self(batch)  # bs * 1200, traj_len * 3
+            model_output = model_output.reshape(
+                model_output.shape[0], -1, 3
+            )  # bs * 1200, traj_len, 3
+
+            batch.traj_noise = self.noise_scheduler.step(
+                model_output.reshape(
+                    -1,
+                    self.sample_size,
+                    model_output.shape[1],
+                    model_output.shape[2],
+                ),
+                t,
+                batch.traj_noise.reshape(
+                    -1,
+                    self.sample_size,
+                    model_output.shape[1],
+                    model_output.shape[2],
+                ),
+            ).prev_sample
+            batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)
+
+        f_pred = batch.traj_noise  # .float()
+        f_pred = normalize_trajectory(f_pred)
+
+        # Compute the loss.
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target  # .float()
+        f_target = normalize_trajectory(f_target)
+        flow_loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(
+            f_pred[f_ix], f_target[f_ix], reduce=False
+        )
+
+        # Aggregate the results
+        # Choose the one with smallest flow loss
+        flow_loss = flow_loss.reshape(bs, -1).mean(-1)
+        rmse = rmse.reshape(bs, -1).mean(-1)
+        cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+        mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+        chosen_id = torch.min(flow_loss, 0)[1]  # index
+        pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+        neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+        multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+        self.log_dict(
+            {
+                f"{mode}_wta/flow_loss": flow_loss[chosen_id].item(),
+                f"{mode}_wta/rmse": rmse[chosen_id].item(),
+                f"{mode}_wta/cosine_similarity": cos_dist[chosen_id].item(),
+                f"{mode}_wta/mag_error": mag_error[chosen_id].item(),
+                f"{mode}_wta/multimodal": multimodal,
+                f"{mode}_wta/pos@0.7": pos_cosine.item(),
+                f"{mode}_wta/neg@0.7": neg_cosine.item(),
+            },
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return (
+            f_pred.reshape(bs, self.sample_size, self.traj_len, 3)[chosen_id],
+            flow_loss[chosen_id],
+            cos_dist.tolist(),
+        )
+
+    def configure_optimizers(self):
+        optimizer = optim.AdamW(self.parameters(), lr=self.lr)
+        lr_scheduler = get_cosine_schedule_with_warmup(
+            optimizer=optimizer,
+            num_warmup_steps=self.lr_warmup_steps,
+            # num_training_steps=(len(train_dataloader) * config.num_epochs),
+            num_training_steps=(
+                (self.train_sample_number // self.batch_size) * self.epochs
+            ),
+        )
+        return [optimizer], [lr_scheduler]
+
+    def training_step(self, batch: tgd.Batch, batch_id):  # type: ignore
+        self.train()
+        batch.delta = normalize_trajectory(batch.delta)
+        # Add timestep & random noise
+        bs = batch.delta.shape[0] // self.sample_size
+        # breakpoint()
+        batch.timesteps = torch.randint(
+            0,
+            self.noise_scheduler.config.num_train_timesteps,
+            (bs,),
+            device=self.device,
+        ).long()
+
+        noise = torch.randn_like(batch.delta, device=self.device)
+        batch.pure_noise = noise
+        # broadcast_timestep = torch.flatten(batch.timesteps.unsqueeze(1).repeat(1, self.sample_size), start_dim=0, end_dim=-1)  # bs * 1200
+
+        batch.traj_noise = self.noise_scheduler.add_noise(
+            batch.delta.reshape(-1, self.sample_size, noise.shape[1], noise.shape[2]),
+            noise.reshape(
+                -1, self.sample_size, noise.shape[1], noise.shape[2]
+            ),  # bs, 1200, traj_len, 3
+            # broadcast_timestep
+            batch.timesteps,
+        )
+        batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)
+
+        _, loss = self._step(batch, "train")
+        return loss
+
+    def validation_step(self, batch: tgd.Batch, batch_id, dataloader_idx=0):  # type: ignore
+        self.eval()
+
+        # Clean cache for a new eval dataloader
+        if batch_id == 0:
+            self.cosine_distribution_cache["x"] = []
+            self.cosine_distribution_cache["y"] = []
+            self.cosine_distribution_cache["colors"] = []
+
+        dataloader_names = ["val", "train", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        with torch.no_grad():
+            f_pred, loss = self.predict(batch, name)
+            if self.wta:
+                f_pred, loss, cosines = self.predict_wta(batch, name)
+                self.cosine_distribution_cache["x"] += [batch_id] * self.wta_trial_times
+                self.cosine_distribution_cache["y"] += cosines
+                self.cosine_distribution_cache["colors"] += [
+                    "blue" if batch_id % 2 == 0 else "red"
+                ] * self.wta_trial_times
+        # breakpoint()
+        return {
+            "preds": f_pred,
+            "loss": loss,
+            "cosine_cache": self.cosine_distribution_cache,
+        }
+
+    @staticmethod
+    def make_plots(preds, batch: tgd.Batch, cosine_cache=None) -> Dict[str, go.Figure]:
+        # 1) Make the flow visualization plots
+        obj_id = batch.id
+        pos = (
+            batch.point[:, -2, :].numpy() if batch.point.shape[1] >= 2 else batch.pos
+        )  # The last step's beinning pos
+        mask = batch.mask.numpy()
+        f_target = batch.delta[:, -1, :]
+        f_pred = preds.reshape(preds.shape[0], -1, 3)[:, -1, :]
+
+        fig = make_subplots(
+            rows=2,
+            cols=2,
+            specs=[
+                [{"type": "scene", "colspan": 2}, None],
+                [{"type": "scene"}, {"type": "scene"}],
+            ],
+            subplot_titles=(
+                "input data",
+                "target flow",
+                "pred flow",
+            ),
+            vertical_spacing=0.05,
+        )
+
+        # Parent/child plot.
+        labelmap = {0: "unselected", 1: "part"}
+        labels = torch.zeros(len(pos)).int()
+        labels[mask == 1.0] = 1
+        fig.add_traces(v3p._segmentation_traces(pos, labels, labelmap, "scene1"))
+
+        fig.update_layout(
+            scene1=v3p._3d_scene(pos),
+            showlegend=True,
+            margin=dict(l=0, r=0, b=0, t=40),
+            legend=dict(x=1.0, y=0.75),
+        )
+
+        # normalize the flow for visualization.
+        n_f_gt = (f_target / f_target.norm(dim=1).max()).numpy()
+        n_f_pred = (f_pred / f_target.norm(dim=1).max()).numpy()
+
+        # GT flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene2", name="pts"), row=2, col=1)
+        f_gt_traces = v3p._flow_traces(
+            pos, n_f_gt, scene="scene2", name="f_gt", legendgroup="1"
+        )
+        fig.add_traces(f_gt_traces, rows=2, cols=1)
+        fig.update_layout(scene2=v3p._3d_scene(pos))
+
+        # Predicted flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene3", name="pts"), row=2, col=2)
+        f_pred_traces = v3p._flow_traces(
+            pos, n_f_pred, scene="scene3", name="f_pred", legendgroup="2"
+        )
+        fig.add_traces(f_pred_traces, rows=2, cols=2)
+        fig.update_layout(scene3=v3p._3d_scene(pos))
+
+        fig.update_layout(title=f"Object {obj_id}")
+
+        # 2) Make the cosine distribution plots
+        cos_fig = None
+        if (
+            cosine_cache is not None and len(cosine_cache["x"]) != 0
+        ):  # Does wta, and needs plots
+            # The following Matplotlib code won't work because some matplotlib version issue (3.4.3 would work, but the version is old)
+            # cos_fig = plt.figure()
+            # ax = cos_fig.add_axes([0.1, 0.1, 0.8, 0.8])
+            # plt.ylim((-1, 1))
+            # ax.axhline(y=0)
+            # ax.axhline(y=0.7)
+            # ax.axhline(y=-0.7)
+            # plt.scatter(cosine_cache["x"], cosine_cache["y"], s=5, c=cosine_cache["colors"])
+            cos_fig = px.scatter(
+                x=cosine_cache["x"], y=cosine_cache["y"], color=cosine_cache["colors"]
+            )
+            cos_fig.update_layout(yaxis_range=[-1, 1])
+
+        return {"diffuser_plot": fig, "cosine_distribution_plot": cos_fig}
+
+
+class FlowTrajectoryDiffuserInferenceModule_PN2(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        # Inference params
+        self.batch_size = inference_cfg.batch_size
+        self.traj_len = inference_cfg.trajectory_len
+
+        # Diffuser params
+        self.sample_size = 1200
+        self.time_embed_dim = model_cfg.time_embed_dim
+        self.in_channels = 3 * self.traj_len + self.time_embed_dim
+        assert (
+            network.in_ch == self.in_channels
+        ), "Network input channels doesn't match expectation"
+
+        # positional time embeddings
+        flip_sin_to_cos = model_cfg.flip_sin_to_cos
+        freq_shift = model_cfg.freq_shift
+        self.time_proj = Timesteps(64, flip_sin_to_cos, freq_shift)
+        timestep_input_dim = model_cfg.time_proj_dim
+        self.time_emb = TimestepEmbedding(timestep_input_dim, self.time_embed_dim)
+
+        self.backbone = network
+        self.noise_scheduler = DDPMScheduler(
+            num_train_timesteps=model_cfg.num_train_timesteps
+        )
+        self.faster_noise_scheduler = DDIMScheduler(
+            num_train_timesteps=model_cfg.num_train_timesteps
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        ckpt = torch.load(ckpt_file)
+        self.load_state_dict(ckpt["state_dict"])
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        timesteps = data.timesteps
+        traj_noise = data.traj_noise  # bs * 1200, traj_len, 3
+        traj_noise = torch.flatten(traj_noise, start_dim=1, end_dim=2)
+
+        t_emb = self.time_emb(self.time_proj(timesteps))  # bs, 64
+        # Repeat the time embedding. MAKE SURE THAT EACH BATCH ITEM IS INDEPENDENT!
+        t_emb = t_emb.unsqueeze(1).repeat(1, self.sample_size, 1)  # bs, 1200, 64
+        t_emb = torch.flatten(t_emb, start_dim=0, end_dim=1)  # bs * 1200, 64
+
+        data.x = torch.cat([traj_noise, t_emb], dim=-1)  # bs * 1200, 64 + 3 * traj_len
+        # Run the model.
+        data.mask = data.mask.to(self.device)
+        data.pos = data.pos.to(self.device)
+        data.ptr = data.ptr.to(self.device)
+        data.batch = data.batch.to(self.device)
+        pred = self.backbone(data)
+
+        return pred
+
+    @torch.no_grad()
+    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:  # type: ignore
+        bs = batch.pos.shape[0] // self.sample_size
+        batch.traj_noise = torch.randn(
+            (batch.pos.shape[0], self.traj_len, 3), device=self.device
+        )  # .float()
+        for t in self.noise_scheduler.timesteps:
+            batch.timesteps = torch.zeros(bs, device=self.device) + t  # Uniform t steps
+            batch.timesteps = batch.timesteps.long()
+            model_output = self(batch)  # bs * 1200, traj_len * 3
+            model_output = model_output.reshape(
+                model_output.shape[0], -1, 3
+            )  # bs * 1200, traj_len, 3
+
+            batch.traj_noise = self.noise_scheduler.step(
+                # batch.traj_noise = self.noise_scheduler_inference.step(
+                model_output.reshape(
+                    -1, self.sample_size, model_output.shape[1], model_output.shape[2]
+                ),
+                t,
+                batch.traj_noise.reshape(
+                    -1, self.sample_size, model_output.shape[1], model_output.shape[2]
+                ),
+            ).prev_sample
+            batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)
+
+        f_pred = batch.traj_noise  # .float()
+
+        # print(f_pred.shape)
+        return f_pred
+
+    @torch.no_grad()
+    def faster_predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:  # type: ignore
+        bs = batch.pos.shape[0] // self.sample_size
+        batch.traj_noise = torch.randn(
+            (batch.pos.shape[0], self.traj_len, 3), device=self.device
+        )  # .float()
+        self.faster_noise_scheduler.set_timesteps(100)
+        for t in self.noise_scheduler.timesteps:
+            batch.timesteps = torch.zeros(bs, device=self.device) + t  # Uniform t steps
+            batch.timesteps = batch.timesteps.long()
+            model_output = self(batch)  # bs * 1200, traj_len * 3
+            model_output = model_output.reshape(
+                model_output.shape[0], -1, 3
+            )  # bs * 1200, traj_len, 3
+
+            batch.traj_noise = self.faster_noise_scheduler.step(
+                # batch.traj_noise = self.noise_scheduler_inference.step(
+                model_output.reshape(
+                    -1, self.sample_size, model_output.shape[1], model_output.shape[2]
+                ),
+                t,
+                batch.traj_noise.reshape(
+                    -1, self.sample_size, model_output.shape[1], model_output.shape[2]
+                ),
+            ).prev_sample
+            batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)
+
+        f_pred = batch.traj_noise  # .float()
+
+        print(f_pred.shape)
+        return f_pred
+
+    # For winner takes it all evaluation
+    @torch.inference_mode()
+    def predict_wta(self, dataloader, mode="delta", trial_times=50):
+        print(self.device)
+        all_rmse = 0
+        all_cos_dist = 0
+        all_mag_error = 0
+        all_flow_loss = 0
+        all_multimodal = 0
+        all_pos_cosine = 0
+        all_neg_cosine = 0
+
+        for id, orig_sample in tqdm.tqdm(enumerate(dataloader)):
+            bs = orig_sample.delta.shape[0] // self.sample_size
+            assert bs == 1, f"batch size should be 1, now is {bs}"
+
+            # batch every sample into bsz of trial_times
+            data_list = orig_sample.to_data_list() * trial_times
+            batch = tgd.Batch.from_data_list(data_list)
+            bs = trial_times
+
+            batch.traj_noise = torch.randn_like(batch.delta, device=self.device).to(
+                self.device
+            )  # .float()
+
+            for t in self.noise_scheduler.timesteps:
+                batch.timesteps = (
+                    torch.zeros(bs, device=self.device) + t
+                )  # Uniform t steps
+                batch.timesteps = batch.timesteps.long()
+
+                model_output = self(batch)  # bs * 1200, traj_len * 3
+                model_output = model_output.reshape(
+                    model_output.shape[0], -1, 3
+                )  # bs * 1200, traj_len, 3
+
+                batch.traj_noise = self.noise_scheduler.step(
+                    model_output.reshape(
+                        -1,
+                        self.sample_size,
+                        model_output.shape[1],
+                        model_output.shape[2],
+                    ),
+                    t,
+                    batch.traj_noise.reshape(
+                        -1,
+                        self.sample_size,
+                        model_output.shape[1],
+                        model_output.shape[2],
+                    ),
+                ).prev_sample
+                batch.traj_noise = torch.flatten(
+                    batch.traj_noise, start_dim=0, end_dim=1
+                )
+
+            f_pred = batch.traj_noise  # .float()
+            f_pred = normalize_trajectory(f_pred)
+
+            # Compute the loss.
+            n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+            f_ix = batch.mask.bool().to(self.device)
+            if mode == "delta":
+                f_target = batch.delta.to(self.device)
+            elif mode == "point":
+                f_target = batch.point.to(self.device)
+
+            f_target = f_target  # .float()
+            f_target = normalize_trajectory(f_target)
+            flow_loss = artflownet_loss(f_pred, f_target, n_nodes, reduce=False)
+
+            # Compute some metrics on flow-only regions.
+            rmse, cos_dist, mag_error = flow_metrics(
+                f_pred[f_ix], f_target[f_ix], reduce=False
+            )
+
+            # Aggregate the results
+            # Choose the one with smallest flow loss
+            flow_loss = flow_loss.reshape(bs, -1).mean(-1)
+            rmse = rmse.reshape(bs, -1).mean(-1)
+            cos_dist = cos_dist.reshape(bs, -1).mean(-1)
+
+            # all_directions += list(cos_dist)
+
+            mag_error = mag_error.reshape(bs, -1).mean(-1)
+
+            chosen_id = torch.min(flow_loss, 0)[1]  # index
+            pos_cosine = torch.sum((cos_dist - 0.7) > 0) / bs
+            neg_cosine = torch.sum((cos_dist + 0.7) < 0) / bs
+            multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
+
+            print(
+                multimodal,
+                rmse[chosen_id],
+                cos_dist[chosen_id],
+                mag_error[chosen_id],
+                flow_loss[chosen_id],
+            )
+            all_multimodal += multimodal  # .item()
+            all_rmse += rmse[chosen_id].item()
+            all_cos_dist += cos_dist[chosen_id].item()
+            all_mag_error += mag_error[chosen_id].item()
+            all_flow_loss += flow_loss[chosen_id].item()
+            all_pos_cosine += pos_cosine.item()
+            all_neg_cosine += neg_cosine.item()
+
+        metric_dict = {
+            f"flow_loss": all_flow_loss / len(dataloader),
+            f"rmse": all_rmse / len(dataloader),
+            f"cosine_similarity": all_cos_dist / len(dataloader),
+            f"mag_error": all_mag_error / len(dataloader),
+            f"multimodal": all_multimodal / len(dataloader),
+            f"pos@0.7": all_pos_cosine / len(dataloader),
+            f"neg@0.7": all_neg_cosine / len(dataloader),
+        }
+
+        self.log_dict(
+            metric_dict,
+            add_dataloader_idx=False,
+            batch_size=len(batch),
+        )
+        return metric_dict, cos_dist.tolist()  # dataloader * trial_times
+
+
+class FlowTrajectoryDiffuserSimulationModule_PN2(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg) -> None:
+        super().__init__()
+        self.model = FlowTrajectoryDiffuserInferenceModule_PN2(
+            network, inference_cfg, model_cfg
+        )
+
+    def load_from_ckpt(self, ckpt_file):
+        self.model.load_from_ckpt(ckpt_file)
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
+
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float().cuda(),
+            # mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        # batch = batch.to(self.device)
+        # batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            # trajectory = self.model.faster_predict_step(batch, 0)
+            trajectory = self.model.predict_step(batch, 0)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.cpu()
diff --git a/src/flowbothd/models/flow_trajectory_predictor.py b/src/flowbothd/models/flow_trajectory_predictor.py
new file mode 100644
index 0000000..3ae1a3e
--- /dev/null
+++ b/src/flowbothd/models/flow_trajectory_predictor.py
@@ -0,0 +1,322 @@
+import typing
+from typing import Any, Dict
+
+import lightning as L
+import plotly.graph_objects as go
+import rpad.visualize_3d.plots as v3p
+import torch
+import torch_geometric.data as tgd
+from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation
+
+# from flowbot3d.models.artflownet import artflownet_loss, flow_metrics
+# from flowbot3d.models.artflownet import artflownet_loss
+from plotly.subplots import make_subplots
+from torch import optim
+
+from flowbothd.metrics.trajectory import artflownet_loss, flow_metrics
+
+# def flow_metrics(pred_flow, gt_flow):
+#     with torch.no_grad():
+#         # RMSE
+#         rmse = (pred_flow - gt_flow).norm(p=2, dim=-1).mean()
+
+#         # Cosine similarity, normalized.
+#         nonzero_gt_flowixs = torch.where(gt_flow.norm(dim=-1) != 0.0)
+#         gt_flow_nz = gt_flow[nonzero_gt_flowixs]
+#         pred_flow_nz = pred_flow[nonzero_gt_flowixs]
+#         cos_dist = torch.cosine_similarity(pred_flow_nz, gt_flow_nz, dim=-1).mean()
+
+#         # Magnitude
+#         mag_error = (
+#             (pred_flow.norm(p=2, dim=-1) - gt_flow.norm(p=2, dim=-1)).abs().mean()
+#         )
+#     print(rmse, cos_dist, mag_error)
+#     return rmse, cos_dist, mag_error
+
+
+# def artflownet_loss(
+#     f_pred: torch.Tensor,
+#     f_target: torch.Tensor,
+#     n_nodes: torch.Tensor,
+# ) -> torch.Tensor:
+#     # Flow loss, per-point.
+#     raw_se = ((f_pred - f_target) ** 2).sum(dim=-1)
+
+#     weights = (1 / n_nodes).repeat_interleave(n_nodes)
+#     l_se = (raw_se * weights[:, None]).sum() / f_pred.shape[1]  # Trajectory length
+
+#     # Full loss, aberaged across the batch.
+#     loss: torch.Tensor = l_se / len(n_nodes)
+
+#     return loss
+
+
+def make_trajectory_animation(traj_data):  # Make trajectory animation
+    animation = FlowNetAnimation()
+    for i in range(traj_data["point"].shape[-2]):
+        pcd = (
+            traj_data["pos"].detach().numpy()
+            if i == 0
+            else traj_data["point"][:, (i - 1)].detach().numpy()
+        )
+        mask = traj_data["mask"].detach().numpy()
+        flow = traj_data["delta"][:, i].detach().numpy()
+        animation.add_trace(
+            torch.as_tensor(pcd),
+            torch.as_tensor([pcd]),
+            torch.as_tensor([flow]),
+            "red",
+        )
+
+    return animation.animate()
+
+
+# Flow predictor
+class FlowTrajectoryTrainingModule(L.LightningModule):
+    def __init__(self, network, training_cfg, model_cfg=None) -> None:
+        super().__init__()
+        self.network = network
+        self.lr = training_cfg.lr
+        self.batch_size = training_cfg.batch_size
+        self.mask_input_channel = training_cfg.mask_input_channel
+        self.mode = training_cfg.mode
+        self.trajectory_len = training_cfg.trajectory_len
+        assert self.mode in ["delta", "point"]
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        if self.mask_input_channel:
+            data.x = data.mask.reshape(len(data.mask), 1)
+
+        # Run the model.
+        flow = typing.cast(torch.Tensor, self.network(data))
+
+        return flow
+
+    def _step(self, batch: tgd.Batch, mode):
+        # Make a prediction.
+        f_pred = self(batch)
+        f_pred = f_pred.reshape(f_pred.shape[0], -1, 3)  # batch * traj_len * 3
+
+        # Compute the loss.
+        n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(self.device)  # type: ignore
+        f_ix = batch.mask.bool()
+        if self.mode == "delta":
+            f_target = batch.delta
+        elif self.mode == "point":
+            f_target = batch.point
+
+        f_target = f_target.float()
+        loss = artflownet_loss(f_pred, f_target, n_nodes)
+
+        # Compute some metrics on flow-only regions.
+        rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
+
+        if mode == "train_train":
+            self.log_dict(
+                {
+                    f"train/loss": loss,
+                },
+                add_dataloader_idx=False,
+                batch_size=len(batch),
+            )
+        else:
+            self.log_dict(
+                {
+                    f"{mode}/flow_loss": loss,
+                    f"{mode}/rmse": rmse,
+                    f"{mode}/cosine_similarity": cos_dist,
+                    f"{mode}/mag_error": mag_error,
+                },
+                add_dataloader_idx=False,
+                batch_size=len(batch),
+            )
+        return f_pred, loss
+
+    def configure_optimizers(self):
+        optimizer = optim.AdamW(self.parameters(), lr=self.lr)
+        lr_scheduler = optim.lr_scheduler.MultiStepLR(
+            optimizer, milestones=[100, 150], gamma=0.1
+        )
+        return [optimizer], [lr_scheduler]
+
+    def training_step(self, batch: tgd.Batch, batch_id):  # type: ignore
+        self.train()
+        _, loss = self._step(batch, "train_train")
+        return loss
+
+    def validation_step(self, batch: tgd.Batch, batch_id, dataloader_idx=0):  # type: ignore
+        self.eval()
+        dataloader_names = ["val", "train", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        f_pred, loss = self._step(batch, name)
+        return {"preds": f_pred, "loss": loss, "cosine_cache": None}
+
+    @staticmethod
+    def make_plots(preds, batch: tgd.Batch, cosine_cache=None) -> Dict[str, go.Figure]:
+        obj_id = batch.id
+        pos = (
+            batch.point[:, -2, :].numpy() if batch.point.shape[1] >= 2 else batch.pos
+        )  # The last step's beinning pos
+        mask = batch.mask.numpy()
+        f_target = batch.delta[:, -1, :]
+        f_pred = preds.reshape(preds.shape[0], -1, 3)[:, -1, :]
+
+        fig = make_subplots(
+            rows=2,
+            cols=2,
+            specs=[
+                [{"type": "scene", "colspan": 2}, None],
+                [{"type": "scene"}, {"type": "scene"}],
+            ],
+            subplot_titles=(
+                "input data",
+                "target flow",
+                "pred flow",
+            ),
+            vertical_spacing=0.05,
+        )
+
+        # Parent/child plot.
+        labelmap = {0: "unselected", 1: "part"}
+        labels = torch.zeros(len(pos)).int()
+        labels[mask == 1.0] = 1
+        fig.add_traces(v3p._segmentation_traces(pos, labels, labelmap, "scene1"))
+
+        fig.update_layout(
+            scene1=v3p._3d_scene(pos),
+            showlegend=True,
+            margin=dict(l=0, r=0, b=0, t=40),
+            legend=dict(x=1.0, y=0.75),
+        )
+
+        # normalize the flow for visualization.
+        n_f_gt = (f_target / f_target.norm(dim=1).max()).numpy()
+        n_f_pred = (f_pred / f_target.norm(dim=1).max()).numpy()
+
+        # GT flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene2", name="pts"), row=2, col=1)
+        f_gt_traces = v3p._flow_traces(
+            pos, n_f_gt, scene="scene2", name="f_gt", legendgroup="1"
+        )
+        fig.add_traces(f_gt_traces, rows=2, cols=1)
+        fig.update_layout(scene2=v3p._3d_scene(pos))
+
+        # Predicted flow.
+        fig.add_trace(v3p.pointcloud(pos, 1, scene="scene3", name="pts"), row=2, col=2)
+        f_pred_traces = v3p._flow_traces(
+            pos, n_f_pred, scene="scene3", name="f_pred", legendgroup="2"
+        )
+        fig.add_traces(f_pred_traces, rows=2, cols=2)
+        fig.update_layout(scene3=v3p._3d_scene(pos))
+
+        fig.update_layout(title=f"Object {obj_id}")
+
+        return {"artflownet_plot": fig}
+
+
+# Implement this for trajectory eval
+class FlowTrajectoryInferenceModule(L.LightningModule):
+    def __init__(self, network, inference_cfg, model_cfg=None) -> None:
+        super().__init__()
+        self.network = network
+        self.mask_input_channel = inference_cfg.mask_input_channel
+        self.trajectory_len = inference_cfg.trajectory_len
+        # self.mpc_step = inference_config.mpc_step
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        if self.mask_input_channel:
+            data.x = data.mask.reshape(len(data.mask), 1)
+
+        # Run the model.
+        trajectory = typing.cast(torch.Tensor, self.network(data))
+
+        return trajectory
+
+    def load_from_ckpt(self, ckpt_file):
+        ckpt = torch.load(ckpt_file)
+        self.load_state_dict(ckpt["state_dict"])
+
+    def predict(self, P_world) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float(),
+            mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        batch = batch.to(self.device)
+        if self.mask_input_channel:
+            batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            trajectory = self.network(batch)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.reshape(trajectory.shape[0], -1, 3).cpu()
+
+    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:  # type: ignore
+        return self.forward(batch)
+
+    # # the predict step input is different now, pay attention
+    # def predict(self, xyz: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
+    #     """Predict the flow for a single object. The point cloud should
+    #     come straight from the maniskill processed observation function.
+
+    #     Args:
+    #         xyz (torch.Tensor): Nx3 pointcloud
+    #         mask (torch.Tensor): Nx1 mask of the part that will move.
+
+    #     Returns:
+    #         torch.Tensor: Nx3 dense flow prediction
+    #     """
+    #     print(xyz, mask)
+    #     assert len(xyz) == len(mask)
+    #     assert len(xyz.shape) == 2
+    #     assert len(mask.shape) == 1
+
+    #     data = tgd.Data(pos=xyz, mask=mask)
+    #     batch = tgd.Batch.from_data_list([data])
+    #     batch = batch.to(self.device)
+    #     self.eval()
+    #     with torch.no_grad():
+    #         trajectory = self.forward(batch)
+    #     return trajectory.reshape(trajectory.shape[0], -1, 3)  # batch * traj_len * 3
+
+
+class FlowSimulationInferenceModule(L.LightningModule):
+    def __init__(self, network, mask_input_channel) -> None:
+        super().__init__()
+        self.network = network
+        self.mask_input_channel = mask_input_channel
+
+    def __init__(
+        self, network, inference_cfg=None, model_cfg=None, mask_input_channel=None
+    ) -> None:
+        super().__init__()
+        self.network = network
+        if inference_cfg is not None:
+            self.mask_input_channel = inference_cfg.mask_input_channel
+        else:
+            self.mask_input_channel = mask_input_channel
+
+    def load_from_ckpt(self, ckpt_file):
+        ckpt = torch.load(ckpt_file)
+        self.load_state_dict(ckpt["state_dict"])
+
+    def forward(self, data) -> torch.Tensor:  # type: ignore
+        # Maybe add the mask as an input to the network.
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
+
+        data = tgd.Data(
+            pos=torch.from_numpy(P_world).float(),
+            mask=torch.ones(P_world.shape[0]).float(),
+        )
+        batch = tgd.Batch.from_data_list([data])
+        batch = batch.to(self.device)
+        if self.mask_input_channel:
+            batch.x = batch.mask.reshape(len(batch.mask), 1)
+        self.eval()
+        with torch.no_grad():
+            trajectory = self.network(batch)
+        # print("Trajectory prediction shape:", trajectory.shape)
+        return trajectory.reshape(trajectory.shape[0], -1, 3).cpu()
diff --git a/src/flowbothd/models/modules/dgcnn.py b/src/flowbothd/models/modules/dgcnn.py
new file mode 100644
index 0000000..998da3c
--- /dev/null
+++ b/src/flowbothd/models/modules/dgcnn.py
@@ -0,0 +1,232 @@
+# Taken from https://github.com/WangYueFt/dcp/blob/master/model.py
+# Provides the baseline architectures for the DCP model.
+# Only changes:
+# - Change `from util import quat2mat` to `from .util import quat2mat`.
+# - Add this comment.
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+
+
+from dataclasses import dataclass
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from diffusers.configuration_utils import ConfigMixin, register_to_config
+from diffusers.models.embeddings import TimestepEmbedding, Timesteps
+from diffusers.models.modeling_utils import ModelMixin
+from diffusers.utils import BaseOutput
+
+# from .util import quat2mat
+
+
+def knn(x, k):
+    inner = -2 * torch.matmul(x.transpose(2, 1).contiguous(), x)
+    xx = torch.sum(x**2, dim=1, keepdim=True)
+    pairwise_distance = -xx - inner - xx.transpose(2, 1).contiguous()
+
+    idx = pairwise_distance.topk(k=k, dim=-1)[1]  # (batch_size, num_points, k)
+    return idx
+
+
+def get_graph_feature(x, k=20):
+    # x = x.squeeze()
+    idx = knn(x, k=k)  # (batch_size, num_points, k)
+    batch_size, num_points, _ = idx.size()
+    device = torch.device("cuda")
+
+    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points
+
+    idx = idx + idx_base
+
+    idx = idx.view(-1)
+
+    _, num_dims, _ = x.size()
+
+    x = x.transpose(
+        2, 1
+    ).contiguous()  # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)
+    feature = x.view(batch_size * num_points, -1)[idx, :]
+    feature = feature.view(batch_size, num_points, k, num_dims)
+    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)
+
+    feature = torch.cat((feature, x), dim=3).permute(0, 3, 1, 2)
+    # feature = torch.cat((feature - x, x), dim=3).permute(0, 3, 1, 2)
+
+    return feature
+
+
+@dataclass
+class DGCNNOutput(BaseOutput):
+    """
+    The output of [`DGCNN`].
+
+    Args:
+        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
+            The hidden states output from the last layer of the model.
+    """
+
+    sample: torch.FloatTensor
+
+
+# class DGCNN(nn.Module):
+class DGCNN(ModelMixin, ConfigMixin):
+    @register_to_config
+    def __init__(self, in_channels, sample_size, time_embed_dim, emb_dims=512):
+        super(DGCNN, self).__init__()
+        self.in_channels = in_channels
+        self.sample_size = sample_size
+        self.time_embed_dim = time_embed_dim
+
+        # positional time embeddings
+        flip_sin_to_cos = True
+        freq_shift = 0
+        self.time_proj = Timesteps(64, flip_sin_to_cos, freq_shift)
+        timestep_input_dim = 64
+        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)
+
+        # out_channels = [64, 256, 512, 1024]
+        out_channels = [256, 512, 512, 1024]
+
+        # goal embedding
+        goal_embed_dim = 64
+        goal_channels = [64, 128, 128, goal_embed_dim]
+        self.goal_sample_size = sample_size
+        self.goal_conv1 = nn.Conv2d(3 * 2, goal_channels[0], kernel_size=1, bias=False)
+        self.goal_conv2 = nn.Conv2d(
+            goal_channels[0], goal_channels[1], kernel_size=1, bias=False
+        )
+        self.goal_conv3 = nn.Conv2d(
+            goal_channels[1], goal_channels[2], kernel_size=1, bias=False
+        )
+        self.goal_conv4 = nn.Conv2d(
+            sum(goal_channels[:-1]), goal_channels[3], kernel_size=1, bias=False
+        )
+
+        # network components
+        # self.conv1 = nn.Conv2d((in_channels+time_embed_dim)*2, out_channels[0], kernel_size=1, bias=False)
+        self.conv1 = nn.Conv2d(
+            (in_channels + goal_embed_dim + time_embed_dim) * 2,
+            out_channels[0],
+            kernel_size=1,
+            bias=False,
+        )
+        # self.conv1 = nn.Conv2d(in_channels*2+time_embed_dim, out_channels[0], kernel_size=1, bias=False)
+        # self.conv1 = nn.Conv2d(in_channels*2, 64, kernel_size=1, bias=False)
+        # self.conv2 = nn.Conv2d(64, 64, kernel_size=1, bias=False)
+        self.conv2 = nn.Conv2d(
+            out_channels[0], out_channels[1], kernel_size=1, bias=False
+        )
+        # self.conv2 = nn.Conv2d(out_channels[0]+time_embed_dim, out_channels[1], kernel_size=1, bias=False)
+        # self.conv3 = nn.Conv2d(64, 128, kernel_size=1, bias=False)
+        # self.conv3 = nn.Conv2d(128, 256, kernel_size=1, bias=False)
+        self.conv3 = nn.Conv2d(
+            out_channels[1], out_channels[2], kernel_size=1, bias=False
+        )
+        # self.conv4 = nn.Conv2d(128, 256, kernel_size=1, bias=False)
+        # self.conv4 = nn.Conv2d(256, 512, kernel_size=1, bias=False)
+        self.conv4 = nn.Conv2d(
+            out_channels[2], out_channels[3], kernel_size=1, bias=False
+        )
+        # self.conv5 = nn.Conv2d(512, emb_dims, kernel_size=1, bias=False)
+        # self.conv5 = nn.Conv2d(960, emb_dims, kernel_size=1, bias=False)
+        self.conv5 = nn.Conv2d(sum(out_channels), emb_dims, kernel_size=1, bias=False)
+        self.bn1 = nn.Identity()  # nn.BatchNorm2d(64)
+        self.bn2 = nn.Identity()  # nn.BatchNorm2d(64)
+        self.bn3 = nn.Identity()  # nn.BatchNorm2d(128)
+        self.bn4 = nn.Identity()  # nn.BatchNorm2d(256)
+        self.bn5 = nn.Identity()  # nn.BatchNorm2d(emb_dims)
+
+    def forward(
+        self,
+        x,
+        timestep,
+        context,
+        return_dict: bool = True,
+    ):
+        """
+        Args:
+            x:  Point clouds and flows at some timestep t, (B, 3+3, N).
+            timestep:     Time. (B, ).
+            context: None.
+        """
+
+        # time embedding
+        timesteps = timestep
+        if not torch.is_tensor(timesteps):
+            timesteps = torch.tensor([timesteps], dtype=torch.long, device=x.device)
+        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:
+            timesteps = timesteps[None].to(x.device)
+
+        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML
+        timesteps = timesteps * torch.ones(
+            x.shape[0], dtype=timesteps.dtype, device=timesteps.device
+        )
+
+        t_emb = self.time_proj(timesteps)
+
+        # timesteps does not contain any weights and will always return f32 tensors
+        # but time_embedding might actually be running in fp16. so we need to cast here.
+        # there might be better ways to encapsulate this.
+        t_emb = t_emb.to(dtype=self.dtype)
+        emb = self.time_embedding(t_emb)
+
+        # goal embedding
+        batch_size, num_dims, num_points = x.size()
+        goal_pcd = context
+        goal_x = get_graph_feature(goal_pcd)
+        goal_x = F.relu(self.goal_conv1(goal_x))
+        goal_x1 = goal_x.max(dim=-1, keepdim=True)[0]
+
+        goal_x = F.relu(self.goal_conv2(goal_x))
+        goal_x2 = goal_x.max(dim=-1, keepdim=True)[0]
+
+        goal_x = F.relu(self.goal_conv3(goal_x))
+        goal_x3 = goal_x.max(dim=-1, keepdim=True)[0]
+
+        goal_x = torch.cat((goal_x1, goal_x2, goal_x3), dim=1)
+        goal_emb = (self.goal_conv4(goal_x)).view(batch_size, -1, self.goal_sample_size)
+
+        # concatenate embeddings
+        emb = emb.view(batch_size, -1, 1).repeat(1, 1, num_points)
+        x = torch.cat((x, goal_emb, emb), dim=1)  # (B, d+64+64, N)
+
+        x = get_graph_feature(x)  # (B, (d+64+64)*2, N, k)
+        # emb = emb.view(batch_size, -1, 1, 1).repeat(1, 1, num_points, 20)
+        # x = torch.cat((x, emb), dim=1) # B, d*2 + 64, N, k)
+        # x = get_graph_feature(x) # (B, d*2+64, N, k)
+        x = F.relu(self.bn1(self.conv1(x)))
+        x1 = x.max(dim=-1, keepdim=True)[0]
+
+        # emb = emb.view(batch_size, -1, num_points, 1).repeat(1, 1, 1, 20)
+        # x = torch.cat([x, emb], dim=1)
+        x = F.relu(self.bn2(self.conv2(x)))
+        x2 = x.max(dim=-1, keepdim=True)[0]
+
+        x = F.relu(self.bn3(self.conv3(x)))
+        x3 = x.max(dim=-1, keepdim=True)[0]
+
+        x = F.relu(self.bn4(self.conv4(x)))
+        x4 = x.max(dim=-1, keepdim=True)[0]
+
+        x = torch.cat((x1, x2, x3, x4), dim=1)
+
+        # x = F.relu(self.bn5(self.conv5(x))).view(batch_size, -1, num_points)
+        x = self.bn5(self.conv5(x)).view(batch_size, -1, num_points)
+
+        if not return_dict:
+            return (x,)
+
+        return DGCNNOutput(sample=x)
+
+
+if __name__ == "__main__":
+    model = DGCNN(in_channels=6, sample_size=1200, time_embed_dim=64, emb_dims=3).cuda()
+
+    noise = torch.randn(1, 3, 1200).cuda()
+    pcd = torch.randn(1, 3, 1200).cuda()
+    x = torch.concat((noise, pcd), dim=1).cuda()
+    t = torch.randint(100, size=(1,)).cuda()
+    context = pcd
+    output = model(x, t, context)
+    breakpoint()
diff --git a/src/flowbothd/models/modules/dit_models.py b/src/flowbothd/models/modules/dit_models.py
new file mode 100644
index 0000000..c3b3ac7
--- /dev/null
+++ b/src/flowbothd/models/modules/dit_models.py
@@ -0,0 +1,1169 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+# --------------------------------------------------------
+# References:
+# GLIDE: https://github.com/openai/glide-text2im
+# MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py
+# --------------------------------------------------------
+
+import math
+
+import numpy as np
+import rpad.pyg.nets.pointnet2 as pnp_original
+import torch
+import torch.nn as nn
+from timm.models.vision_transformer import Attention, Mlp
+
+import flowbothd.models.modules.pn2 as pnp
+from flowbothd.models.modules.dgcnn import DGCNN
+
+
+def modulate(x, shift, scale):
+    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)
+
+
+#################################################################################
+#               Embedding Layers for Timesteps and Class Labels                 #
+#################################################################################
+
+
+class TimestepEmbedder(nn.Module):
+    """
+    Embeds scalar timesteps into vector representations.
+    """
+
+    def __init__(self, hidden_size, frequency_embedding_size=256):
+        super().__init__()
+        self.mlp = nn.Sequential(
+            nn.Linear(frequency_embedding_size, hidden_size, bias=True),
+            nn.SiLU(),
+            nn.Linear(hidden_size, hidden_size, bias=True),
+        )
+        self.frequency_embedding_size = frequency_embedding_size
+
+    @staticmethod
+    def timestep_embedding(t, dim, max_period=10000):
+        """
+        Create sinusoidal timestep embeddings.
+        :param t: a 1-D Tensor of N indices, one per batch element.
+                          These may be fractional.
+        :param dim: the dimension of the output.
+        :param max_period: controls the minimum frequency of the embeddings.
+        :return: an (N, D) Tensor of positional embeddings.
+        """
+        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py
+        half = dim // 2
+        freqs = torch.exp(
+            -math.log(max_period)
+            * torch.arange(start=0, end=half, dtype=torch.float32)
+            / half
+        ).to(device=t.device)
+        args = t[:, None].float() * freqs[None]
+        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
+        if dim % 2:
+            embedding = torch.cat(
+                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1
+            )
+        return embedding
+
+    def forward(self, t):
+        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)
+        t_emb = self.mlp(t_freq)
+        return t_emb
+
+
+class LabelEmbedder(nn.Module):
+    """
+    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.
+    """
+
+    def __init__(self, num_classes, hidden_size, dropout_prob):
+        super().__init__()
+        use_cfg_embedding = dropout_prob > 0
+        self.embedding_table = nn.Embedding(
+            num_classes + use_cfg_embedding, hidden_size
+        )
+        self.num_classes = num_classes
+        self.dropout_prob = dropout_prob
+
+    def token_drop(self, labels, force_drop_ids=None):
+        """
+        Drops labels to enable classifier-free guidance.
+        """
+        if force_drop_ids is None:
+            drop_ids = (
+                torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob
+            )
+        else:
+            drop_ids = force_drop_ids == 1
+        labels = torch.where(drop_ids, self.num_classes, labels)
+        return labels
+
+    def forward(self, labels, train, force_drop_ids=None):
+        use_dropout = self.dropout_prob > 0
+        if (train and use_dropout) or (force_drop_ids is not None):
+            labels = self.token_drop(labels, force_drop_ids)
+        embeddings = self.embedding_table(labels)
+        return embeddings
+
+
+class RotaryPositionEncoding(nn.Module):
+    def __init__(self, feature_dim, pe_type="Rotary1D"):
+        super().__init__()
+
+        self.feature_dim = feature_dim
+        self.pe_type = pe_type
+
+    @staticmethod
+    def embed_rotary(x, cos, sin):
+        x2 = (
+            torch.stack([-x[..., 1::2], x[..., ::2]], dim=-1).reshape_as(x).contiguous()
+        )
+        x = x * cos + x2 * sin
+        return x
+
+    def forward(self, x_position):
+        bsize, npoint = x_position.shape
+        div_term = torch.exp(
+            torch.arange(
+                0, self.feature_dim, 2, dtype=torch.float, device=x_position.device
+            )
+            * (-math.log(10000.0) / (self.feature_dim))
+        )
+        div_term = div_term.view(1, 1, -1)  # [1, 1, d]
+
+        sinx = torch.sin(x_position * div_term)  # [B, N, d]
+        cosx = torch.cos(x_position * div_term)
+
+        sin_pos, cos_pos = map(
+            lambda feat: torch.stack([feat, feat], dim=-1).view(bsize, npoint, -1),
+            [sinx, cosx],
+        )
+        position_code = torch.stack([cos_pos, sin_pos], dim=-1)
+
+        if position_code.requires_grad:
+            position_code = position_code.detach()
+
+        return position_code
+
+
+class RotaryPositionEncoding3D(RotaryPositionEncoding):
+    def __init__(self, feature_dim, pe_type="Rotary3D"):
+        super().__init__(feature_dim, pe_type)
+
+    @torch.no_grad()
+    def forward(self, XYZ):
+        """
+        @param XYZ: [B,N,3]
+        @return:
+        """
+        bsize, npoint, _ = XYZ.shape
+        x_position, y_position, z_position = XYZ[..., 0:1], XYZ[..., 1:2], XYZ[..., 2:3]
+        div_term = torch.exp(
+            torch.arange(
+                0, self.feature_dim // 3, 2, dtype=torch.float, device=XYZ.device
+            )
+            * (-math.log(10000.0) / (self.feature_dim // 3))
+        )
+        div_term = div_term.view(1, 1, -1)  # [1, 1, d//6]
+
+        sinx = torch.sin(x_position * div_term)  # [B, N, d//6]
+        cosx = torch.cos(x_position * div_term)
+        siny = torch.sin(y_position * div_term)
+        cosy = torch.cos(y_position * div_term)
+        sinz = torch.sin(z_position * div_term)
+        cosz = torch.cos(z_position * div_term)
+
+        sinx, cosx, siny, cosy, sinz, cosz = map(
+            lambda feat: torch.stack([feat, feat], -1).view(bsize, npoint, -1),
+            [sinx, cosx, siny, cosy, sinz, cosz],
+        )
+
+        position_code = torch.stack(
+            [
+                torch.cat([cosx, cosy, cosz], dim=-1),  # cos_pos
+                torch.cat([sinx, siny, sinz], dim=-1),  # sin_pos
+            ],
+            dim=-1,
+        )
+
+        if position_code.requires_grad:
+            position_code = position_code.detach()
+
+        return position_code
+
+
+#################################################################################
+#                                 Core DiT Model                                #
+#################################################################################
+
+
+class DiTBlock(nn.Module):
+    """
+    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.
+    """
+
+    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):
+        super().__init__()
+        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
+        self.attn = Attention(
+            hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs
+        )
+        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
+        mlp_hidden_dim = int(hidden_size * mlp_ratio)
+        approx_gelu = lambda: nn.GELU(approximate="tanh")
+        self.mlp = Mlp(
+            in_features=hidden_size,
+            hidden_features=mlp_hidden_dim,
+            act_layer=approx_gelu,
+            drop=0,
+        )
+        self.adaLN_modulation = nn.Sequential(
+            nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size, bias=True)
+        )
+
+    def forward(self, x, c):
+        (
+            shift_msa,
+            scale_msa,
+            gate_msa,
+            shift_mlp,
+            scale_mlp,
+            gate_mlp,
+        ) = self.adaLN_modulation(c).chunk(6, dim=1)
+        x = x + gate_msa.unsqueeze(1) * self.attn(
+            modulate(self.norm1(x), shift_msa, scale_msa)
+        )
+        x = x + gate_mlp.unsqueeze(1) * self.mlp(
+            modulate(self.norm2(x), shift_mlp, scale_mlp)
+        )
+        return x
+
+
+class FinalLayer(nn.Module):
+    """
+    The final layer of DiT.
+    """
+
+    def __init__(self, hidden_size, patch_size, out_channels):
+        super().__init__()
+        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
+        self.linear = nn.Linear(
+            hidden_size, patch_size * patch_size * out_channels, bias=True
+        )
+        self.adaLN_modulation = nn.Sequential(
+            nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True)
+        )
+
+    def forward(self, x, c):
+        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)
+        # print("shift: ", shift)
+        # print("scale: ", scale)
+        # print("After norm:", self.norm_final(x)[0, 0, :])
+        x = modulate(self.norm_final(x), shift, scale)
+        x = self.linear(x)
+        return x
+
+
+class PN2DiT(nn.Module):
+    """
+    Diffusion model with a Transformer backbone.
+    """
+
+    def __init__(
+        self,
+        input_size=[30, 40],
+        patch_size=1,
+        in_channels=3,
+        hidden_size=1152,
+        depth=28,
+        num_heads=16,
+        mlp_ratio=4.0,
+        class_dropout_prob=0.1,
+        num_classes=1000,
+        pos_embed_freq_L=10,
+        n_points=1200,
+        time_embed_dim=64,
+        learn_sigma=True,
+    ):
+        super().__init__()
+        self.learn_sigma = learn_sigma
+        self.in_channels = in_channels
+        self.out_channels = in_channels * 2 if learn_sigma else in_channels
+        self.patch_size = patch_size
+        self.num_heads = num_heads
+        self.n_points = n_points
+
+        self.h, self.w = input_size[0], input_size[1]
+
+        # # 0) Pure input
+        # self.x_embedder = PatchEmbed(input_size, patch_size, in_channels + pos_embed_freq_L * 6, hidden_size, bias=True)
+        # # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):
+        # w = self.x_embedder.proj.weight.data
+        # nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
+        # nn.init.constant_(self.x_embedder.proj.bias, 0)
+
+        # 1) Point Cloud features
+        self.x_embedder = pnp_original.PN2Dense(
+            in_channels=3,
+            out_channels=hidden_size,
+            p=pnp_original.PN2DenseParams(),
+        )
+
+        # self.x_embedder = PatchEmbed(input_size, patch_size, in_channels + 3, hidden_size, bias=True)
+        self.t_embedder = TimestepEmbedder(hidden_size)
+        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)
+        # num_patches = self.x_embedder.num_patches
+        num_patches = self.n_points
+        # Will use fixed sin-cos embedding:
+        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)
+
+        self.blocks = nn.ModuleList(
+            [
+                DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio)
+                for _ in range(depth)
+            ]
+        )
+        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)
+        self.initialize_weights()
+        self.pos_embed_freq_L = pos_embed_freq_L
+
+    def initialize_weights(self):
+        # Initialize transformer layers:
+        def _basic_init(module):
+            if isinstance(module, nn.Linear):
+                torch.nn.init.xavier_uniform_(module.weight)
+                if module.bias is not None:
+                    nn.init.constant_(module.bias, 0)
+
+        self.apply(_basic_init)
+
+        # # Initialize (and freeze) pos_embed by sin-cos embedding:
+        # pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))
+        # self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))
+
+        # Initialize label embedding table:
+        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)
+
+        # Initialize timestep embedding MLP:
+        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
+        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
+
+        # Zero-out adaLN modulation layers in DiT blocks:
+        for block in self.blocks:
+            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)
+            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)
+
+        # Zero-out output layers:
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)
+        nn.init.constant_(self.final_layer.linear.weight, 0)
+        nn.init.constant_(self.final_layer.linear.bias, 0)
+
+    def unpatchify(self, x):
+        """
+        x: (N, T, patch_size**2 * C)
+        imgs: (N, H, W, C)
+        """
+        c = self.out_channels
+        p = self.patch_size
+        # h = w = int(x.shape[1] ** 0.5)
+        h, w = self.h, self.w
+        assert h * w == x.shape[1]
+
+        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))
+        x = torch.einsum("nhwpqc->nchpwq", x)
+        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))
+        return imgs
+
+    def pcd_positional_encoding(self, xyz):
+        """
+        Apply sinusoidal positional encoding to the given 3D coordinates.
+
+        :param xyz: A numpy array of shape (N, 3), where N is the number of points, and each point has x, y, z coordinates.
+        :return: A numpy array of shape (N, 6*L) containing the positional embeddings.
+        """
+
+        L = self.pos_embed_freq_L
+
+        # Initialize an array to hold the positional encodings
+        embeddings = np.zeros((xyz.shape[0], 6 * L))
+
+        # Frequencies: 2^0, 2^1, ..., 2^(L-1)
+        frequencies = 2 ** np.arange(L)
+
+        # Apply sinusoidal encoding
+        for i, freq in enumerate(frequencies):
+            for j, func in enumerate([np.sin, np.cos]):
+                embeddings[:, (6 * i + 2 * j) : (6 * i + 2 * j + 3)] = func(
+                    2 * np.pi * xyz.detach().cpu().numpy() * freq
+                )
+
+        return torch.from_numpy(embeddings).float().cuda()
+
+    def forward(self, x, t, pos, context):
+        """
+        Forward pass of DiT.
+        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)
+        t: (N,) tensor of diffusion timesteps
+        pos: (N, H*W, C)
+        """
+        # # 0) Takes original point cloud
+        # pos_embed = self.pcd_positional_encoding(torch.flatten(pos, start_dim=0, end_dim=1))  # N*T * D
+        # x = torch.flatten(x, start_dim=2, end_dim=3).permute(0, 2, 1)  # (N, H*W, C)
+        # x = torch.concat((x, pos_embed.reshape(x.shape[0], x.shape[1], -1)), dim=-1)
+        # x = self.x_embedder(x.reshape(x.shape[0], -1, 30, 40))
+        # 1) Take pointnet++ encoded point cloud
+        context.x = (
+            torch.flatten(x, start_dim=2, end_dim=3).permute(0, 2, 1).reshape(-1, 3)
+        )
+        encoded_pcd = self.x_embedder(context.cuda())
+        x = encoded_pcd.reshape(x.shape[0], 1200, -1)
+
+        # # 2) Take DGCNN encoded point cloud
+        # # print(torch.flatten(x, start_dim=2, end_dim=3).shape, pos.permute(0, 2, 1).shape)
+        # x = torch.cat(
+        #     (torch.flatten(x, start_dim=2, end_dim=3), pos.permute(0, 2, 1)), dim=1
+        # )
+        # encoded_pcd = self.x_embedder(x, t, pos.permute(0, 2, 1)).sample
+        # x = encoded_pcd.permute(0, 2, 1)
+
+        t = self.t_embedder(t)  # (N, D)
+
+        # print("x", x.shape, "t", t.shape)
+
+        # y = self.y_embedder(y, self.training)    # (N, D)
+        c = t
+        # print("c", c.shape)                              # (N, D)
+        for block in self.blocks:
+            x = block(x, c)  # (N, T, D)
+        x = self.final_layer(x, c)  # (N, T, patch_size ** 2 * out_channels)
+        # print("after final layer:", x.shape)
+        x = self.unpatchify(x)  # (N, out_channels, H, W)
+        return x
+
+    def forward_with_cfg(self, x, t, cfg_scale, pos, context):
+        """
+        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.
+        """
+        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb
+        # half = x[: len(x) // 2]
+        model_out = self.forward(x, t, pos, context)
+        # For exact reproducibility reasons, we apply classifier-free guidance on only
+        # three channels by default. The standard approach to cfg applies it to all channels.
+        # This can be done by uncommenting the following line and commenting-out the line following that.
+        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]
+        eps, rest = model_out[:, : self.in_channels], model_out[:, self.in_channels :]
+        return torch.cat([eps, rest], dim=1)
+
+
+class PN2HisDiT(nn.Module):  # With history latent everywhere version
+    """
+    Diffusion model with a Transformer backbone.
+    """
+
+    def __init__(
+        self,
+        history_embed_dim=128,
+        input_size=[30, 40],
+        patch_size=1,
+        in_channels=3,
+        hidden_size=1152,
+        depth=28,
+        num_heads=16,
+        mlp_ratio=4.0,
+        class_dropout_prob=0.1,
+        num_classes=1000,
+        pos_embed_freq_L=10,
+        n_points=1200,
+        time_embed_dim=64,
+        learn_sigma=True,
+    ):
+        super().__init__()
+        self.learn_sigma = learn_sigma
+        self.in_channels = in_channels
+        self.out_channels = in_channels * 2 if learn_sigma else in_channels
+        self.patch_size = patch_size
+        self.num_heads = num_heads
+        self.n_points = n_points
+
+        self.h, self.w = input_size[0], input_size[1]
+
+        # # 0) Pure input
+        # self.x_embedder = PatchEmbed(input_size, patch_size, in_channels + pos_embed_freq_L * 6, hidden_size, bias=True)
+        # # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):
+        # w = self.x_embedder.proj.weight.data
+        # nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
+        # nn.init.constant_(self.x_embedder.proj.bias, 0)
+
+        # 1) Point Cloud features
+        self.x_embedder = pnp.PN2DenseLatentEncodingEverywhere(
+            history_embed_dim=history_embed_dim,
+            in_channels=3,
+            out_channels=hidden_size,
+        )
+
+        # self.x_embedder = PatchEmbed(input_size, patch_size, in_channels + 3, hidden_size, bias=True)
+        self.t_embedder = TimestepEmbedder(hidden_size)
+        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)
+        # num_patches = self.x_embedder.num_patches
+        num_patches = self.n_points
+        # Will use fixed sin-cos embedding:
+        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)
+
+        self.blocks = nn.ModuleList(
+            [
+                DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio)
+                for _ in range(depth)
+            ]
+        )
+        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)
+        self.initialize_weights()
+        self.pos_embed_freq_L = pos_embed_freq_L
+
+    def initialize_weights(self):
+        # Initialize transformer layers:
+        def _basic_init(module):
+            if isinstance(module, nn.Linear):
+                torch.nn.init.xavier_uniform_(module.weight)
+                if module.bias is not None:
+                    nn.init.constant_(module.bias, 0)
+
+        self.apply(_basic_init)
+
+        # # Initialize (and freeze) pos_embed by sin-cos embedding:
+        # pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))
+        # self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))
+
+        # Initialize label embedding table:
+        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)
+
+        # Initialize timestep embedding MLP:
+        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
+        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
+
+        # Zero-out adaLN modulation layers in DiT blocks:
+        for block in self.blocks:
+            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)
+            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)
+
+        # Zero-out output layers:
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)
+        nn.init.constant_(self.final_layer.linear.weight, 0)
+        nn.init.constant_(self.final_layer.linear.bias, 0)
+
+    def unpatchify(self, x):
+        """
+        x: (N, T, patch_size**2 * C)
+        imgs: (N, H, W, C)
+        """
+        c = self.out_channels
+        p = self.patch_size
+        # h = w = int(x.shape[1] ** 0.5)
+        h, w = self.h, self.w
+        assert h * w == x.shape[1]
+
+        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))
+        x = torch.einsum("nhwpqc->nchpwq", x)
+        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))
+        return imgs
+
+    def pcd_positional_encoding(self, xyz):
+        """
+        Apply sinusoidal positional encoding to the given 3D coordinates.
+
+        :param xyz: A numpy array of shape (N, 3), where N is the number of points, and each point has x, y, z coordinates.
+        :return: A numpy array of shape (N, 6*L) containing the positional embeddings.
+        """
+
+        L = self.pos_embed_freq_L
+
+        # Initialize an array to hold the positional encodings
+        embeddings = np.zeros((xyz.shape[0], 6 * L))
+
+        # Frequencies: 2^0, 2^1, ..., 2^(L-1)
+        frequencies = 2 ** np.arange(L)
+
+        # Apply sinusoidal encoding
+        for i, freq in enumerate(frequencies):
+            for j, func in enumerate([np.sin, np.cos]):
+                embeddings[:, (6 * i + 2 * j) : (6 * i + 2 * j + 3)] = func(
+                    2 * np.pi * xyz.detach().cpu().numpy() * freq
+                )
+
+        return torch.from_numpy(embeddings).float().cuda()
+
+    def forward(self, x, t, pos, context):
+        """
+        Forward pass of DiT.
+        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)
+        t: (N,) tensor of diffusion timesteps
+        pos: (N, H*W, C)
+        """
+        # # 0) Takes original point cloud
+        # pos_embed = self.pcd_positional_encoding(torch.flatten(pos, start_dim=0, end_dim=1))  # N*T * D
+        # x = torch.flatten(x, start_dim=2, end_dim=3).permute(0, 2, 1)  # (N, H*W, C)
+        # x = torch.concat((x, pos_embed.reshape(x.shape[0], x.shape[1], -1)), dim=-1)
+        # x = self.x_embedder(x.reshape(x.shape[0], -1, 30, 40))
+        # 1) Take pointnet++ encoded point cloud
+        context.x = (
+            torch.flatten(x, start_dim=2, end_dim=3).permute(0, 2, 1).reshape(-1, 3)
+        )
+        encoded_pcd = self.x_embedder(context.cuda(), latents=context.history_embed)
+        x = encoded_pcd.reshape(x.shape[0], 1200, -1)
+
+        # # 2) Take DGCNN encoded point cloud
+        # # print(torch.flatten(x, start_dim=2, end_dim=3).shape, pos.permute(0, 2, 1).shape)
+        # x = torch.cat(
+        #     (torch.flatten(x, start_dim=2, end_dim=3), pos.permute(0, 2, 1)), dim=1
+        # )
+        # encoded_pcd = self.x_embedder(x, t, pos.permute(0, 2, 1)).sample
+        # x = encoded_pcd.permute(0, 2, 1)
+
+        t = self.t_embedder(t)  # (N, D)
+
+        # print("x", x.shape, "t", t.shape)
+
+        # y = self.y_embedder(y, self.training)    # (N, D)
+        c = t
+        # print("c", c.shape)                              # (N, D)
+        for block in self.blocks:
+            x = block(x, c)  # (N, T, D)
+        x = self.final_layer(x, c)  # (N, T, patch_size ** 2 * out_channels)
+        # print("after final layer:", x.shape)
+        x = self.unpatchify(x)  # (N, out_channels, H, W)
+        return x
+
+    def forward_with_cfg(self, x, t, cfg_scale, pos, context):
+        """
+        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.
+        """
+        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb
+        # half = x[: len(x) // 2]
+        model_out = self.forward(x, t, pos, context)
+        # For exact reproducibility reasons, we apply classifier-free guidance on only
+        # three channels by default. The standard approach to cfg applies it to all channels.
+        # This can be done by uncommenting the following line and commenting-out the line following that.
+        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]
+        eps, rest = model_out[:, : self.in_channels], model_out[:, self.in_channels :]
+        return torch.cat([eps, rest], dim=1)
+
+
+class DGDiT(nn.Module):
+    """
+    Diffusion model with a Transformer backbone.
+    """
+
+    def __init__(
+        self,
+        input_size=[30, 40],
+        patch_size=1,
+        in_channels=3,
+        hidden_size=1152,
+        depth=28,
+        num_heads=16,
+        mlp_ratio=4.0,
+        class_dropout_prob=0.1,
+        num_classes=1000,
+        pos_embed_freq_L=10,
+        n_points=1200,
+        time_embed_dim=64,
+        learn_sigma=True,
+    ):
+        super().__init__()
+        self.learn_sigma = learn_sigma
+        self.in_channels = in_channels
+        self.out_channels = in_channels * 2 if learn_sigma else in_channels
+        self.patch_size = patch_size
+        self.num_heads = num_heads
+        self.n_points = n_points
+
+        self.h, self.w = input_size[0], input_size[1]
+
+        # # 0) Pure input
+        # self.x_embedder = PatchEmbed(input_size, patch_size, in_channels + pos_embed_freq_L * 6, hidden_size, bias=True)
+        # # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):
+        # w = self.x_embedder.proj.weight.data
+        # nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
+        # nn.init.constant_(self.x_embedder.proj.bias, 0)
+
+        # # 1) Point Cloud features
+        # self.x_embedder = pnp.PN2Dense(
+        #     in_channels=3,
+        #     out_channels=hidden_size,
+        #     p=pnp.PN2DenseParams(),
+        # )
+
+        # 2) DGCNN features
+        self.x_embedder = DGCNN(
+            in_channels=in_channels + 3,
+            sample_size=n_points,
+            time_embed_dim=time_embed_dim,
+            emb_dims=hidden_size,
+        )
+
+        # self.x_embedder = PatchEmbed(input_size, patch_size, in_channels + 3, hidden_size, bias=True)
+        self.t_embedder = TimestepEmbedder(hidden_size)
+        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)
+        # num_patches = self.x_embedder.num_patches
+        num_patches = self.n_points
+        # Will use fixed sin-cos embedding:
+        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)
+
+        self.blocks = nn.ModuleList(
+            [
+                DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio)
+                for _ in range(depth)
+            ]
+        )
+        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)
+        self.initialize_weights()
+        self.pos_embed_freq_L = pos_embed_freq_L
+
+    def initialize_weights(self):
+        # Initialize transformer layers:
+        def _basic_init(module):
+            if isinstance(module, nn.Linear):
+                torch.nn.init.xavier_uniform_(module.weight)
+                if module.bias is not None:
+                    nn.init.constant_(module.bias, 0)
+
+        self.apply(_basic_init)
+
+        # # Initialize (and freeze) pos_embed by sin-cos embedding:
+        # pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))
+        # self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))
+
+        # Initialize label embedding table:
+        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)
+
+        # Initialize timestep embedding MLP:
+        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
+        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
+
+        # Zero-out adaLN modulation layers in DiT blocks:
+        for block in self.blocks:
+            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)
+            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)
+
+        # Zero-out output layers:
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)
+        nn.init.constant_(self.final_layer.linear.weight, 0)
+        nn.init.constant_(self.final_layer.linear.bias, 0)
+
+    def unpatchify(self, x):
+        """
+        x: (N, T, patch_size**2 * C)
+        imgs: (N, H, W, C)
+        """
+        c = self.out_channels
+        p = self.patch_size
+        # h = w = int(x.shape[1] ** 0.5)
+        h, w = self.h, self.w
+        assert h * w == x.shape[1]
+
+        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))
+        x = torch.einsum("nhwpqc->nchpwq", x)
+        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))
+        return imgs
+
+    def pcd_positional_encoding(self, xyz):
+        """
+        Apply sinusoidal positional encoding to the given 3D coordinates.
+
+        :param xyz: A numpy array of shape (N, 3), where N is the number of points, and each point has x, y, z coordinates.
+        :return: A numpy array of shape (N, 6*L) containing the positional embeddings.
+        """
+
+        L = self.pos_embed_freq_L
+
+        # Initialize an array to hold the positional encodings
+        embeddings = np.zeros((xyz.shape[0], 6 * L))
+
+        # Frequencies: 2^0, 2^1, ..., 2^(L-1)
+        frequencies = 2 ** np.arange(L)
+
+        # Apply sinusoidal encoding
+        for i, freq in enumerate(frequencies):
+            for j, func in enumerate([np.sin, np.cos]):
+                embeddings[:, (6 * i + 2 * j) : (6 * i + 2 * j + 3)] = func(
+                    2 * np.pi * xyz.detach().cpu().numpy() * freq
+                )
+
+        return torch.from_numpy(embeddings).float().cuda()
+
+    def forward(self, x, t, pos, context):
+        """
+        Forward pass of DiT.
+        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)
+        t: (N,) tensor of diffusion timesteps
+        pos: (N, H*W, C)
+        """
+        # # 0) Takes original point cloud
+        # pos_embed = self.pcd_positional_encoding(torch.flatten(pos, start_dim=0, end_dim=1))  # N*T * D
+        # x = torch.flatten(x, start_dim=2, end_dim=3).permute(0, 2, 1)  # (N, H*W, C)
+        # x = torch.concat((x, pos_embed.reshape(x.shape[0], x.shape[1], -1)), dim=-1)
+        # x = self.x_embedder(x.reshape(x.shape[0], -1, 30, 40))
+
+        # # 1) Take pointnet++ encoded point cloud
+        # context.x = torch.flatten(x, start_dim=2, end_dim=3).permute(0, 2, 1).reshape(-1, 3)
+        # encoded_pcd = self.x_embedder(context)
+        # x = encoded_pcd.reshape(x.shape[0], 1200, -1)
+
+        # 2) Take DGCNN encoded point cloud
+        # print(torch.flatten(x, start_dim=2, end_dim=3).shape, pos.permute(0, 2, 1).shape)
+        x = torch.cat(
+            (torch.flatten(x, start_dim=2, end_dim=3), pos.permute(0, 2, 1)), dim=1
+        )
+        encoded_pcd = self.x_embedder(x, t, pos.permute(0, 2, 1)).sample
+        x = encoded_pcd.permute(0, 2, 1)
+
+        t = self.t_embedder(t)  # (N, D)
+
+        # print("x", x.shape, "t", t.shape)
+
+        # y = self.y_embedder(y, self.training)    # (N, D)
+        c = t
+        # print("c", c.shape)                              # (N, D)
+        for block in self.blocks:
+            x = block(x, c)  # (N, T, D)
+        x = self.final_layer(x, c)  # (N, T, patch_size ** 2 * out_channels)
+        # print("after final layer:", x.shape)
+        x = self.unpatchify(x)  # (N, out_channels, H, W)
+        return x
+
+    def forward_with_cfg(self, x, t, cfg_scale, pos, context):
+        """
+        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.
+        """
+        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb
+        # half = x[: len(x) // 2]
+        model_out = self.forward(x, t, pos, context)
+        # For exact reproducibility reasons, we apply classifier-free guidance on only
+        # three channels by default. The standard approach to cfg applies it to all channels.
+        # This can be done by uncommenting the following line and commenting-out the line following that.
+        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]
+        eps, rest = model_out[:, : self.in_channels], model_out[:, self.in_channels :]
+        return torch.cat([eps, rest], dim=1)
+
+
+class DiT(nn.Module):
+    """
+    Diffusion model with a Transformer backbone - point cloud, unconditional.
+    """
+
+    def __init__(
+        self,
+        in_channels=3,
+        hidden_size=1152,
+        depth=28,
+        num_heads=16,
+        mlp_ratio=4.0,
+        learn_sigma=True,
+    ):
+        super().__init__()
+        self.learn_sigma = learn_sigma
+        self.in_channels = in_channels
+        # self.out_channels = in_channels * 2 if learn_sigma else in_channels
+        self.out_channels = 6 if learn_sigma else 3
+        self.num_heads = num_heads
+        # x_embedder is conv1d layer instead of 2d patch embedder
+        self.x_embedder = nn.Conv1d(
+            in_channels, hidden_size, kernel_size=1, stride=1, padding=0, bias=True
+        )
+        # no pos_embed, or y_embedder
+        self.t_embedder = TimestepEmbedder(hidden_size)
+        self.blocks = nn.ModuleList(
+            [
+                DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio)
+                for _ in range(depth)
+            ]
+        )
+        # functionally setting patch size to 1 for a point cloud
+        self.final_layer = FinalLayer(hidden_size, 1, self.out_channels)
+        self.initialize_weights()
+
+    def initialize_weights(self):
+        # Initialize transformer layers:
+        def _basic_init(module):
+            if isinstance(module, nn.Linear):
+                torch.nn.init.xavier_uniform_(module.weight)
+                if module.bias is not None:
+                    nn.init.constant_(module.bias, 0)
+
+        self.apply(_basic_init)
+
+        # Initialize x_embed like nn.Linear (instead of nn.Conv2d):
+        w = self.x_embedder.weight.data
+        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
+        nn.init.constant_(self.x_embedder.bias, 0)
+
+        # Initialize timestep embedding MLP:
+        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
+        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
+
+        # Zero-out adaLN modulation layers in DiT blocks:
+        for block in self.blocks:
+            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)
+            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)
+
+        # Zero-out output layers:
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)
+        nn.init.constant_(self.final_layer.linear.weight, 0)
+        nn.init.constant_(self.final_layer.linear.bias, 0)
+
+    def forward(self, x, t, pos):
+        """
+        Forward pass of DiT.
+        x: (N, 3, L) tensor of noisy flows
+        t: (N,) tensor of diffusion timesteps
+        pos: (N, 3, L) tensor of 3D coordinates
+        """
+        # NOTE: the patchify/unpatchify layers handle the dimension swapping, so we need to manually do that here
+
+        # concat x and pos
+        # print(x.shape, pos.shape)
+        x = torch.cat((x, pos), dim=1)
+        x = torch.transpose(self.x_embedder(x), -1, -2)
+        t = self.t_embedder(t)  # (N, D)
+        c = t  # (N, D)
+        for block in self.blocks:
+            x = block(x, c)  # (N, L, D)
+        x = self.final_layer(x, c)  # (N, L, patch_size ** 2 * out_channels)
+        # transpose back to (N, out_channels, L)
+        x = torch.transpose(x, -1, -2)
+        return x
+
+
+class RoPEDiT(nn.Module):
+    """
+    Diffusion model with a Transformer backbone - point cloud, unconditional.
+    """
+
+    def __init__(
+        self,
+        in_channels=3,
+        hidden_size=1152,
+        depth=28,
+        num_heads=16,
+        mlp_ratio=4.0,
+        learn_sigma=True,
+    ):
+        super().__init__()
+        self.learn_sigma = learn_sigma
+        self.in_channels = in_channels
+        # self.out_channels = in_channels * 2 if learn_sigma else in_channels
+        self.out_channels = 6 if learn_sigma else 3
+        self.num_heads = num_heads
+        self.rope_embedder = RotaryPositionEncoding3D(hidden_size)
+        # x_embedder is conv1d layer instead of 2d patch embedder
+        self.x_embedder = nn.Conv1d(
+            in_channels, hidden_size, kernel_size=1, stride=1, padding=0, bias=True
+        )
+        # no pos_embed, or y_embedder
+        self.t_embedder = TimestepEmbedder(hidden_size)
+        self.blocks = nn.ModuleList(
+            [
+                DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio)
+                for _ in range(depth)
+            ]
+        )
+        # functionally setting patch size to 1 for a point cloud
+        self.final_layer = FinalLayer(hidden_size, 1, self.out_channels)
+        self.initialize_weights()
+
+    def initialize_weights(self):
+        # Initialize transformer layers:
+        def _basic_init(module):
+            if isinstance(module, nn.Linear):
+                torch.nn.init.xavier_uniform_(module.weight)
+                if module.bias is not None:
+                    nn.init.constant_(module.bias, 0)
+
+        self.apply(_basic_init)
+
+        # Initialize x_embed like nn.Linear (instead of nn.Conv2d):
+        w = self.x_embedder.weight.data
+        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
+        nn.init.constant_(self.x_embedder.bias, 0)
+
+        # Initialize timestep embedding MLP:
+        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
+        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
+
+        # Zero-out adaLN modulation layers in DiT blocks:
+        for block in self.blocks:
+            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)
+            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)
+
+        # Zero-out output layers:
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)
+        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)
+        nn.init.constant_(self.final_layer.linear.weight, 0)
+        nn.init.constant_(self.final_layer.linear.bias, 0)
+
+    def forward(self, x, t, pos):
+        """
+        Forward pass of DiT.
+        x: (N, 3, L) tensor of noisy flows
+        t: (N,) tensor of diffusion timesteps
+        pos: (N, 3, L) tensor of 3D coordinates
+        """
+        # NOTE: the patchify/unpatchify layers handle the dimension swapping, so we need to manually do that here
+
+        # concat x and pos
+        # print(x.shape, pos.shape)
+        x = self.x_embedder(x)
+        x += self.rope_embedder(pos)
+        x = torch.transpose(x, -1, -2)
+        t = self.t_embedder(t)  # (N, D)
+        c = t  # (N, D)
+        for block in self.blocks:
+            x = block(x, c)  # (N, L, D)
+        x = self.final_layer(x, c)  # (N, L, patch_size ** 2 * out_channels)
+        # transpose back to (N, out_channels, L)
+        x = torch.transpose(x, -1, -2)
+        return x
+
+
+#################################################################################
+#                   Sine/Cosine Positional Embedding Functions                  #
+#################################################################################
+# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py
+
+
+def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):
+    """
+    grid_size: int of the grid height and width
+    return:
+    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
+    """
+    grid_h = np.arange(grid_size, dtype=np.float32)
+    grid_w = np.arange(grid_size, dtype=np.float32)
+    grid = np.meshgrid(grid_w, grid_h)  # here w goes first
+    grid = np.stack(grid, axis=0)
+
+    grid = grid.reshape([2, 1, grid_size, grid_size])
+    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
+    if cls_token and extra_tokens > 0:
+        pos_embed = np.concatenate(
+            [np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0
+        )
+    return pos_embed
+
+
+def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
+    assert embed_dim % 2 == 0
+
+    # use half of dimensions to encode grid_h
+    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)
+    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)
+
+    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)
+    return emb
+
+
+def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
+    """
+    embed_dim: output dimension for each position
+    pos: a list of positions to be encoded: size (M,)
+    out: (M, D)
+    """
+    assert embed_dim % 2 == 0
+    omega = np.arange(embed_dim // 2, dtype=np.float64)
+    omega /= embed_dim / 2.0
+    omega = 1.0 / 10000**omega  # (D/2,)
+
+    pos = pos.reshape(-1)  # (M,)
+    out = np.einsum("m,d->md", pos, omega)  # (M, D/2), outer product
+
+    emb_sin = np.sin(out)  # (M, D/2)
+    emb_cos = np.cos(out)  # (M, D/2)
+
+    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
+    return emb
+
+
+#################################################################################
+#                                   DiT Configs                                  #
+#################################################################################
+
+
+def DiT_XL_2(**kwargs):
+    return DiT(depth=28, hidden_size=1152, patch_size=2, num_heads=16, **kwargs)
+
+
+def DiT_XL_4(**kwargs):
+    return DiT(depth=28, hidden_size=1152, patch_size=4, num_heads=16, **kwargs)
+
+
+def DiT_XL_8(**kwargs):
+    return DiT(depth=28, hidden_size=1152, patch_size=8, num_heads=16, **kwargs)
+
+
+def DiT_L_2(**kwargs):
+    return DiT(depth=24, hidden_size=1024, patch_size=2, num_heads=16, **kwargs)
+
+
+def DiT_L_4(**kwargs):
+    return DiT(depth=24, hidden_size=1024, patch_size=4, num_heads=16, **kwargs)
+
+
+def DiT_L_8(**kwargs):
+    return DiT(depth=24, hidden_size=1024, patch_size=8, num_heads=16, **kwargs)
+
+
+def DiT_B_2(**kwargs):
+    return DiT(depth=12, hidden_size=768, patch_size=2, num_heads=12, **kwargs)
+
+
+def DiT_B_4(**kwargs):
+    return DiT(depth=12, hidden_size=768, patch_size=4, num_heads=12, **kwargs)
+
+
+def DiT_B_8(**kwargs):
+    return DiT(depth=12, hidden_size=768, patch_size=8, num_heads=12, **kwargs)
+
+
+def DiT_S_2(**kwargs):
+    return DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6, **kwargs)
+
+
+def DiT_S_4(**kwargs):
+    return DiT(depth=12, hidden_size=384, patch_size=4, num_heads=6, **kwargs)
+
+
+def DiT_S_8(**kwargs):
+    return DiT(depth=12, hidden_size=384, patch_size=8, num_heads=6, **kwargs)
+
+
+DiT_models = {
+    "DiT-XL/2": DiT_XL_2,
+    "DiT-XL/4": DiT_XL_4,
+    "DiT-XL/8": DiT_XL_8,
+    "DiT-L/2": DiT_L_2,
+    "DiT-L/4": DiT_L_4,
+    "DiT-L/8": DiT_L_8,
+    "DiT-B/2": DiT_B_2,
+    "DiT-B/4": DiT_B_4,
+    "DiT-B/8": DiT_B_8,
+    "DiT-S/2": DiT_S_2,
+    "DiT-S/4": DiT_S_4,
+    "DiT-S/8": DiT_S_8,
+}
+
+
+if __name__ == "__main__":
+    model = DiT(
+        input_size=[30, 40], depth=28, hidden_size=1152, patch_size=1, num_heads=16
+    )
+    input = torch.randn(1, 3, 30, 40)
+    pos = torch.randn(1, 1200, 3)
+    t = torch.randint(100, size=(1,))
+    print(model(input, t, pos).shape)
diff --git a/src/flowbothd/models/modules/history_encoder.py b/src/flowbothd/models/modules/history_encoder.py
new file mode 100644
index 0000000..017106b
--- /dev/null
+++ b/src/flowbothd/models/modules/history_encoder.py
@@ -0,0 +1,196 @@
+# Define the history encoder
+import lightning as L
+import torch
+import torch.nn as nn
+import torch_geometric.data as tgd
+
+
+# Yishu's old old old version - history : grasp point & direction & outcome
+class PDOHistoryEncoder(nn.Module):
+    def __init__(
+        self,
+        input_dim=7,
+        output_dim=32,
+        d_model=128,
+        nhead=4,
+        num_layers=2,
+        dim_feedforward=256,
+        batch_norm=False,
+    ):
+        super(PDOHistoryEncoder, self).__init__()
+        self.input_dim = input_dim
+        self.d_model = d_model
+        self.transformer = nn.TransformerEncoder(
+            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward), num_layers
+        )
+        self.input_linear = nn.Linear(input_dim, d_model)
+        self.output_linear = nn.Linear(d_model, output_dim)
+        self.activation = nn.Sigmoid()
+
+    def forward(self, raw_src):
+        # raw_src shape: [batch_size, seq_len, input_dim]
+        src = raw_src.transpose(0, 1)
+        # src shape: [seq_len, batch_size, input_dim]
+        src = self.input_linear(src)
+        # Transformer expects input of shape: [seq_len, batch_size, d_model]
+        transformer_output = self.transformer(src)
+        # Pooling over the time dimension to get fixed size output
+        pooled_output = torch.mean(transformer_output, dim=0)
+        # Final output linear layer to get desired output dimension
+        output = self.activation(self.output_linear(pooled_output))
+        return output
+
+
+def get_history_batch(batch):
+    """Extracts a single batch of the history data for encoding,
+    because each history element is processed separately."""
+    has_history_ids = []
+    no_history_ids = []
+    history_datas = []
+    for id, data in enumerate(batch.to_data_list()):
+        if data.K == 0:  # No history
+            no_history_ids.append(id)
+            continue
+        has_history_ids.append(id)
+        history_data = []
+        # Get start/end positions based on lengths.
+
+        # HACK: remove once the data has been regenerated...
+        if len(data.history.shape) == 3:
+            data.history = data.history.reshape(-1, 3)
+            data.flow_history = data.flow_history.reshape(-1, 3)
+
+        N = data.pos.shape[0]  # num of points
+        if hasattr(data, "lengths"):
+            ixs = [0] + data.lengths.cumsum(0).tolist()
+        else:
+            ixs = [(i * N) for i in range(data.K + 1)]
+        for i in range(len(ixs) - 1):
+            history_data.append(
+                tgd.Data(
+                    x=data.flow_history[ixs[i] : ixs[i + 1]],
+                    pos=data.history[ixs[i] : ixs[i + 1]],
+                )
+            )
+
+        history_datas.extend(history_data)
+    if len(history_datas) == 0:
+        return no_history_ids, has_history_ids, None  # No has_history batch
+    return no_history_ids, has_history_ids, tgd.Batch.from_data_list(history_datas)
+
+
+def history_latents_to_nested_list(batch, history_latents):
+    """Converting history latents from stacked form to nested list"""
+    datas = batch.to_data_list()
+    # history_lengths = [0] + [data.K.item() for data in datas]
+    history_lengths = [0] + [1 for data in datas]
+    ixs = torch.tensor(history_lengths).cumsum(0).tolist()
+    post_encoder_latents = []
+    for i, data in enumerate(datas):
+        post_encoder_latents.append(history_latents[ixs[i] : ixs[i + 1]])
+
+    return post_encoder_latents
+
+
+# Previous flow history
+class HistoryEncoder(L.LightningModule):
+    def __init__(
+        self,
+        history_dim=128,
+        history_len=1,
+        batch_norm=False,
+        transformer=True,
+        repeat_dim=True,
+    ):
+        super(HistoryEncoder, self).__init__()
+        self.point_cnts = 1200
+        self.history_len = history_len
+        assert self.history_len == 1, "currently only supports 1 previous step history"
+        self.history_dim = history_dim
+
+        if batch_norm:
+            from rpad.pyg.nets import pointnet2 as pnp
+        else:
+            import flowbothd.models.modules.pn2 as pnp
+        self.prev_flow_encoder = pnp.PN2Encoder(in_dim=3, out_dim=history_dim)
+        self.no_history_embedding = nn.Parameter(
+            torch.randn(history_dim), requires_grad=True
+        )
+        self.repeat_dim = repeat_dim
+        self.transformer = transformer
+        if self.transformer:
+            self.transformer = nn.Transformer(d_model=history_dim)
+
+    def forward(self, batch):
+        # point_cnts = batch.lengths[0]
+
+        history_embeds = torch.zeros(len(batch.lengths), self.history_dim).to(
+            self.device
+        )  # Also add the no history batch
+        no_history_ids, has_history_ids, history_batch = get_history_batch(batch)
+        # print("bsz = ", len(batch.lengths))
+        if len(has_history_ids) != 0:  # Has history samples
+            history_batch = history_batch.to(self.device)
+            has_history_embeds = self.prev_flow_encoder(history_batch)
+            history_embeds[has_history_ids] += has_history_embeds
+        if len(no_history_ids) != 0:  # Has no history samples
+            history_embeds[no_history_ids] += self.no_history_embedding
+
+        if self.transformer:
+            history_nested_list = history_latents_to_nested_list(batch, history_embeds)
+            src_padded = nn.utils.rnn.pad_sequence(
+                history_nested_list, batch_first=False, padding_value=0
+            )
+            # Create a mask for the padded sequences.
+            src_mask = (src_padded == 0.0).all(dim=-1)
+
+            # This is our query vector. It has shape [S, N, E], where S is the sequence length, N is the batch size, and E is the embedding size.
+            tgt = torch.ones(1, batch.num_graphs, self.history_dim).to(self.device)
+
+            # The transformer also expects the input to be of type float.
+            src_padded = src_padded.float()
+            tgt = tgt.float()
+            # Pass the input through the transformer, with mask and tgt.
+            out = self.transformer(
+                src_padded, tgt, src_key_padding_mask=src_mask.transpose(1, 0)
+            )
+
+            embeddings = out.permute(1, 0, 2).squeeze(1)  # history step = 1
+        else:
+            embeddings = history_embeds
+
+        if self.repeat_dim == True:  # To point-wise features to concat to DiT
+            embeddings = embeddings.unsqueeze(1).repeat(1, self.point_cnts, 1)
+        else:
+            embeddings = embeddings.unsqueeze(1)
+        return embeddings
+
+
+if __name__ == "__main__":
+    # Example usage
+    # seq_len = 10  # This can vary
+    # batch_size = 1
+    # input_dim = 7
+
+    # model = HistoryEncoder()
+    # src = torch.rand(seq_len, batch_size, input_dim)  # Random input
+    # output = model(src)
+    # print(output.shape)  # Should be [batch_size, 128]
+    import numpy as np
+    from torch_geometric.data import Batch, Data
+
+    model = HistoryEncoder(transformer=True)
+    data = Data(
+        num_points=torch.tensor([1200]),  # N: shape of point cloud
+        pos=torch.from_numpy(np.zeros((1200, 3))).float(),
+        delta=torch.from_numpy(np.zeros((1200, 3))).float(),
+        mask=torch.from_numpy(np.ones(1200)).float(),
+        history=torch.from_numpy(np.zeros((1200, 3))).float(),  # N*K, 3
+        flow_history=torch.from_numpy(  # N*K, 3
+            np.zeros((1200, 3))
+        ).float(),  # Snapshot of flow history
+        K=1,  # length of history
+        lengths=torch.as_tensor([1200]).int(),  # size of point cloud
+    )
+    batch = Batch.from_data_list([data, data, data])
+    history_embed = model(batch)
diff --git a/src/flowbothd/models/modules/history_translator.py b/src/flowbothd/models/modules/history_translator.py
new file mode 100644
index 0000000..02debf7
--- /dev/null
+++ b/src/flowbothd/models/modules/history_translator.py
@@ -0,0 +1,141 @@
+# Define the history encoder
+import lightning as L
+import torch
+import torch.nn as nn
+import torch_geometric.data as tgd
+
+# from rpad.pyg.nets import pointnet2 as pnp
+
+
+def get_history_batch(batch):
+    """Extracts a single batch of the history data for encoding,
+    because each history element is processed separately."""
+    history_datas = []
+    for data in batch.to_data_list():
+        history_data = []
+        # Get start/end positions based on lengths.
+
+        # HACK: remove once the data has been regenerated...
+        if len(data.history.shape) == 3:
+            data.history = data.history.reshape(-1, 3)
+            data.flow_history = data.flow_history.reshape(-1, 3)
+
+        N = data.pos.shape[0]  # num of points
+        if hasattr(data, "lengths"):
+            ixs = [0] + data.lengths.cumsum(0).tolist()
+        else:
+            ixs = [(i * N) for i in range(data.K + 1)]
+        for i in range(len(ixs) - 1):
+            history_data.append(
+                tgd.Data(
+                    x=data.flow_history[ixs[i] : ixs[i + 1]],
+                    pos=data.history[ixs[i] : ixs[i + 1]],
+                )
+            )
+
+        history_datas.extend(history_data)
+    return tgd.Batch.from_data_list(history_datas)
+
+
+def history_latents_to_nested_list(batch, history_latents):
+    """Converting history latents from stacked form to nested list"""
+    datas = batch.to_data_list()
+    history_lengths = [0] + [data.K.item() for data in datas]
+    ixs = torch.tensor(history_lengths).cumsum(0).tolist()
+    post_encoder_latents = []
+    for i, data in enumerate(datas):
+        post_encoder_latents.append(history_latents[ixs[i] : ixs[i + 1]])
+
+    return post_encoder_latents
+
+
+# Previous flow history
+class HistoryTranslator(L.LightningModule):
+    def __init__(self, history_dim=128, history_len=1, hidden_dim=128):
+        super(HistoryTranslator, self).__init__()
+        print("Using translator!!!")
+        self.history_len = history_len
+        assert self.history_len == 1, "currently only supports 1 previous step history"
+        self.history_dim = history_dim
+        # self.prev_flow_encoder = pnp.PN2Encoder(in_dim=3, out_dim=history_dim)
+
+        # self.history_pcd_encoder = pnp.PN2Dense(
+        #     in_channels=3,
+        #     out_channels=hidden_dim,
+        #     p=pnp.PN2DenseParams(),
+        # )
+        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=3)
+        self.transformer_encoder = nn.TransformerEncoder(
+            self.encoder_layer, num_layers=2
+        )
+        self.output_layer = nn.Linear(
+            hidden_dim, history_dim
+        )  # Output the same feature dimension as input
+
+    def forward(self, batch):
+        point_cnts = batch.lengths[0]
+
+        # Data(
+        #     num_points=torch.tensor([1200]), # N: shape of point cloud
+        #     pos=torch.from_numpy(np.zeros((1200, 3))).float(),
+        #     delta=torch.from_numpy(np.zeros((1200, 3))).float(),
+        #     mask=torch.from_numpy(np.ones(1200)).float(),
+        #     history=torch.from_numpy(np.zeros((1200, 3))).float(),  # N*K, 3
+        #     flow_history=torch.from_numpy( # N*K, 3
+        #         np.zeros((1200, 3))
+        #     ).float(),  # Snapshot of flow history
+        #     K=1,  # length of history
+        #     lengths=torch.as_tensor([1200]).int(), # size of point cloud
+        # )
+        # batch = Batch.from_data_list([data, data, data])
+        # history_batch =
+
+        history_pcd_features = self.history_pcd_encoder(batch)
+
+        history_batch = torch.concat(
+            [batch.history, history_pcd_features], dim=-1
+        ).reshape(-1, point_cnts, 6)
+        separate_token = torch.zeros(
+            history_batch.shape[0], 1, 6, device=batch.pos.device
+        )  # Empty token
+        current_batch = torch.concat(
+            [batch.pos, torch.zeros_like(batch.pos, device=batch.pos.device)], dim=-1
+        ).reshape(-1, point_cnts, 6)
+        full_sequence = torch.concat(
+            [history_batch, separate_token, current_batch], dim=-2
+        )  # bsz * point_size * 128
+        # breakpoint()
+        transformed = self.transformer_encoder(full_sequence.permute(1, 0, 2))
+        # breakpoint()
+        embeddings = self.output_layer(transformed).permute(1, 0, 2)[:, -point_cnts:]
+        return embeddings
+
+
+if __name__ == "__main__":
+    # Example usage
+    # seq_len = 10  # This can vary
+    # batch_size = 1
+    # input_dim = 7
+
+    # model = HistoryEncoder()
+    # src = torch.rand(seq_len, batch_size, input_dim)  # Random input
+    # output = model(src)
+    # print(output.shape)  # Should be [batch_size, 128]
+    import numpy as np
+    from torch_geometric.data import Batch, Data
+
+    model = HistoryTranslator()
+    data = Data(
+        num_points=torch.tensor([1200]),  # N: shape of point cloud
+        pos=torch.from_numpy(np.zeros((1200, 3))).float(),
+        delta=torch.from_numpy(np.zeros((1200, 3))).float(),
+        mask=torch.from_numpy(np.ones(1200)).float(),
+        history=torch.from_numpy(np.zeros((1200, 3))).float(),  # N*K, 3
+        flow_history=torch.from_numpy(  # N*K, 3
+            np.zeros((1200, 3))
+        ).float(),  # Snapshot of flow history
+        K=1,  # length of history
+        lengths=torch.as_tensor([1200]).int(),  # size of point cloud
+    )
+    batch = Batch.from_data_list([data, data, data])
+    history_embed = model(batch)
diff --git a/src/flowbothd/models/modules/pn2.py b/src/flowbothd/models/modules/pn2.py
new file mode 100644
index 0000000..20c5d1b
--- /dev/null
+++ b/src/flowbothd/models/modules/pn2.py
@@ -0,0 +1,391 @@
+from dataclasses import dataclass
+from typing import Literal
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from rpad.pyg.nets import pointnet2 as pnp_bn
+from rpad.pyg.nets.mlp import MLP, MLPParams
+from torch_geometric.data import Data
+from torch_geometric.nn import PointConv, fps, global_max_pool, knn_interpolate, radius
+
+
+@dataclass
+class SAParams:
+    # Ratio of points to sample.
+    ratio: float
+
+    # Radius for the ball query.
+    r: float
+
+    # Parameters for the PointNet MLP.
+    net_params: MLPParams
+
+    # Maximum number of neighbors for the ball query to return.
+    max_num_neighbors: int = 64
+
+
+class SAModule(nn.Module):
+    def __init__(self, in_chan: int, out_chan: int, p: SAParams):
+        """Create a Set Aggregation module from PointNet++.
+
+        Algorithm:
+          1) Perform "farthest point sampling", selecting a number of points proportional
+             to the specified ratio. i.e. if the input has 10000 points and the ratio is .2,
+             we select 2000 points with FPS.
+          2) For each selected points, get all points within radius r, up to max_num_neighbors.
+          3) Run PointNet (vanilla) on each region independently.
+
+        Args:
+            p (SAParams) See above.
+
+        """
+        super(SAModule, self).__init__()
+        self.ratio = p.ratio
+        self.max_num_neighbors = p.max_num_neighbors
+        self.r = p.r
+
+        # NOTE: "add_self_loops" for some reason breaks the gradient flow in a batch!!!
+        # See my bug at https://github.com/rusty1s/pytorch_geometric/issues/2558
+        self.conv = PointConv(
+            MLP(in_chan, out_chan, p.net_params), add_self_loops=False
+        )
+
+    def forward(self, x, pos, batch):
+        if self.ratio == 1.0:
+            selected_pos = pos
+            selected_batch = batch
+        else:
+            # Select the points using "Farthest Point Sampling".
+            idx = fps(pos, batch, ratio=self.ratio)
+
+            selected_pos = pos[idx]
+            selected_batch = batch[idx]
+
+        # Perform a ball query around each of the points.
+        row, col = radius(
+            pos,
+            selected_pos,
+            self.r,
+            batch,
+            selected_batch,
+            max_num_neighbors=self.max_num_neighbors,
+        )
+
+        # Run PointNet on each point set independently.
+        edge_index = torch.stack([col, row], dim=0)
+        x = self.conv(x, (pos, selected_pos), edge_index)
+        pos, batch = selected_pos, selected_batch
+
+        return x, pos, batch
+
+
+@dataclass
+class GlobalSAParams:
+    net_params: MLPParams
+
+
+class GlobalSAModule(nn.Module):
+    def __init__(self, in_chan: int, out_chan: int, p: GlobalSAParams):
+        """Module to perform the Global Set Aggregation operation.
+
+        Two steps:
+        1) For each point, apply the mlp specified in net_params.
+        2) Use the global_max_pool operation to get a final feature vector.
+
+        Args:
+            p (GlobalSAParams): Parameters.
+        """
+        super(GlobalSAModule, self).__init__()
+        self.net = MLP(in_chan, out_chan, p.net_params)
+
+    def forward(self, x, pos, batch):
+        # Batch application of the MLP.
+        x = self.net(torch.cat([x, pos], dim=1))
+
+        # MaxPool operation.
+        x = global_max_pool(x, batch)
+        pos = pos.new_zeros((x.size(0), 3))
+        batch = torch.arange(x.size(0), device=batch.device)
+        return x, pos, batch
+
+
+@dataclass
+class FPParams:
+    # MLP Parameters for the feature update step.
+    net_params: MLPParams
+
+    # Number of neighbors used for knn_interpolate.
+    k: int = 3
+
+
+class FPModule(torch.nn.Module):
+    def __init__(self, in_channels: int, out_channels: int, params: FPParams):
+        """Feature Propagation Module.
+
+        1) Interpolate each point based on the k nearest embedded points.
+        2) Concatenate skip features.
+        3) Update feature vectors with another MLP.
+
+        Args:
+            params (FPParams): Parameters.
+
+        """
+        super().__init__()
+        self.k = params.k
+        self.net = MLP(in_channels, out_channels, params.net_params)
+
+    def forward(self, x, pos, batch, x_skip, pos_skip, batch_skip):
+        # If we need to interpolate, interpolate.
+        # Otherwise (i.e. when the sampling ratio is 1.0, we don't need to.
+        if pos.shape[0] != pos_skip.shape[0]:
+            # Perform the interpolation.
+            x = knn_interpolate(x, pos, pos_skip, batch, batch_skip, k=self.k)
+
+        # If we have skip connections concatenate them.
+        if x_skip is not None:
+            x = torch.cat([x, x_skip], dim=1)
+
+        # Run them
+        x = self.net(x)
+
+        return x, pos_skip, batch_skip
+
+
+@dataclass
+class PN2EncoderParams:
+    # Layer 1. See SAParams for description.
+    sa1: SAParams = SAParams(0.2, 0.2, MLPParams((64, 64), batch_norm=False), 64)
+    sa1_outdim: int = 128
+
+    # Layer 2. See SAParams for description.
+    sa2: SAParams = SAParams(0.25, 0.4, MLPParams((128, 128), batch_norm=False), 64)
+    sa2_outdim: int = 256
+
+    # Global aggregation. See GlobalSAParams for description.
+    gsa: GlobalSAParams = GlobalSAParams(MLPParams((256, 512), batch_norm=False))
+
+
+class PN2Encoder(nn.Module):
+    def __init__(
+        self,
+        in_dim: int = 0,
+        out_dim: int = 1024,
+        p: PN2EncoderParams = PN2EncoderParams(),
+    ):
+        """A PointNet++ encoder. Takes in a pointcloud, outputs a single latent vector.
+
+        Args:
+            in_dim: The dimensionality of the feature vector attached to each point. i.e. if it's
+                    a mask, then in_dim=1. No features => in_dim=1
+            out_dim: Dimensionality of the output.
+            p: Internal parameters for the network.
+        """
+        super().__init__()
+
+        # The Set Aggregation modules.
+        self.sa1_module = SAModule(in_chan=3 + in_dim, out_chan=p.sa1_outdim, p=p.sa1)
+        self.sa2_module = SAModule(
+            in_chan=3 + p.sa1_outdim, out_chan=p.sa2_outdim, p=p.sa2
+        )
+        self.sa3_module = GlobalSAModule(
+            in_chan=3 + p.sa2_outdim, out_chan=out_dim, p=p.gsa
+        )
+
+    def forward(self, data):
+        sa0_out = (data.x, data.pos, data.batch)
+        sa1_out = self.sa1_module(*sa0_out)
+        sa2_out = self.sa2_module(*sa1_out)
+        sa3_out = self.sa3_module(*sa2_out)
+        x, pos, batch = sa3_out
+
+        return x
+
+
+@dataclass
+class PN2DenseParams:
+    ###########################################################################
+    # SET AGGREGATION
+    ###########################################################################
+
+    # Layer 1. See SAParams for description.
+    sa1: SAParams = SAParams(
+        0.2, 0.2, MLPParams((64, 64), out_act="none", batch_norm=False), 64
+    )
+    sa1_outdim: int = 128
+
+    # Layer 2. See SAParams for description.
+    sa2: SAParams = SAParams(
+        0.25, 0.4, MLPParams((128, 128), out_act="none", batch_norm=False), 64
+    )
+    sa2_outdim: int = 256
+
+    # Global aggregation. See GlobalSAParams for description.
+    gsa: GlobalSAParams = GlobalSAParams(
+        MLPParams((256, 512), out_act="none", batch_norm=False)
+    )
+    gsa_outdim: int = 1024
+
+    ###########################################################################
+    # FEATURE PROPAGATION
+    # Since this is the decoder, execution happens 3->2->1.
+    ###########################################################################
+
+    # Layer 3. See FPParams for description.
+    fp3: FPParams = FPParams(MLPParams((256,), out_act="none", batch_norm=False), k=1)
+
+    # Layer 2. See FPParams for description.
+    fp2: FPParams = FPParams(MLPParams((256,), out_act="none", batch_norm=False), k=3)
+
+    # Layer 1. See FPParams for description.
+    fp1: FPParams = FPParams(
+        MLPParams((128, 128), out_act="none", batch_norm=False), k=3
+    )
+    fp1_outdim: int = 128
+
+    # Dimensions of the final 2 linear layers.
+    lin1_dim: int = 128
+    lin2_dim: int = 128
+
+    # Output layer activation.
+    out_act: Literal["none", "softmax", "relu"] = "none"
+
+
+class PN2Dense(nn.Module):
+    def __init__(
+        self,
+        in_channels: int = 0,
+        out_channels: int = 3,
+        p: PN2DenseParams = PN2DenseParams(),
+    ):
+        """The PointNet++ "dense" network architecture, as proposed in the original paper.
+        In general, the parameters "p" are architecture or hyperparameter choices for the network.
+        The other arguments are structural ones, determining the input and output dimensionality.
+
+        It's a bit of a U-Net architecture, so I've written some automatic wiring to make sure that
+        the layers all agree.
+
+        Args:
+            in_channels: The number of non-XYZ channels attached to each point. For instance, no additional
+                         features would have in_channels=0, RGB features would be in_channels=3, a binary mask
+                         would be in_channels=1, etc. For a point cloud passed to the network with N points,
+                         the `x` property must be set on the object to a float tensor of shape [N x in_channels].
+            out_channels: The dimension of the per-point output channels.
+            p: Architecture and hyperparameters for the network. Default is the original set from the paper.
+        """
+        breakpoint()
+        super().__init__()
+
+        self.in_ch = in_channels
+        self.out_ch = out_channels
+
+        # Construct the set aggregation modules. This is the encoder.
+        self.sa1 = SAModule(3 + self.in_ch, p.sa1_outdim, p=p.sa1)
+        self.sa2 = SAModule(3 + p.sa1_outdim, p.sa2_outdim, p=p.sa2)
+        self.sa3 = GlobalSAModule(3 + p.sa2_outdim, p.gsa_outdim, p=p.gsa)
+
+        # The Feature Propagation modules. This is the decoder.
+        self.fp3 = FPModule(p.gsa_outdim + p.sa2_outdim, p.sa2_outdim, p.fp3)
+        self.fp2 = FPModule(p.sa2_outdim + p.sa1_outdim, p.sa1_outdim, p.fp2)
+        self.fp1 = FPModule(p.sa1_outdim + in_channels, p.fp1_outdim, p.fp1)
+
+        # Final linear layers at the output.
+        self.lin1 = torch.nn.Linear(p.fp1_outdim, p.lin1_dim)
+        self.lin2 = torch.nn.Linear(p.lin1_dim, p.lin2_dim)
+        self.lin3 = torch.nn.Linear(p.lin2_dim, out_channels)
+        self.out_act = p.out_act
+
+    def forward(self, data: Data):
+        sa0_out = (data.x, data.pos, data.batch)
+
+        # Encode.
+        sa1_out = self.sa1(*sa0_out)
+        sa2_out = self.sa2(*sa1_out)
+        sa3_out = self.sa3(*sa2_out)
+
+        # Decode.
+        fp3_out = self.fp3(*sa3_out, *sa2_out)
+        fp2_out = self.fp2(*fp3_out, *sa1_out)
+        x, _, _ = self.fp1(*fp2_out, *sa0_out)
+
+        # Final layers.
+        x = F.leaky_relu(self.lin1(x))
+        x = F.leaky_relu(self.lin2(x))
+        x = self.lin3(x)
+
+        if self.out_act != "none":
+            raise ValueError()
+
+        return x
+
+
+class PN2DenseLatentEncodingEverywhere(nn.Module):
+    def __init__(
+        self,
+        history_embed_dim,
+        in_channels: int = 0,
+        out_channels: int = 3,
+        p: pnp_bn.PN2DenseParams = pnp_bn.PN2DenseParams(),  # With Batch Norm
+    ):
+        super().__init__()
+
+        self.in_ch = in_channels
+        self.out_ch = out_channels
+        # Construct the set aggregation modules. This is the encoder.
+        self.sa1 = SAModule(3 + self.in_ch, p.sa1_outdim, p=p.sa1)
+        self.sa2 = SAModule(3 + p.sa1_outdim, p.sa2_outdim, p=p.sa2)
+        self.sa3 = GlobalSAModule(3 + p.sa2_outdim, p.gsa_outdim, p=p.gsa)
+
+        # The Feature Propagation modules. This is the decoder.
+        self.fp3 = FPModule(p.gsa_outdim + p.sa2_outdim, p.sa2_outdim, p.fp3)
+        self.fp2 = FPModule(p.sa2_outdim + p.sa1_outdim, p.sa1_outdim, p.fp2)
+        self.fp1 = FPModule(p.sa1_outdim + in_channels, p.fp1_outdim, p.fp1)
+
+        # Linear projection layers to incorporate the flwo embedding.
+        # Option: could add relu?
+        self.global_linear = torch.nn.Linear(history_embed_dim, p.gsa_outdim)
+        self.fp3_embedding_linear = torch.nn.Linear(history_embed_dim, p.sa2_outdim)
+        self.fp2_embedding_linear = torch.nn.Linear(history_embed_dim, p.sa1_outdim)
+        self.fp1_embedding_linear = torch.nn.Linear(history_embed_dim, p.fp1_outdim)
+
+        # Final linear layers at the output.
+        self.lin1 = torch.nn.Linear(p.fp1_outdim, p.lin1_dim)
+        self.lin2 = torch.nn.Linear(p.lin1_dim, p.lin2_dim)
+        self.lin3 = torch.nn.Linear(p.lin2_dim, out_channels)
+        self.out_act = p.out_act
+
+    def forward(self, data: Data, latents):
+        sa0_out = (data.x, data.pos, data.batch)
+        # Encode.
+        sa1_out = self.sa1(*sa0_out)
+        sa2_out = self.sa2(*sa1_out)
+        x3, pos3, batch3 = self.sa3(*sa2_out)
+
+        # No concatenation! just hadamard!
+        x3 = self.global_linear(latents) * x3
+        sa3_out = x3, pos3, batch3
+
+        # Decode.
+        x_fp3, pos_fp3, batch_fp3 = self.fp3(*sa3_out, *sa2_out)
+        fp3_latents = self.fp3_embedding_linear(latents)
+        x_fp3 = fp3_latents.repeat_interleave(torch.bincount(batch_fp3), dim=0) * x_fp3
+        fp3_out = x_fp3, pos_fp3, batch_fp3
+
+        x_fp2, pos_fp2, batch_fp2 = self.fp2(*fp3_out, *sa1_out)
+        fp2_latents = self.fp2_embedding_linear(latents)
+        x_fp2 = fp2_latents.repeat_interleave(torch.bincount(batch_fp2), dim=0) * x_fp2
+        fp2_out = x_fp2, pos_fp2, batch_fp2
+
+        x, _, batch_fp1 = self.fp1(*fp2_out, *sa0_out)
+        fp1_latents = self.fp1_embedding_linear(latents)
+        x = fp1_latents.repeat_interleave(torch.bincount(batch_fp1), dim=0) * x
+
+        # Final layers.
+        x = F.leaky_relu(self.lin1(x))
+        x = F.leaky_relu(self.lin2(x))
+        x = self.lin3(x)
+
+        if self.out_act != "none":
+            raise ValueError()
+
+        return x
diff --git a/src/flowbothd/py.typed b/src/flowbothd/py.typed
new file mode 100644
index 0000000..e69de29
diff --git a/src/flowbothd/simulations/simulation.py b/src/flowbothd/simulations/simulation.py
new file mode 100644
index 0000000..1f5cc71
--- /dev/null
+++ b/src/flowbothd/simulations/simulation.py
@@ -0,0 +1,696 @@
+# Simulation (w/ suction gripper):
+# move the object according to calculated trajectory.
+import os
+
+import numpy as np
+import pybullet as p
+import rpad.pyg.nets.pointnet2 as pnp
+import torch
+from rpad.partnet_mobility_utils.data import PMObject
+
+from flowbothd.models.flow_trajectory_predictor import (
+    FlowSimulationInferenceModule,
+)
+from flowbothd.simulations.suction import (  # compute_flow,; run_trial_with_history,
+    GTFlowModel,
+    GTTrajectoryModel,
+    PMSuctionSim,
+    run_trial,
+    run_trial_with_history_filter,
+    run_trial_with_switch_models,
+)
+
+# from flowbothd.simulations.suction_v2 import PMSuctionSim
+# from flowbothd.simulations.suction_v2 import run_trial, run_trial_with_history_filter
+
+
+def trial_flow(
+    obj_id="41083",
+    n_steps=30,
+    all_joint=True,
+    available_joints=None,
+    gui=False,
+    website=False,
+    pm_dir=os.path.expanduser("~/datasets/partnet-mobility/convex"),
+    # pm_dir=os.path.expanduser("~/datasets/partnet-mobility/raw"),
+):
+    # env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+    raw_data = PMObject(os.path.join(pm_dir, obj_id))
+
+    if available_joints is None:  # Use the passed in joint sets
+        available_joints = raw_data.semantics.by_type(
+            "hinge"
+        ) + raw_data.semantics.by_type("slider")
+        available_joints = [joint.name for joint in available_joints]
+
+    if all_joint:  # Need to traverse all the joints
+        picked_joints = available_joints
+    else:
+        picked_joints = [available_joints[np.random.randint(0, len(available_joints))]]
+
+    sim_trajectories = []
+    results = []
+    figs = {}
+    for joint_name in picked_joints:
+        # t0 = time.perf_counter()
+        # print(f"opening {joint.name}, {joint.label}")
+        print(f"opening {joint_name}")
+        env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+        model = GTFlowModel(raw_data, env)
+        fig, result, sim_trajectory = run_trial(
+            env,
+            raw_data,
+            joint_name,
+            model,
+            n_steps=n_steps,
+            save_name=f"{obj_id}_{joint_name}",
+            website=website,
+            gui=gui,
+        )
+        sim_trajectories.append(sim_trajectory)
+        if result.assertion is False:
+            with open(
+                "/home/yishu/flowbothd/logs/assertion_failure.txt", "a"
+            ) as f:
+                f.write(f"Object: {obj_id}; Joint: {joint_name}\n")
+            continue
+        if result.contact is False:
+            continue
+        figs[joint_name] = fig
+        results.append(result)
+
+    return figs, results, sim_trajectories
+
+
+# Trial with groundtruth trajectories
+def trial_gt_trajectory(
+    obj_id="41083",
+    traj_len=10,
+    n_steps=30,
+    all_joint=True,
+    available_joints=None,
+    gui=False,
+    website=False,
+):
+    # pm_dir = os.path.expanduser("~/datasets/partnet-mobility/raw")
+    pm_dir = os.path.expanduser("~/datasets/partnet-mobility/convex")
+    # env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+    raw_data = PMObject(os.path.join(pm_dir, obj_id))
+
+    if available_joints is None:  # Use the passed in joint sets
+        available_joints = raw_data.semantics.by_type(
+            "hinge"
+        ) + raw_data.semantics.by_type("slider")
+        available_joints = [joint.name for joint in available_joints]
+
+    if all_joint:  # Need to traverse all the joints
+        picked_joints = available_joints
+    else:
+        picked_joints = [available_joints[np.random.randint(0, len(available_joints))]]
+
+    sim_trajectories = []
+    results = []
+    movable_links = []
+    figs = {}
+    for joint_name in picked_joints:
+        # t0 = time.perf_counter()
+        # print(f"opening {joint.name}, {joint.label}")
+        print(f"opening {joint_name}")
+        env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+
+        # Close all joints:
+        for link_to_restore in [
+            joint.name
+            for joint in raw_data.semantics.by_type("hinge")
+            + raw_data.semantics.by_type("slider")
+        ]:
+            info = p.getJointInfo(
+                env.render_env.obj_id,
+                env.render_env.link_name_to_index[link_to_restore],
+                env.render_env.client_id,
+            )
+            init_angle, target_angle = info[8], info[9]
+            env.set_joint_state(link_to_restore, init_angle)
+
+        # model = GTFlowModel(raw_data, env)
+        model = GTTrajectoryModel(raw_data, env, traj_len)
+        fig, result, sim_trajectory = run_trial(
+            env,
+            raw_data,
+            joint_name,
+            model,
+            n_steps=n_steps,
+            save_name=f"{obj_id}_{joint_name}",
+            website=website,
+            gui=gui,
+        )
+        # raw_data = PMObject(os.path.join(pm_dir, obj_id))
+        sim_trajectories.append(sim_trajectory)
+        if result.success:
+            movable_links.append(joint_name)
+        if result.assertion is False:
+            with open(
+                "/home/yishu/flowbothd/logs/assertion_failure.txt", "a"
+            ) as f:
+                f.write(f"Object: {obj_id}; Joint: {joint_name}\n")
+            continue
+        if result.contact is False:
+            continue
+        figs[joint_name] = fig
+        results.append(result)
+
+    return figs, results, movable_links, sim_trajectories
+
+    # pm_dir = os.path.expanduser("~/datasets/partnet-mobility/raw")
+    # env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+    # raw_data = PMObject(os.path.join(pm_dir, obj_id))
+
+    # available_joints = raw_data.semantics.by_type("hinge") + raw_data.semantics.by_type(
+    #     "slider"
+    # )
+
+    # joint = available_joints[np.random.randint(0, len(available_joints))]
+    # model = GTTrajectoryModel(raw_data, env, traj_len)
+
+    # # t0 = time.perf_counter()
+    # print(f"opening {joint.name}, {joint.label}")
+    # run_trial(env, raw_data, joint.name, model, n_steps=1)
+
+
+def create_network(traj_len=15, ckpt_file=None):
+    network = pnp.PN2Dense(
+        in_channels=1, out_channels=3 * traj_len, p=pnp.PN2DenseParams()
+    )
+    ckpt = torch.load(ckpt_file)
+    network.load_state_dict(
+        {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
+    )
+    return network
+
+
+# Trial with model predicted trajectories
+def trial_with_prediction(
+    obj_id="41083",
+    network=None,
+    n_step=1,
+    gt_mask=False,
+    gui=False,
+    all_joint=False,
+    website=False,
+    sgp=True,
+    available_joints=None,
+    analysis=False,
+):
+    # pm_dir = os.path.expanduser("~/datasets/partnet-mobility/raw")
+    pm_dir = os.path.expanduser("~/datasets/partnet-mobility/convex")
+    # env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+    raw_data = PMObject(os.path.join(pm_dir, obj_id))
+
+    if available_joints is None:  # Use the passed in joint sets
+        available_joints = raw_data.semantics.by_type(
+            "hinge"
+        ) + raw_data.semantics.by_type("slider")
+        available_joints = [joint.name for joint in available_joints]
+
+    print("available_joints:", available_joints)
+
+    model = FlowSimulationInferenceModule(network, mask_input_channel=gt_mask)
+
+    if all_joint:  # Need to traverse all the joints
+        picked_joints = available_joints
+    else:
+        picked_joints = [available_joints[np.random.randint(0, len(available_joints))]]
+
+    sim_trajectories = []
+    results = []
+    figs = {}
+    for joint_name in picked_joints:
+        # t0 = time.perf_counter()
+        # print(f"opening {joint.name}, {joint.label}")
+        print(f"opening {joint_name}")
+        env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+        gt_model = GTFlowModel(raw_data, env) if gt_mask else None
+
+        # Close all joints:
+        for link_to_restore in [
+            joint.name
+            for joint in raw_data.semantics.by_type("hinge")
+            + raw_data.semantics.by_type("slider")
+        ]:
+            info = p.getJointInfo(
+                env.render_env.obj_id,
+                env.render_env.link_name_to_index[link_to_restore],
+                env.render_env.client_id,
+            )
+            init_angle, target_angle = info[8], info[9]
+            env.set_joint_state(link_to_restore, init_angle)
+
+        fig, result, sim_trajectory = run_trial(
+            env,
+            raw_data,
+            joint_name,
+            model,
+            gt_model=gt_model,
+            n_steps=n_step,
+            save_name=f"{obj_id}_{joint_name}",
+            website=website,
+            sgp=sgp,
+            gui=gui,
+            analysis=analysis,
+        )
+        sim_trajectories.append(sim_trajectory)
+        if result.assertion is False:
+            with open(
+                "/home/yishu/flowbothd/logs/assertion_failure.txt", "a"
+            ) as f:
+                f.write(f"Object: {obj_id}; Joint: {joint_name}\n")
+            continue
+        if result.contact is False:
+            continue
+        figs[joint_name] = fig
+        results.append(result)
+
+    return figs, results, sim_trajectories
+
+
+def trial_with_diffuser(
+    obj_id="41083",
+    model=None,
+    n_step=30,
+    gui=False,
+    all_joint=False,
+    website=False,
+    available_joints=None,
+    sgp=True,
+    consistency_check=False,
+    analysis=False,
+):
+    # pm_dir = os.path.expanduser("~/datasets/partnet-mobility/raw")
+    pm_dir = os.path.expanduser("~/datasets/partnet-mobility/convex")
+    # env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+    raw_data = PMObject(os.path.join(pm_dir, obj_id))
+
+    if available_joints is None:  # Use the passed in joint sets
+        available_joints = raw_data.semantics.by_type(
+            "hinge"
+        ) + raw_data.semantics.by_type("slider")
+        available_joints = [joint.name for joint in available_joints]
+
+    print("available_joints:", available_joints)
+    if all_joint:  # Need to traverse all the joints
+        picked_joints = available_joints
+    else:
+        picked_joints = [available_joints[np.random.randint(0, len(available_joints))]]
+
+    sim_trajectories = []
+    results = []
+    figs = {}
+    for joint_name in picked_joints:
+        # t0 = time.perf_counter()
+        # print(f"opening {joint.name}, {joint.label}")
+        print(f"opening {joint_name}")
+        env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+
+        # Close all joints:
+        for link_to_restore in [
+            joint.name
+            for joint in raw_data.semantics.by_type("hinge")
+            + raw_data.semantics.by_type("slider")
+        ]:
+            info = p.getJointInfo(
+                env.render_env.obj_id,
+                env.render_env.link_name_to_index[link_to_restore],
+                env.render_env.client_id,
+            )
+            init_angle, target_angle = info[8], info[9]
+            env.set_joint_state(link_to_restore, init_angle)
+
+        # gt_model = GTFlowModel(raw_data, env)
+        fig, result, sim_trajectory = run_trial(
+            env,
+            raw_data,
+            joint_name,
+            model,
+            gt_model=None,  # Don't need mask
+            n_steps=n_step,
+            save_name=f"{obj_id}_{joint_name}",
+            website=website,
+            gui=gui,
+            sgp=sgp,
+            consistency_check=consistency_check,
+            analysis=analysis,
+        )
+        sim_trajectories.append(sim_trajectory)
+        if result.assertion is False:
+            with open(
+                "/home/yishu/flowbothd/logs/assertion_failure.txt", "a"
+            ) as f:
+                f.write(f"Object: {obj_id}; Joint: {joint_name}\n")
+            continue
+        if result.contact is False:
+            continue
+        figs[joint_name] = fig
+        results.append(result)
+
+    return figs, results, sim_trajectories
+
+
+def trial_with_diffuser_history(  # Always use sgp.... (Have already proved it's effect)
+    obj_id="41083",
+    model=None,
+    history_model=None,
+    n_step=30,
+    gui=False,
+    all_joint=False,
+    website=False,
+    available_joints=None,
+    sgp=True,
+    consistency_check=True,
+    history_filter=True,
+    analysis=False,
+):
+    # pm_dir = os.path.expanduser("~/datasets/partnet-mobility/raw")
+    pm_dir = os.path.expanduser("~/datasets/partnet-mobility/convex")
+    # env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+    raw_data = PMObject(os.path.join(pm_dir, obj_id))
+
+    if available_joints is None:  # Use the passed in joint sets
+        available_joints = raw_data.semantics.by_type(
+            "hinge"
+        ) + raw_data.semantics.by_type("slider")
+        available_joints = [joint.name for joint in available_joints]
+
+    print("available_joints:", available_joints)
+    if all_joint:  # Need to traverse all the joints
+        picked_joints = available_joints
+    else:
+        picked_joints = [available_joints[np.random.randint(0, len(available_joints))]]
+
+    sim_trajectories = []
+    results = []
+    figs = {}
+    for joint_name in picked_joints:
+        # t0 = time.perf_counter()
+        # print(f"opening {joint.name}, {joint.label}")
+        print(f"opening {joint_name}")
+        env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+
+        # Close all joints:
+        for link_to_restore in [
+            joint.name
+            for joint in raw_data.semantics.by_type("hinge")
+            + raw_data.semantics.by_type("slider")
+        ]:
+            info = p.getJointInfo(
+                env.render_env.obj_id,
+                env.render_env.link_name_to_index[link_to_restore],
+                env.render_env.client_id,
+            )
+            init_angle, target_angle = info[8], info[9]
+            env.set_joint_state(link_to_restore, init_angle)
+
+        # gt_model = GTFlowModel(raw_data, env)
+        # fig, result, sim_trajectory = run_trial_with_history(
+        fig, result, sim_trajectory = run_trial_with_history_filter(
+            env,
+            raw_data,
+            joint_name,
+            model,
+            history_model,
+            gt_model=None,  # Don't need mask
+            n_steps=n_step,
+            save_name=f"{obj_id}_{joint_name}",
+            website=website,
+            gui=gui,
+            consistency_check=consistency_check,
+            history_filter=history_filter,
+            analysis=analysis,
+        )
+        sim_trajectories.append(sim_trajectory)
+        if result.assertion is False:
+            with open(
+                "/home/yishu/flowbothd/logs/assertion_failure.txt", "a"
+            ) as f:
+                f.write(f"Object: {obj_id}; Joint: {joint_name}\n")
+            continue
+        if result.contact is False:
+            continue
+        figs[joint_name] = fig
+        results.append(result)
+
+    return figs, results, sim_trajectories
+
+
+def trial_with_switch_models(
+    obj_id="41083",
+    model=None,
+    switch_model=None,
+    history_for_models=[False, False],
+    n_step=30,
+    gui=False,
+    all_joint=False,
+    website=False,
+    available_joints=None,
+    return_switch_ids=False,
+):
+    # pm_dir = os.path.expanduser("~/datasets/partnet-mobility/raw")
+    pm_dir = os.path.expanduser("~/datasets/partnet-mobility/convex")
+    # env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+    raw_data = PMObject(os.path.join(pm_dir, obj_id))
+
+    if available_joints is None:  # Use the passed in joint sets
+        available_joints = raw_data.semantics.by_type(
+            "hinge"
+        ) + raw_data.semantics.by_type("slider")
+        available_joints = [joint.name for joint in available_joints]
+
+    print("available_joints:", available_joints)
+    if all_joint:  # Need to traverse all the joints
+        picked_joints = available_joints
+    else:
+        picked_joints = [available_joints[np.random.randint(0, len(available_joints))]]
+
+    sim_trajectories = []
+    results = []
+    figs = {}
+    for joint_name in picked_joints:
+        # t0 = time.perf_counter()
+        # print(f"opening {joint.name}, {joint.label}")
+        print(f"opening {joint_name}")
+        env = PMSuctionSim(obj_id, pm_dir, gui=gui)
+
+        # Close all joints:
+        for link_to_restore in [
+            joint.name
+            for joint in raw_data.semantics.by_type("hinge")
+            + raw_data.semantics.by_type("slider")
+        ]:
+            info = p.getJointInfo(
+                env.render_env.obj_id,
+                env.render_env.link_name_to_index[link_to_restore],
+                env.render_env.client_id,
+            )
+            init_angle, target_angle = info[8], info[9]
+            env.set_joint_state(link_to_restore, init_angle)
+
+        # gt_model = GTFlowModel(raw_data, env)
+        fig, result, sim_trajectory = run_trial_with_switch_models(
+            env,
+            raw_data,
+            joint_name,
+            model,
+            switch_model,
+            history_for_models,
+            gt_model=None,  # Don't need mask
+            n_steps=n_step,
+            save_name=f"{obj_id}_{joint_name}",
+            website=website,
+            gui=gui,
+            return_switch_ids=return_switch_ids,
+        )
+        sim_trajectories.append(sim_trajectory)
+        if result.assertion is False:
+            with open(
+                "/home/yishu/flowbothd/logs/assertion_failure.txt", "a"
+            ) as f:
+                f.write(f"Object: {obj_id}; Joint: {joint_name}\n")
+            continue
+        if result.contact is False:
+            continue
+        figs[joint_name] = fig
+        results.append(result)
+
+    return figs, results, sim_trajectories
+
+
+if __name__ == "__main__":
+    np.random.seed(2003)
+    torch.manual_seed(2003)
+    # trial_flow(obj_id="41083", available_joints=["link_0"], gui=True, website=False)
+    # trial_gt_trajectory(obj_id="8877", traj_len=3, available_joints=['link_2'], gui=False, website=True)
+    # breakpoint()
+    # trial_with_prediction(obj_id="35059", traj_len=15, n_step=1, gui=True)
+
+    # length = 15
+    # network_15 = create_network(
+    #     traj_len=15,
+    #     ckpt_file="/home/yishu/flowbothd/scripts/logs/train_flowbot/2023-07-19/14-51-22/checkpoints/epoch=94-step=74670-val_loss=0.00-weights-only.ckpt",
+    # )
+
+    # # length = 1
+    # network_1 = pnp.PN2Dense(
+    #     in_channels=0,
+    #     out_channels=3,
+    #     p=pnp.PN2DenseParams(),
+    # )
+    # ckpt = torch.load("/home/yishu/flowbothd/pretrained/fullset_half_half_flowbot.ckpt")
+    # network_1.load_state_dict(
+    #     {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
+    # )
+    # network_1.eval()
+    # # network_1.load_state_dict(torch.load()["state_dict"])
+    # trial_figs, trial_results, sim_trajectory = trial_with_prediction(
+    #     obj_id="102358", network=network_1, n_step=30, gui=False, website=True, all_joint=True
+    # )
+    # print(trial_results)
+
+    # figs[list(figs.keys())[0]].show()
+    # trial_with_prediction(obj_id="35059", network=network_15, n_step=1, gui=False, all_joint=False)
+
+    # # Trial with dit
+    # from flowbothd.models.modules.dit_models import DiT
+
+    # torch.set_printoptions(precision=10)  # Set higher precision for PyTorch outputs
+    # np.set_printoptions(precision=10)
+
+    # network = DiT(
+    #     in_channels=3 + 3,
+    #     depth=5,
+    #     hidden_size=128,
+    #     num_heads=4,
+    #     # depth=12,
+    #     # hidden_size=384,
+    #     # num_heads=6,
+    #     learn_sigma=True,
+    # ).cuda()
+    # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_dit/2024-03-30/07-12-41/checkpoints/epoch=359-step=199080-val_loss=0.00-weights-only.ckpt"
+    # # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_dit/2024-05-02/12-35-27/checkpoints/epoch=109-step=243100-val_loss=0.00-weights-only.ckpt"
+    from hydra import compose, initialize
+
+    initialize(config_path="../../../configs", version_base="1.3")
+    cfg = compose(config_name="eval_sim")
+
+    # from flowbothd.models.flow_diffuser_dit import (
+    #     FlowTrajectoryDiffuserSimulationModule_DiT,
+    # )
+
+    # model = FlowTrajectoryDiffuserSimulationModule_DiT(
+    #     network, inference_cfg=cfg.inference, model_cfg=cfg.model
+    # ).cuda()
+    # model.load_from_ckpt(ckpt_file)
+    # model.eval()
+
+    from flowbothd.models.flow_diffuser_hispndit import (
+        FlowTrajectoryDiffuserSimulationModule_HisPNDiT,
+    )
+    from flowbothd.models.modules.dit_models import PN2HisDiT
+    from flowbothd.models.modules.history_encoder import HistoryEncoder
+
+    # History model
+    network = {
+        "DiT": PN2HisDiT(
+            history_embed_dim=128,
+            in_channels=3,
+            depth=5,
+            hidden_size=128,
+            num_heads=4,
+            # depth=8,
+            # hidden_size=256,
+            # num_heads=4,
+            learn_sigma=True,
+        ).cuda(),
+        "History": HistoryEncoder(
+            history_dim=128,
+            history_len=1,
+            batch_norm=True,
+            transformer=False,
+            repeat_dim=False,
+        ).cuda(),
+    }
+
+    # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_hisdit/2024-05-10/12-09-08/checkpoints/epoch=439-step=243320-val_loss=0.00-weights-only.ckpt"
+    ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_hispndit/2024-05-25/02-00-54/checkpoints/epoch=299-step=248700-val_loss=0.00-weights-only-backup.ckpt"
+    history_model = FlowTrajectoryDiffuserSimulationModule_HisPNDiT(
+        network, inference_cfg=cfg.inference, model_cfg=cfg.model
+    ).cuda()
+    history_model.load_from_ckpt(ckpt_file)
+    history_model.eval()
+
+    # import rpad.pyg.nets.pointnet2 as pnp_orig
+
+    # from flowbothd.models.flow_trajectory_predictor import (
+    #     FlowSimulationInferenceModule,
+    # )
+
+    # network = pnp_orig.PN2Dense(
+    #     in_channels=0,
+    #     out_channels=3,
+    #     p=pnp_orig.PN2DenseParams(),
+    # )#.cuda()
+    # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_pn++/2024-05-26/02-37-08/checkpoints/epoch=98-step=109395-val_loss=0.00-weights-only.ckpt"
+    # ckpt = torch.load(ckpt_file)
+    # model = FlowSimulationInferenceModule(
+    #     network, cfg.inference, cfg.model
+    # )
+
+    # trial_figs, trial_results, sim_trajectory = trial_with_prediction(
+    #     obj_id="8877", network=model, n_step=30, gui=False, website=True, available_joints=["link_1"], all_joint=False
+    # )
+    # breakpoint()
+
+    # switch_model = FlowSimulationInferenceModule(
+    #     network, cfg.switch_inference, cfg.switch_model
+    # ).cuda()
+    # # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_pn++/2024-03-30/08-16-05/checkpoints/epoch=88-step=98345-val_loss=0.00-weights-only.ckpt"
+    # # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_pn++/2024-05-25/04-17-41/checkpoints/epoch=95-step=53088-val_loss=0.00-weights-only.ckpt"
+    # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_pn++/2024-05-26/02-37-08/checkpoints/epoch=98-step=109395-val_loss=0.00-weights-only.ckpt"
+    # switch_model.load_from_ckpt(ckpt_file)
+    # switch_model.eval()
+
+    obj_id = "8877"  # 8877
+    # trial_figs, trial_results, sim_trajectory = trial_with_diffuser(
+    # trial_figs, trial_results, sim_trajectory = trial_with_switch_models(
+    trial_figs, trial_results, sim_trajectory = trial_with_diffuser_history(
+        # obj_id="8877",
+        obj_id=obj_id,
+        # model=model,
+        # switch_model=switch_model,
+        model=history_model,
+        history_model=history_model,
+        # history_for_models=[False, False],
+        n_step=30,
+        gui=False,
+        website=cfg.website,
+        all_joint=False,
+        available_joints=["link_1"],
+        # return_switch_ids=True,
+    )
+
+    # x = [i for i in range(31)]
+    # y, colors = sim_trajectory[0]
+    # colors = ["red" if color else "blue" for color in colors[1:]]
+
+    # import matplotlib.pyplot as plt
+
+    # plt.figure(figsize=(10, 6))
+    # for i in range(len(x) - 1):
+    #     plt.plot(x[i : i + 2], y[i : i + 2], color=colors[i])
+
+    # plt.xlabel("Step")
+    # plt.yticks(np.linspace(0, 1, 11))
+    # plt.ylabel("Open ratio")
+    # plt.title(f"DiT & FowBot - Door {obj_id}")
+    # plt.savefig(
+    #     f"/home/yishu/flowbothd/notebooks/analysis/traj_visuals/{obj_id}_dit&flowbot.jpg"
+    # )
+    breakpoint()
diff --git a/src/flowbothd/simulations/suction.py b/src/flowbothd/simulations/suction.py
new file mode 100644
index 0000000..96ead10
--- /dev/null
+++ b/src/flowbothd/simulations/suction.py
@@ -0,0 +1,2566 @@
+import copy
+import time
+from dataclasses import dataclass
+from typing import Optional
+
+import imageio
+import numpy as np
+import pybullet as p
+import torch
+from flowbot3d.datasets.flow_dataset import compute_normalized_flow
+from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation
+from rpad.partnet_mobility_utils.data import PMObject
+from rpad.partnet_mobility_utils.render.pybullet import PMRenderEnv
+from rpad.pybullet_envs.suction_gripper import FloatingSuctionGripper
+from scipy.spatial.transform import Rotation as R
+
+from flowbothd.datasets.flow_trajectory_dataset import (
+    compute_flow_trajectory,
+)
+from flowbothd.metrics.trajectory import normalize_trajectory
+
+
+class PMSuctionSim:
+    def __init__(self, obj_id: str, dataset_path: str, gui: bool = False):
+        self.render_env = PMRenderEnv(obj_id=obj_id, dataset_path=dataset_path, gui=gui)
+        self.gui = gui
+        self.gripper = FloatingSuctionGripper(self.render_env.client_id)
+        self.gripper.set_pose(
+            [-1, 0.6, 0.8], p.getQuaternionFromEuler([0, np.pi / 2, 0])
+        )
+        self.writer = None
+
+    # def run_demo(self):
+    #     while True:
+    #         self.gripper.set_velocity([0.4, 0, 0.0], [0, 0, 0])
+    #         for i in range(10):
+    #             p.stepSimulation(self.render_env.client_id)
+    #             time.sleep(1 / 240.0)
+    #         contact = self.gripper.detect_contact()
+    #         if contact:
+    #             break
+
+    #     print("stopping gripper")
+
+    #     self.gripper.set_velocity([0.001, 0, 0.0], [0, 0, 0])
+    #     for i in range(10):
+    #         p.stepSimulation(self.render_env.client_id)
+    #         time.sleep(1 / 240.0)
+    #         contact = self.gripper.detect_contact()
+    #         print(contact)
+
+    #     print("starting activation")
+
+    #     self.gripper.activate()
+
+    #     self.gripper.set_velocity([0, 0, 0.0], [0, 0, 0])
+    #     for i in range(100):
+    #         p.stepSimulation(self.render_env.client_id)
+    #         time.sleep(1 / 240.0)
+
+    #     # print("releasing")
+    #     # self.gripper.release()
+
+    #     print("starting motion")
+    #     for i in range(100):
+    #         p.stepSimulation(self.render_env.client_id)
+    #         time.sleep(1 / 240.0)
+
+    #     for _ in range(20):
+    #         for i in range(100):
+    #             self.gripper.set_velocity([-0.4, 0, 0.0], [0, 0, 0])
+    #             self.gripper.apply_force([-500, 0, 0])
+    #             p.stepSimulation(self.render_env.client_id)
+    #             time.sleep(1 / 240.0)
+
+    #         for i in range(100):
+    #             self.gripper.set_velocity([-0.4, 0, 0.0], [0, 0, 0])
+    #             self.gripper.apply_force([-500, 0, 0])
+    #             p.stepSimulation(self.render_env.client_id)
+    #             time.sleep(1 / 240.0)
+
+    #     print("releasing")
+    #     self.gripper.release()
+
+    #     for i in range(1000):
+    #         p.stepSimulation(self.render_env.client_id)
+    #         time.sleep(1 / 240.0)
+
+    def reset(self):
+        pass
+
+    def set_writer(self, writer):
+        self.writer = writer
+
+    def reset_gripper(self, target_link):
+        # print(self.gripper.contact_const)
+        curr_pos = self.get_joint_value(target_link)
+        self.gripper.release()
+        self.gripper.set_pose(
+            [-1, 0.6, 0.8], p.getQuaternionFromEuler([0, np.pi / 2, 0])
+        )
+        self.gripper.set_velocity([0, 0, 0], [0, 0, 0])
+        self.set_joint_state(target_link, curr_pos)
+
+    def set_gripper_pose(self, pos, ori):
+        self.gripper.set_pose(pos, ori)
+
+    def set_joint_state(self, link_name: str, value: float):
+        p.resetJointState(
+            self.render_env.obj_id,
+            self.render_env.link_name_to_index[link_name],
+            value,
+            0.0,
+            self.render_env.client_id,
+        )
+
+    def render(self, filter_nonobj_pts: bool = False, n_pts: Optional[int] = None):
+        output = self.render_env.render()
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = output
+
+        if filter_nonobj_pts:
+            pc_seg_obj = np.ones_like(pc_seg) * -1
+            for k, (body, link) in segmap.items():
+                if body == self.render_env.obj_id:
+                    ixs = pc_seg == k
+                    pc_seg_obj[ixs] = link
+
+            is_obj = pc_seg_obj != -1
+            P_cam = P_cam[is_obj]
+            P_world = P_world[is_obj]
+            pc_seg = pc_seg_obj[is_obj]
+        if n_pts is not None:
+            perm = np.random.permutation(len(P_world))[:n_pts]
+            P_cam = P_cam[perm]
+            P_world = P_world[perm]
+            pc_seg = pc_seg[perm]
+
+        return rgb, depth, seg, P_cam, P_world, pc_seg, segmap
+
+    def set_camera(self):
+        pass
+
+    def teleport_and_approach(
+        self, point, contact_vector, video_writer=None, standoff_d: float = 0.2
+    ):
+        # Normalize contact vector.
+        contact_vector = (contact_vector / contact_vector.norm(dim=-1)).float()
+
+        p_teleport = (torch.from_numpy(point) + contact_vector * standoff_d).float()
+
+        # breakpoint()
+
+        e_z_init = torch.tensor([0, 0, 1.0]).float()
+        e_y = -contact_vector
+        e_x = torch.cross(-contact_vector, e_z_init)
+        e_x = e_x / e_x.norm(dim=-1)
+        e_z = torch.cross(e_x, e_y)
+        e_z = e_z / e_z.norm(dim=-1)
+        R_teleport = torch.stack([e_x, e_y, e_z], dim=1)
+        R_gripper = torch.as_tensor(
+            [
+                [1, 0, 0],
+                [0, 0, 1.0],
+                [0, -1.0, 0],
+            ]
+        )
+        # breakpoint()
+        o_teleport = R.from_matrix(R_teleport @ R_gripper).as_quat()
+
+        self.gripper.set_pose(p_teleport, o_teleport)
+
+        contact = self.gripper.detect_contact(self.render_env.obj_id)
+        max_steps = 500
+        curr_steps = 0
+        self.gripper.set_velocity(-contact_vector * 0.4, [0, 0, 0])
+        while not contact and curr_steps < max_steps:
+            p.stepSimulation(self.render_env.client_id)
+
+            if video_writer is not None and curr_steps % 50 == 49:
+                # if video_writer is not None:
+                frame_width = 640
+                frame_height = 480
+                width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+                    width=frame_width,
+                    height=frame_height,
+                    viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+                        cameraTargetPosition=[0, 0, 0],
+                        distance=5,
+                        yaw=270,
+                        # distance=3,
+                        # yaw=180,
+                        pitch=-30,
+                        roll=0,
+                        upAxisIndex=2,
+                    ),
+                    projectionMatrix=p.computeProjectionMatrixFOV(
+                        fov=60,
+                        aspect=float(frame_width) / frame_height,
+                        nearVal=0.1,
+                        farVal=100.0,
+                    ),
+                )
+                image = np.array(rgbImg, dtype=np.uint8)
+                image = image[:, :, :3]
+
+                # Add the frame to the video
+                video_writer.append_data(image)
+
+            curr_steps += 1
+            if self.gui:
+                time.sleep(1 / 240.0)
+            if curr_steps % 1 == 0:
+                contact = self.gripper.detect_contact(self.render_env.obj_id)
+
+        # Give it another chance
+        if contact:
+            print("contact detected")
+
+        return contact
+
+    def teleport(
+        self,
+        points,
+        contact_vectors,
+        video_writer=None,
+        standoff_d: float = 0.2,
+        target_link=None,
+    ):
+        # p.setTimeStep(1.0/240)
+        for id, (point, contact_vector) in enumerate(zip(points, contact_vectors)):
+            # Normalize contact vector.
+            contact_vector = (contact_vector / contact_vector.norm(dim=-1)).float()
+            p_teleport = (torch.from_numpy(point) + contact_vector * standoff_d).float()
+            # print(p_teleport)
+            e_z_init = torch.tensor([0, 0, 1.0]).float()
+            e_y = -contact_vector
+            e_x = torch.cross(-contact_vector, e_z_init)
+            e_x = e_x / e_x.norm(dim=-1)
+            e_z = torch.cross(e_x, e_y)
+            e_z = e_z / e_z.norm(dim=-1)
+            R_teleport = torch.stack([e_x, e_y, e_z], dim=1)
+            R_gripper = torch.as_tensor(
+                [
+                    [1, 0, 0],
+                    [0, 0, 1.0],
+                    [0, -1.0, 0],
+                ]
+            )
+            o_teleport = R.from_matrix(R_teleport @ R_gripper).as_quat()
+            self.gripper.set_pose(p_teleport, o_teleport)
+
+            contact = self.gripper.detect_contact(self.render_env.obj_id)
+            max_steps = 500
+            curr_steps = 0
+            # self.gripper.set_velocity(-contact_vector * 0.4, [0, 0, 0])
+            while not contact and curr_steps < max_steps:
+                self.gripper.set_velocity(-contact_vector * 0.4, [0, 0, 0])
+                p.stepSimulation(self.render_env.client_id)
+                # print(point, p.getBasePositionAndOrientation(self.gripper.body_id),p.getBasePositionAndOrientation(self.gripper.base_id))
+                # if video_writer is not None and curr_steps % 50 == 49:
+                if video_writer is not None and False:  # Don't save this
+                    # if video_writer is not None and True:
+                    # if video_writer is not None:
+                    # for i in range(10 if curr_steps == max_steps - 1 else 1):
+                    frame_width = 640
+                    frame_height = 480
+                    width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+                        width=frame_width,
+                        height=frame_height,
+                        viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+                            cameraTargetPosition=[0, 0, 0],
+                            distance=5,
+                            yaw=270,
+                            # yaw=90,
+                            pitch=-30,
+                            roll=0,
+                            upAxisIndex=2,
+                        ),
+                        projectionMatrix=p.computeProjectionMatrixFOV(
+                            fov=60,
+                            aspect=float(frame_width) / frame_height,
+                            nearVal=0.1,
+                            farVal=100.0,
+                        ),
+                    )
+                    image = np.array(rgbImg, dtype=np.uint8)
+                    image = image[:, :, :3]
+                    # if curr_steps == 0:
+                    #     cv2.imwrite('/home/yishu/flowbothd/src/flowbothd/simulations/logs/simu_eval/video_assets/static_frames/reset_gripper.jpg', image)
+                    # if curr_steps == max_steps - 1:
+                    #     cv2.imwrite('/home/yishu/flowbothd/src/flowbothd/simulations/logs/simu_eval/video_assets/static_frames/attach_gripper.jpg', image)
+                    # Add the frame to the video
+                    video_writer.append_data(image)
+
+                # if target_link is not None:
+                #     print("DEBUG whether the joint moved when approaching...")
+                #     link_index = self.render_env.link_name_to_index[target_link]
+                #     curr_pos = self.get_joint_value(target_link)
+                #     print("Current pos:", curr_pos)
+
+                curr_steps += 1
+                if self.gui:
+                    time.sleep(1 / 240.0)
+                if curr_steps % 1 == 0:
+                    contact = self.gripper.detect_contact(self.render_env.obj_id)
+
+            # Give it another chance
+            if contact:
+                print("contact detected")
+                return id, True
+
+        return -1, False
+
+    def attach(self):
+        self.gripper.activate(self.render_env.obj_id)
+
+    def pull(self, direction, n_steps: int = 100):
+        direction = torch.as_tensor(direction)
+        direction = direction / direction.norm(dim=-1)
+        # breakpoint()
+        for _ in range(n_steps):
+            self.gripper.set_velocity(direction * 0.4, [0, 0, 0])
+            p.stepSimulation(self.render_env.client_id)
+            if self.gui:
+                time.sleep(1 / 240.0)
+
+        return False
+
+    def pull_with_constraint(
+        self, direction, n_steps: int = 100, target_link: str = "", constraint=True
+    ):
+        if not constraint:
+            return self.pull(direction, n_steps)
+        # Link info
+        link_index = self.render_env.link_name_to_index[target_link]
+        info = p.getJointInfo(
+            self.render_env.obj_id, link_index, self.render_env.client_id
+        )
+        lower, upper = info[8], info[9]
+
+        direction = torch.as_tensor(direction)
+        direction = direction / (direction.norm(dim=-1) + 1e-12)
+        for _ in range(n_steps):
+            self.gripper.set_velocity(direction * 0.4, [0, 0, 0])
+            p.stepSimulation(self.render_env.client_id)
+
+            # frame_width = 640
+            # frame_height = 480
+            # width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+            #     width=frame_width,
+            #     height=frame_height,
+            #     viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+            #         cameraTargetPosition=[0, 0, 0],
+            #         distance=5,
+            #         yaw=270,
+            #         # yaw=90,
+            #         pitch=-30,
+            #         roll=0,
+            #         upAxisIndex=2,
+            #     ),
+            #     projectionMatrix=p.computeProjectionMatrixFOV(
+            #         fov=60,
+            #         aspect=float(frame_width) / frame_height,
+            #         nearVal=0.1,
+            #         farVal=100.0,
+            #     ),
+            # )
+            # image = np.array(rgbImg, dtype=np.uint8)
+            # image = image[:, :, :3]
+            # # Add the frame to the video
+            # self.writer.append_data(image)
+            # print("Current !: ", self.get_joint_value(target_link))
+
+            if self.gui:
+                time.sleep(1 / 240.0)
+
+        # Check if the object is below initial_angle
+        curr_pos = self.get_joint_value(target_link)
+        if curr_pos < lower < upper or curr_pos > lower > upper:
+            print(curr_pos, lower)
+            p.resetJointState(
+                self.render_env.obj_id, link_index, lower, 0, self.render_env.client_id
+            )
+            return True  # Need a reset
+
+        return False  # Don't need reset
+
+    def get_joint_value(self, target_link: str):
+        link_index = self.render_env.link_name_to_index[target_link]
+        state = p.getJointState(
+            self.render_env.obj_id, link_index, self.render_env.client_id
+        )
+        joint_pos = state[0]
+        return joint_pos
+
+    def detect_success(self, target_link: str):
+        link_index = self.render_env.link_name_to_index[target_link]
+        info = p.getJointInfo(
+            self.render_env.obj_id, link_index, self.render_env.client_id
+        )
+        lower, upper = info[8], info[9]
+        curr_pos = self.get_joint_value(target_link)
+
+        sign = -1 if upper < lower else 1
+        print(
+            f"lower: {lower}, upper: {upper}, curr: {curr_pos}, success:{(upper - curr_pos) / (upper - lower) < 0.1}"
+        )
+
+        return (upper - curr_pos) / (upper - lower) < 0.1, (curr_pos - lower) / (
+            upper - lower
+        )
+
+    def randomize_joints(self):
+        for i in range(
+            p.getNumJoints(self.render_env.obj_id, self.render_env.client_id)
+        ):
+            jinfo = p.getJointInfo(self.render_env.obj_id, i, self.render_env.client_id)
+            if jinfo[2] == p.JOINT_REVOLUTE or jinfo[2] == p.JOINT_PRISMATIC:
+                lower, upper = jinfo[8], jinfo[9]
+                angle = np.random.random() * (upper - lower) + lower
+                p.resetJointState(
+                    self.render_env.obj_id, i, angle, 0, self.render_env.client_id
+                )
+
+    def randomize_specific_joints(self, joint_list):
+        for i in range(
+            p.getNumJoints(self.render_env.obj_id, self.render_env.client_id)
+        ):
+            jinfo = p.getJointInfo(self.render_env.obj_id, i, self.render_env.client_id)
+            if jinfo[12].decode("UTF-8") in joint_list:
+                lower, upper = jinfo[8], jinfo[9]
+                angle = np.random.random() * (upper - lower) + lower
+                p.resetJointState(
+                    self.render_env.obj_id, i, angle, 0, self.render_env.client_id
+                )
+
+    def articulate_specific_joints(self, joint_list, amount):
+        for i in range(
+            p.getNumJoints(self.render_env.obj_id, self.render_env.client_id)
+        ):
+            jinfo = p.getJointInfo(self.render_env.obj_id, i, self.render_env.client_id)
+            if jinfo[12].decode("UTF-8") in joint_list:
+                lower, upper = jinfo[8], jinfo[9]
+                angle = amount * (upper - lower) + lower
+                p.resetJointState(
+                    self.render_env.obj_id, i, angle, 0, self.render_env.client_id
+                )
+
+    def randomize_joints_openclose(self, joint_list):
+        randind = np.random.choice([0, 1])
+        # Close: 0
+        # Open: 1
+        self.close_or_open = randind
+        for i in range(
+            p.getNumJoints(self.render_env.obj_id, self.render_env.client_id)
+        ):
+            jinfo = p.getJointInfo(self.render_env.obj_id, i, self.render_env.client_id)
+            if jinfo[12].decode("UTF-8") in joint_list:
+                lower, upper = jinfo[8], jinfo[9]
+                angles = [lower, upper]
+                angle = angles[randind]
+                p.resetJointState(
+                    self.render_env.obj_id, i, angle, 0, self.render_env.client_id
+                )
+
+
+@dataclass
+class TrialResult:
+    success: bool
+    contact: bool
+    assertion: bool
+    init_angle: float
+    final_angle: float
+    now_angle: float
+
+    # UMPNet metric goes here
+    metric: float
+
+
+class GTFlowModel:
+    def __init__(self, raw_data, env):
+        self.env = env
+        self.raw_data = raw_data
+
+    def __call__(self, obs) -> torch.Tensor:
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = obs
+        env = self.env
+        raw_data = self.raw_data
+
+        links = raw_data.semantics.by_type("slider")
+        links += raw_data.semantics.by_type("hinge")
+        current_jas = {}
+        for link in links:
+            linkname = link.name
+            chain = raw_data.obj.get_chain(linkname)
+            for joint in chain:
+                current_jas[joint.name] = 0
+
+        normalized_flow = compute_normalized_flow(
+            P_world,
+            env.render_env.T_world_base,
+            current_jas,
+            pc_seg,
+            env.render_env.link_name_to_index,
+            raw_data,
+            "all",
+        )
+
+        return torch.from_numpy(normalized_flow)
+
+    def get_movable_mask(self, obs) -> torch.Tensor:
+        flow = self(obs)
+        mask = (~(np.isclose(flow, 0.0)).all(axis=-1)).astype(np.bool_)
+        return mask
+
+
+class GTTrajectoryModel:
+    def __init__(self, raw_data, env, traj_len=20):
+        self.raw_data = raw_data
+        self.env = env
+        self.traj_len = traj_len
+
+    def __call__(self, obs) -> torch.Tensor:
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = obs
+        env = self.env
+        raw_data = self.raw_data
+
+        links = raw_data.semantics.by_type("slider")
+        links += raw_data.semantics.by_type("hinge")
+        current_jas = {}
+        for link in links:
+            linkname = link.name
+            chain = raw_data.obj.get_chain(linkname)
+            for joint in chain:
+                current_jas[joint.name] = 0
+        trajectory, _ = compute_flow_trajectory(
+            self.traj_len,
+            P_world,
+            env.render_env.T_world_base,
+            current_jas,
+            pc_seg,
+            env.render_env.link_name_to_index,
+            raw_data,
+            "all",
+        )
+        return torch.from_numpy(trajectory)
+
+    def get_gt_force_vector(self, obs, link_ixs) -> torch.Tensor:  # Just for debug!!!!
+        pred_flow = self(obs)[link_ixs, 0, :]
+        best_flow_ix = torch.topk(pred_flow.norm(dim=-1), 1)[1]
+        # breakpoint()
+        return pred_flow[best_flow_ix] / pred_flow[best_flow_ix].norm(2)
+
+
+def choose_grasp_points(
+    raw_pred_flow, raw_point_cloud, filter_edge=False, k=40, last_correct_direction=None
+):
+    pred_flow = raw_pred_flow.clone()
+    point_cloud = raw_point_cloud
+    # Choose top k non-edge grasp points:
+    if filter_edge:  # Need to filter the edge points
+        squared_diff = (
+            point_cloud[:, np.newaxis, :] - point_cloud[np.newaxis, :, :]
+        ) ** 2
+        dists = np.sqrt(np.sum(squared_diff, axis=2))
+        dist_thres = np.percentile(dists, 10)
+        neighbour_points = np.sum(dists < dist_thres, axis=0)
+        invalid_points = neighbour_points < np.percentile(
+            neighbour_points, 30
+        )  # Not edge
+        pred_flow[invalid_points] = 0  # Don't choose these edge points!!!!!
+
+    top_k_point = min(k, len(pred_flow))
+    best_flow_ix = torch.topk(pred_flow.norm(dim=-1), top_k_point)[1]
+    if top_k_point == 1:
+        best_flow_ix = torch.tensor(list(best_flow_ix) * 2)
+    best_flow = pred_flow[best_flow_ix]
+    best_point = point_cloud[best_flow_ix]
+
+    if last_correct_direction is None:  # No past direction as filter
+        # print(best_flow_ix.shape, best_flow.shape, best_point.shape)
+        return best_flow_ix, best_flow, best_point
+    else:
+        filtered_best_flow_ix = []
+        filtered_best_flow = []
+        filtered_best_point = []
+        for ix, flow, point in zip(best_flow_ix, best_flow, best_point):
+            # if np.dot(flow, last_correct_direction) > 0:  # angle < 90
+            if (
+                np.dot(
+                    flow / (np.linalg.norm(flow) + 1e-12),
+                    last_correct_direction
+                    / (np.linalg.norm(last_correct_direction) + 1e-12),
+                )
+                > 0.80
+            ):  # angle < 60
+                # print("last correct_direction: ", last_correct_direction / np.linalg.norm(last_correct_direction))
+                # print("good prediction:", ix, flow, point, np.dot(flow / np.linalg.norm(flow), last_correct_direction / np.linalg.norm(last_correct_direction)))
+                filtered_best_flow_ix.append(ix)
+                filtered_best_flow.append(flow)
+                filtered_best_point.append(point)
+
+        if len(filtered_best_flow) == 0:
+            return [], [], []
+        return (
+            torch.stack(filtered_best_flow_ix),
+            torch.stack(filtered_best_flow),
+            np.array(filtered_best_point),
+        )
+
+
+def choose_grasp_points_density(
+    raw_pred_flow, raw_point_cloud, k=40, last_correct_direction=None
+):
+    pred_flow = raw_pred_flow.clone()
+    point_cloud = raw_point_cloud
+
+    flow_norms = pred_flow.norm(dim=-1)
+    point_density = flow_norms / flow_norms.sum()
+    # breakpoint()
+    choices = np.arange(0, len(point_density))
+
+    top_k_point = min(k, len(pred_flow))
+    # best_flow_ix = torch.topk(pred_flow.norm(dim=-1), top_k_point)[1]
+    best_flow_ix = np.random.choice(
+        choices, size=top_k_point, replace=False, p=point_density.numpy()
+    )
+    best_flow_ix = torch.from_numpy(best_flow_ix)
+    if top_k_point == 1:
+        best_flow_ix = torch.tensor(list(best_flow_ix) * 2)
+    best_flow = pred_flow[best_flow_ix]
+    best_point = point_cloud[best_flow_ix]
+
+    if last_correct_direction is None:  # No past direction as filter
+        # print(best_flow_ix.shape, best_flow.shape, best_point.shape)
+        return best_flow_ix, best_flow, best_point
+    else:
+        filtered_best_flow_ix = []
+        filtered_best_flow = []
+        filtered_best_point = []
+        for ix, flow, point in zip(best_flow_ix, best_flow, best_point):
+            # if np.dot(flow, last_correct_direction) > 0:  # angle < 90
+            if (
+                np.dot(
+                    flow / (np.linalg.norm(flow) + 1e-12),
+                    last_correct_direction
+                    / (np.linalg.norm(last_correct_direction) + 1e-12),
+                )
+                > 0.80
+            ):  # angle < 60
+                # print("last correct_direction: ", last_correct_direction / np.linalg.norm(last_correct_direction))
+                # print("good prediction:", ix, flow, point, np.dot(flow / np.linalg.norm(flow), last_correct_direction / np.linalg.norm(last_correct_direction)))
+                filtered_best_flow_ix.append(ix)
+                filtered_best_flow.append(flow)
+                filtered_best_point.append(point)
+
+        if len(filtered_best_flow) == 0:
+            return [], [], []
+        return (
+            torch.stack(filtered_best_flow_ix),
+            torch.stack(filtered_best_flow),
+            np.array(filtered_best_point),
+        )
+
+
+def get_local_point(object_id, link_index, world_point):
+    if link_index == -1:
+        # Base link (root link)
+        position, orientation = p.getBasePositionAndOrientation(object_id)
+    else:
+        # Specific link
+        link_state = p.getLinkState(object_id, link_index)
+        position = link_state[4]  # Link world position
+        orientation = link_state[5]  # Link world orientation
+
+    # Convert orientation to a rotation matrix
+    rotation_matrix = p.getMatrixFromQuaternion(orientation)
+    rotation_matrix = np.array(rotation_matrix).reshape(3, 3)
+
+    # Transform the world point to local coordinates
+    local_point = np.dot(
+        np.linalg.inv(rotation_matrix), (world_point - np.array(position))
+    )
+    return local_point
+
+
+def get_world_point(object_id, link_index, local_point):
+    if link_index == -1:
+        # Base link (root link)
+        position, orientation = p.getBasePositionAndOrientation(object_id)
+    else:
+        # Specific link
+        link_state = p.getLinkState(object_id, link_index)
+        position = link_state[4]  # Link world position
+        orientation = link_state[5]  # Link world orientation
+
+    # Convert orientation to a rotation matrix
+    rotation_matrix = p.getMatrixFromQuaternion(orientation)
+    rotation_matrix = np.array(rotation_matrix).reshape(3, 3)
+
+    # Transform the local point to world coordinates
+    world_point = np.dot(rotation_matrix, local_point) + np.array(position)
+    return world_point
+
+
+def run_trial(
+    env: PMSuctionSim,
+    raw_data: PMObject,
+    target_link: str,
+    model,
+    gt_model=None,  # When we use mask_input_channel=True, this is the mask generator
+    n_steps: int = 30,
+    n_pts: int = 1200,
+    save_name: str = "unknown",
+    website: bool = False,
+    gui: bool = False,
+    sgp: bool = True,
+    consistency_check: bool = False,
+    analysis: bool = False,
+) -> TrialResult:
+    torch.manual_seed(42)
+    torch.set_printoptions(precision=10)  # Set higher precision for PyTorch outputs
+    np.set_printoptions(precision=10)
+    # p.setPhysicsEngineParameter(numSolverIterations=10)
+    # p.setPhysicsEngineParameter(contactBreakingThreshold=0.01, contactSlop=0.001)
+
+    initial_movement_thres = 1e-6
+    good_movement_thres = 0.01
+    max_trial_per_step = 50
+    this_step_trial = 0
+
+    sim_trajectory = [0.0] + [0] * (n_steps)  # start from 0.05
+    correct_direction_stack = []  # The direction stack
+
+    # For analysis:
+    sgp_signals = [1]  # Record the steps which we switched grasp point
+
+    # For website demo
+    if analysis:
+        visual_all_points = []
+        visual_link_ixs = []
+        visual_grasp_points_idx = []
+        visual_grasp_points = []
+        visual_flows = []
+
+    if website:
+        # Flow animation
+        animation = FlowNetAnimation()
+
+    # First, reset the environment.
+    env.reset()
+    # Joint information
+    info = p.getJointInfo(
+        env.render_env.obj_id,
+        env.render_env.link_name_to_index[target_link],
+        env.render_env.client_id,
+    )
+    init_angle, target_angle = info[8], info[9]
+
+    # Sometimes doors collide with themselves. It's dumb.
+    if (
+        raw_data.category == "Door"
+        and raw_data.semantics.by_name(target_link).type == "hinge"
+    ):
+        env.set_joint_state(target_link, init_angle + 0.0 * (target_angle - init_angle))
+        # env.set_joint_state(target_link, 0.2)
+
+    if raw_data.semantics.by_name(target_link).type == "hinge":
+        env.set_joint_state(target_link, init_angle + 0.0 * (target_angle - init_angle))
+        # env.set_joint_state(target_link, 0.05)
+
+    # Predict the flow on the observation.
+    pc_obs = env.render(filter_nonobj_pts=True, n_pts=n_pts)
+    rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs
+
+    if init_angle == target_angle:  # Not movable
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        return (
+            None,
+            TrialResult(
+                success=False,
+                assertion=False,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory,
+        )
+
+    # breakpoint()
+    if gt_model is None:  # GT Flow model
+        pred_trajectory = model(copy.deepcopy(pc_obs))
+    else:
+        movable_mask = gt_model.get_movable_mask(pc_obs)
+        pred_trajectory = model(copy.deepcopy(pc_obs), movable_mask)
+    # pred_trajectory = model(copy.deepcopy(pc_obs))
+    # breakpoint()
+    pred_trajectory = pred_trajectory.reshape(
+        pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]
+    )
+    traj_len = pred_trajectory.shape[1]  # Trajectory length
+    print(f"Predicting {traj_len} length trajectories.")
+    pred_flow = pred_trajectory[:, 0, :]
+
+    # flow_fig(torch.from_numpy(P_world), pred_flow, sizeref=0.1, use_v2=True).show()
+    # breakpoint()
+
+    # Filter down just the points on the target link.
+    link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]
+    # assert link_ixs.any()
+    if not link_ixs.any():
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        print("link_ixs finds no point")
+        animation_results = animation.animate() if website else None
+        return (
+            animation_results,
+            TrialResult(
+                success=False,
+                assertion=False,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory,
+        )
+
+    if website:
+        if gui:
+            # Record simulation video
+            log_id = p.startStateLogging(
+                p.STATE_LOGGING_VIDEO_MP4,
+                f"./logs/simu_eval/video_assets/{save_name}.mp4",
+            )
+        else:
+            video_file = f"./logs/simu_eval/video_assets/{save_name}.mp4"
+            # # cv2 output videos won't show on website
+            frame_width = 640
+            frame_height = 480
+            # fps = 5
+            # fourcc = cv2.VideoWriter_fourcc(*'mp4v')
+            # videoWriter = cv2.VideoWriter(video_file, fourcc, fps, (frame_width, frame_height))
+            # videoWriter.write(rgbImgOpenCV)
+
+            # Camera param
+            writer = imageio.get_writer(video_file, fps=5)
+
+            # Capture image
+            width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+                width=frame_width,
+                height=frame_height,
+                viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+                    cameraTargetPosition=[0, 0, 0],
+                    distance=5,
+                    # yaw=180,
+                    yaw=270,
+                    # pitch=90,
+                    pitch=-30,
+                    roll=0,
+                    upAxisIndex=2,
+                ),
+                projectionMatrix=p.computeProjectionMatrixFOV(
+                    fov=60,
+                    aspect=float(frame_width) / frame_height,
+                    nearVal=0.1,
+                    farVal=100.0,
+                ),
+            )
+            image = np.array(rgbImg, dtype=np.uint8)
+            image = image[:, :, :3]
+
+            # Add the frame to the video
+            writer.append_data(image)
+
+    # The attachment point is the point with the highest flow.
+    # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()
+    best_flow_ixs, best_flows, best_points = choose_grasp_points(
+        pred_flow[link_ixs], P_world[link_ixs], filter_edge=False, k=20
+    )
+
+    # Teleport to an approach pose, approach, the object and grasp.
+    if website and not gui:
+        # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)
+        best_flow_ix_id, contact = env.teleport(
+            best_points, best_flows, video_writer=writer
+        )
+    else:
+        # contact = env.teleport_and_approach(best_point, best_flow)
+        best_flow_ix_id, contact = env.teleport(best_points, best_flows)
+    best_flow = pred_flow[link_ixs][best_flow_ixs[best_flow_ix_id]]
+    best_point = P_world[link_ixs][best_flow_ixs[best_flow_ix_id]]
+    last_step_grasp_point = best_point
+    # For website demo
+    if analysis:
+        visual_all_points.append(P_world)
+        visual_link_ixs.append(link_ixs)
+        visual_grasp_points_idx.append(best_flow_ixs[best_flow_ix_id])
+        visual_grasp_points.append(best_point)
+        visual_flows.append(best_flow)
+
+    if website:
+        segmented_flow = np.zeros_like(pred_flow)
+        segmented_flow[link_ixs] = pred_flow[link_ixs]
+        segmented_flow = np.array(
+            normalize_trajectory(
+                torch.from_numpy(np.expand_dims(segmented_flow, 1))
+            ).squeeze()
+        )
+        animation.add_trace(
+            torch.as_tensor(P_world),
+            torch.as_tensor([P_world]),
+            torch.as_tensor([segmented_flow * 3]),
+            "red",
+        )
+
+    if not contact:
+        if website:
+            if gui:
+                p.stopStateLogging(log_id)
+            else:
+                # Write video
+                writer.close()
+                # videoWriter.release()
+
+        print("No contact!")
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        animation_results = None if not website else animation.animate()
+        return (
+            animation_results,
+            TrialResult(
+                success=False,
+                assertion=True,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory,
+        )
+
+    env.attach()
+    gripper_tip_pos_before = best_point
+    gripper_object_contact_local = get_local_point(
+        env.render_env.obj_id,
+        env.render_env.link_name_to_index[target_link],
+        gripper_tip_pos_before,
+    )
+    reset = env.pull_with_constraint(best_flow, target_link=target_link, constraint=sgp)
+    if not reset:
+        env.attach()
+        gripper_tip_pos_after = get_world_point(
+            env.render_env.obj_id,
+            env.render_env.link_name_to_index[target_link],
+            gripper_object_contact_local,
+        )
+
+        delta_gripper = np.array(gripper_tip_pos_after) - np.array(
+            gripper_tip_pos_before
+        )
+
+        if np.linalg.norm(delta_gripper) > initial_movement_thres:  # Because
+            correct_direction_stack.append(delta_gripper)
+
+        last_step_grasp_point = best_point
+    else:
+        last_step_grasp_point = None
+
+    pc_obs = env.render(filter_nonobj_pts=True, n_pts=n_pts)
+    success, sim_trajectory[1] = env.detect_success(target_link=target_link)
+
+    global_step = 1
+    # for i in range(n_steps):
+    while not success and global_step < n_steps:
+        # Predict the flow on the observation.
+        if gt_model is None:  # GT Flow model
+            pred_trajectory = model(copy.deepcopy(pc_obs))
+        else:
+            movable_mask = gt_model.get_movable_mask(pc_obs)
+            # breakpoint()
+            pred_trajectory = model(pc_obs, movable_mask)
+            # pred_trajectory = model(pc_obs)
+        pred_trajectory = pred_trajectory.reshape(
+            pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]
+        )
+
+        for traj_step in range(pred_trajectory.shape[1]):
+            if global_step == n_steps:
+                break
+            global_step += 1
+            pred_flow = pred_trajectory[:, traj_step, :]
+            rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs
+
+            # Filter down just the points on the target link.
+            # breakpoint()
+            link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]
+            # assert link_ixs.any()
+            if not link_ixs.any():
+                if website:
+                    if gui:
+                        p.stopStateLogging(log_id)
+                    else:
+                        writer.close()
+                        # videoWriter.release()
+                p.disconnect(physicsClientId=env.render_env.client_id)
+                print("link_ixs finds no point")
+                animation_results = animation.animate() if website else None
+                return (
+                    animation_results,
+                    TrialResult(
+                        assertion=False,
+                        success=False,
+                        contact=False,
+                        init_angle=0,
+                        final_angle=0,
+                        now_angle=0,
+                        metric=0,
+                    ),
+                    sim_trajectory,
+                )
+
+            # Get the best direction.
+            # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()
+            best_flow_ixs, best_flows, best_points = choose_grasp_points(
+                pred_flow[link_ixs],
+                P_world[link_ixs],
+                filter_edge=False,
+                k=20,
+                last_correct_direction=None
+                if len(correct_direction_stack) == 0 or not consistency_check
+                else correct_direction_stack[-1],
+            )
+
+            have_to_execute_incorrect = False
+
+            if (
+                len(best_flows) == 0
+            ):  # All top 20 points are filtered out! - Not a good prediction - move on!
+                this_step_trial += 1
+                if (
+                    this_step_trial > max_trial_per_step
+                ):  # To make the process go on, must make an action!
+                    have_to_execute_incorrect = True
+                    print("has to execute incorrect!!!")
+
+                    # Density choosing
+                    (
+                        best_flow_ixs,
+                        best_flows,
+                        best_points,
+                    ) = choose_grasp_points_density(
+                        pred_flow[link_ixs],
+                        P_world[link_ixs],
+                        k=20,
+                        last_correct_direction=None,
+                    )
+                else:
+                    continue
+
+            # (1) Strategy 1 - Don't change grasp point
+            # # (2) Strategy 2 - Change grasp point when leverage difference is large
+            if sgp:
+                lev_diff_thres = 0.2
+                no_movement_thres = -1
+            else:
+                # Don't use this policy
+                lev_diff_thres = 100
+                no_movement_thres = -1
+                good_movement_thres = 1000
+
+            # print(f"Trial {this_step_trial} times")
+            if last_step_grasp_point is not None:
+                gripper_tip_pos, _ = p.getBasePositionAndOrientation(
+                    env.gripper.body_id
+                )
+                pcd_dist = torch.tensor(
+                    P_world[link_ixs] - np.array(gripper_tip_pos)
+                ).norm(dim=-1)
+                grasp_point_id = pcd_dist.argmin()
+                lev_diff = best_flows.norm(dim=-1) - pred_flow[link_ixs][
+                    grasp_point_id
+                ].norm(dim=-1)
+
+            # gripper_movement = torch.from_numpy(
+            #     P_world[link_ixs][grasp_point_id] - last_step_grasp_point
+            # ).norm()
+            # print("gripper: ",gripper_movement)
+            # breakpoint()
+            # if (
+            #     gripper_movement < no_movement_thres or lev_diff[0] > lev_diff_thres
+            # ):  # pcd_dist < 0.05 -> didn't move much....
+            if last_step_grasp_point is None or lev_diff[0] > lev_diff_thres:
+                sgp_signals.append(1)
+                env.reset_gripper(target_link)
+                p.stepSimulation(
+                    env.render_env.client_id
+                )  # Make sure the constraint is lifted
+
+                if website and not gui:
+                    # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)
+                    best_flow_ix_id, contact = env.teleport(
+                        best_points,
+                        best_flows,
+                        video_writer=writer,
+                        target_link=target_link,
+                    )
+                else:
+                    # contact = env.teleport_and_approach(best_point, best_flow)
+                    best_flow_ix_id, contact = env.teleport(
+                        best_points, best_flows, target_link=target_link
+                    )
+
+                # image.save('/home/yishu/flowbothd/src/flowbothd/simulations/logs/simu_eval/video_assets/static_frames/attach_gripper.jpg')
+                best_flow = pred_flow[link_ixs][best_flow_ixs[best_flow_ix_id]]
+                best_point = P_world[link_ixs][best_flow_ixs[best_flow_ix_id]]
+                last_step_grasp_point = best_point  # Grasp a new point
+                # print("new!", last_step_grasp_point)
+
+                # For website demo
+                if analysis:
+                    visual_all_points.append(P_world)
+                    visual_link_ixs.append(link_ixs)
+                    visual_grasp_points_idx.append(best_flow_ixs[best_flow_ix_id])
+                    visual_grasp_points.append(best_point)
+                    visual_flows.append(best_flow)
+
+                if not contact:
+                    if website:
+                        segmented_flow = np.zeros_like(pred_flow)
+                        segmented_flow[link_ixs] = pred_flow[link_ixs]
+                        segmented_flow = np.array(
+                            normalize_trajectory(
+                                torch.from_numpy(np.expand_dims(segmented_flow, 1))
+                            ).squeeze()
+                        )
+                        animation.add_trace(
+                            torch.as_tensor(P_world),
+                            torch.as_tensor([P_world]),
+                            torch.as_tensor([segmented_flow * 3]),
+                            "red",
+                        )
+                        if gui:
+                            p.stopStateLogging(log_id)
+                        else:
+                            # Write video
+                            writer.close()
+                            # videoWriter.release()
+
+                    print("No contact!")
+                    p.disconnect(physicsClientId=env.render_env.client_id)
+                    animation_results = None if not website else animation.animate()
+                    return (
+                        animation_results,
+                        TrialResult(
+                            success=False,
+                            assertion=True,
+                            contact=False,
+                            init_angle=0,
+                            final_angle=0,
+                            now_angle=0,
+                            metric=0,
+                        ),
+                        sim_trajectory,
+                    )
+
+                env.attach()
+            else:
+                sgp_signals.append(0)
+                best_flow = pred_flow[link_ixs][best_flow_ixs[0]]
+                best_point = P_world[link_ixs][grasp_point_id]
+                last_step_grasp_point = best_point
+                # The original point - don't need to change
+                # print("same:", last_step_grasp_point)
+
+                # For website demo
+                if analysis:
+                    visual_all_points.append(P_world)
+                    visual_link_ixs.append(link_ixs)
+                    visual_grasp_points_idx.append(grasp_point_id)
+                    visual_grasp_points.append(best_point)
+                    visual_flows.append(best_flow)
+
+            # Execute the step:
+            env.attach()
+            gripper_tip_pos_before = last_step_grasp_point
+            gripper_object_contact_local = get_local_point(
+                env.render_env.obj_id,
+                env.render_env.link_name_to_index[target_link],
+                gripper_tip_pos_before,
+            )
+            reset = env.pull_with_constraint(
+                best_flow, target_link=target_link, constraint=sgp
+            )
+            if not reset:
+                env.attach()
+                last_step_grasp_point = best_point
+                gripper_tip_pos_after = get_world_point(
+                    env.render_env.obj_id,
+                    env.render_env.link_name_to_index[target_link],
+                    gripper_object_contact_local,
+                )
+
+                # Now with filter: we guarantee that every step is correct!!
+                delta_gripper = np.array(gripper_tip_pos_after) - np.array(
+                    gripper_tip_pos_before
+                )
+
+                # -----------Update the direction and history stack!!!!-----------
+                if len(correct_direction_stack) == 0:
+                    # Update direction stack
+                    if np.linalg.norm(delta_gripper) > initial_movement_thres:
+                        correct_direction_stack.append(
+                            delta_gripper / (np.linalg.norm(delta_gripper) + 1e-12)
+                        )
+                else:
+                    # Update direction stack:
+                    if (
+                        np.dot(delta_gripper, correct_direction_stack[-1]) > 0
+                    ):  # Consistent
+                        correct_direction_stack.append(
+                            delta_gripper / (np.linalg.norm(delta_gripper) + 1e-12)
+                        )
+
+            else:  # Need to reset gripper
+                last_step_grasp_point = None
+            # print(best_flow)
+            env.attach()
+            # print("After pulling!!", env.get_joint_value(target_link))
+            # breakpoint()
+
+            if website:
+                # Add pcd to flow animation
+                segmented_flow = np.zeros_like(pred_flow)
+                segmented_flow[link_ixs] = pred_flow[link_ixs]
+                segmented_flow = np.array(
+                    normalize_trajectory(
+                        torch.from_numpy(np.expand_dims(segmented_flow, 1))
+                    ).squeeze()
+                )
+                animation.add_trace(
+                    torch.as_tensor(P_world),
+                    torch.as_tensor([P_world]),
+                    torch.as_tensor([segmented_flow * 3]),
+                    "red",
+                )
+
+                # Capture frame
+                width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+                    width=frame_width,
+                    height=frame_height,
+                    viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+                        cameraTargetPosition=[0, 0, 0],
+                        distance=5,
+                        yaw=270,
+                        # yaw=90,
+                        pitch=-30,
+                        roll=0,
+                        upAxisIndex=2,
+                    ),
+                    projectionMatrix=p.computeProjectionMatrixFOV(
+                        fov=60,
+                        aspect=float(frame_width) / frame_height,
+                        nearVal=0.1,
+                        farVal=100.0,
+                    ),
+                )
+                # rgbImgOpenCV = cv2.cvtColor(np.array(rgbImg), cv2.COLOR_RGB2BGR)
+                # videoWriter.write(rgbImgOpenCV)
+                image = np.array(rgbImg, dtype=np.uint8)
+                image = image[:, :, :3]
+
+                # Add the frame to the video
+                writer.append_data(image)
+
+            success, sim_trajectory[global_step] = env.detect_success(target_link)
+
+            if success:
+                for left_step in range(global_step, 31):
+                    sim_trajectory[left_step] = sim_trajectory[global_step]
+                break
+
+            pc_obs = env.render(filter_nonobj_pts=True, n_pts=1200)
+            this_step_trial = 0  # This step is executed!
+
+        if success:
+            for left_step in range(global_step, 31):
+                sim_trajectory[left_step] = sim_trajectory[global_step]
+            break
+
+    # calculate the metrics
+    curr_pos = env.get_joint_value(target_link)
+    metric = (curr_pos - init_angle) / (target_angle - init_angle)
+    metric = min(max(metric, 0), 1)
+
+    if website:
+        if gui:
+            p.stopStateLogging(log_id)
+        else:
+            writer.close()
+            # videoWriter.release()
+
+    p.disconnect(physicsClientId=env.render_env.client_id)
+    animation_results = None if not website else animation.animate()
+    return (
+        animation_results,
+        TrialResult(  # Save the flow visuals
+            success=success,
+            contact=True,
+            assertion=True,
+            init_angle=init_angle,
+            final_angle=target_angle,
+            now_angle=curr_pos,
+            metric=metric,
+        ),
+        sim_trajectory
+        if not analysis
+        else [
+            sim_trajectory,
+            None,
+            None,
+            sgp_signals,
+            visual_all_points,
+            visual_link_ixs,
+            visual_grasp_points_idx,
+            visual_grasp_points,
+            visual_flows,
+        ],
+    )
+
+
+# Policy to filter the inconsistent actions and incorrect histories
+def run_trial_with_history_filter(
+    env: PMSuctionSim,
+    raw_data: PMObject,
+    target_link: str,
+    model,
+    model_with_history,
+    gt_model=None,  # When we use mask_input_channel=True, this is the mask generator
+    n_steps: int = 30,
+    n_pts: int = 1200,
+    save_name: str = "unknown",
+    website: bool = False,
+    gui: bool = False,
+    consistency_check=True,
+    history_filter=True,
+    analysis=False,
+) -> TrialResult:
+    # torch.manual_seed(42)
+    torch.set_printoptions(precision=10)  # Set higher precision for PyTorch outputs
+    np.set_printoptions(precision=10)
+    # p.setPhysicsEngineParameter(numSolverIterations=10)
+    # p.setPhysicsEngineParameter(contactBreakingThreshold=0.01, contactSlop=0.001)
+    print("Use consistency check:", consistency_check)
+    print("Use history filter:", history_filter)
+
+    initial_movement_thres = 1e-6
+    good_movement_thres = 0.01
+    max_trial_per_step = 50
+    this_step_trial = 0
+    prev_flow_pred = None
+    prev_point_cloud = None
+
+    sim_trajectory = [0.0] + [0] * (n_steps)  # start from 0.05
+    correct_direction_stack = []  # The direction stack
+
+    # For analysis:
+    update_history_step = []  # Record the steps which we update the history
+    cc_cnts = []  # Record the consistency failure times we had for each step
+    sgp_signals = [1]  # Record the steps which we switched grasp point
+
+    # For website demo
+    if analysis:
+        visual_all_points = []
+        visual_link_ixs = []
+        visual_grasp_points_idx = []
+        visual_grasp_points = []
+        visual_flows = []
+
+    if website:
+        # Flow animation
+        animation = FlowNetAnimation()
+
+    # First, reset the environment.
+    env.reset()
+    # Joint information
+    info = p.getJointInfo(
+        env.render_env.obj_id,
+        env.render_env.link_name_to_index[target_link],
+        env.render_env.client_id,
+    )
+    init_angle, target_angle = info[8], info[9]
+
+    if (
+        raw_data.category == "Door"
+        and raw_data.semantics.by_name(target_link).type == "hinge"
+    ):
+        env.set_joint_state(target_link, init_angle + 0.0 * (target_angle - init_angle))
+        # env.set_joint_state(target_link, 0.2)
+
+    if raw_data.semantics.by_name(target_link).type == "hinge":
+        env.set_joint_state(target_link, init_angle + 0.0 * (target_angle - init_angle))
+        # env.set_joint_state(target_link, 0.05)
+
+    # Predict the flow on the observation.
+    pc_obs = env.render(filter_nonobj_pts=True, n_pts=n_pts)
+    rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs
+
+    if init_angle == target_angle:  # Not movable
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        return (
+            None,
+            TrialResult(
+                success=False,
+                assertion=False,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory,
+        )
+
+    # breakpoint()
+    if gt_model is None:  # GT Flow model
+        pred_trajectory = model(copy.deepcopy(pc_obs))
+    else:
+        movable_mask = gt_model.get_movable_mask(pc_obs)
+        pred_trajectory = model(copy.deepcopy(pc_obs), movable_mask)
+    # pred_trajectory = model(copy.deepcopy(pc_obs))
+    # breakpoint()
+    pred_trajectory = pred_trajectory.reshape(
+        pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]
+    )
+    traj_len = pred_trajectory.shape[1]  # Trajectory length
+    print(f"Predicting {traj_len} length trajectories.")
+    pred_flow = pred_trajectory[:, 0, :]
+
+    # flow_fig(torch.from_numpy(P_world), pred_flow, sizeref=0.1, use_v2=True).show()
+    # breakpoint()
+
+    # Filter down just the points on the target link.
+    link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]
+    # assert link_ixs.any()
+    if not link_ixs.any():
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        print("link_ixs finds no point")
+        animation_results = animation.animate() if website else None
+        return (
+            animation_results,
+            TrialResult(
+                success=False,
+                assertion=False,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory,
+        )
+
+    if website:
+        if gui:
+            # Record simulation video
+            log_id = p.startStateLogging(
+                p.STATE_LOGGING_VIDEO_MP4,
+                f"./logs/simu_eval/video_assets/{save_name}.mp4",
+            )
+        else:
+            video_file = f"./logs/simu_eval/video_assets/{save_name}.mp4"
+            # # cv2 output videos won't show on website
+            frame_width = 640
+            frame_height = 480
+            # fps = 5
+            # fourcc = cv2.VideoWriter_fourcc(*'mp4v')
+            # videoWriter = cv2.VideoWriter(video_file, fourcc, fps, (frame_width, frame_height))
+            # videoWriter.write(rgbImgOpenCV)
+
+            # Camera param
+            writer = imageio.get_writer(video_file, fps=5)
+            env.set_writer(writer)
+
+            # Capture image
+            width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+                width=frame_width,
+                height=frame_height,
+                viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+                    cameraTargetPosition=[0, 0, 0],
+                    distance=5,
+                    yaw=270,
+                    # yaw=90,
+                    pitch=-30,
+                    roll=0,
+                    upAxisIndex=2,
+                ),
+                projectionMatrix=p.computeProjectionMatrixFOV(
+                    fov=60,
+                    aspect=float(frame_width) / frame_height,
+                    nearVal=0.1,
+                    farVal=100.0,
+                ),
+            )
+            image = np.array(rgbImg, dtype=np.uint8)
+            image = image[:, :, :3]
+
+            # Add the frame to the video
+            writer.append_data(image)
+
+    # The attachment point is the point with the highest flow.
+    # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()
+
+    # best_flow_ixs, best_flows, best_points = choose_grasp_points(
+    #     pred_flow[link_ixs], P_world[link_ixs], filter_edge=False, k=40
+    # )
+
+    # Density choosing
+    best_flow_ixs, best_flows, best_points = choose_grasp_points_density(
+        pred_flow[link_ixs], P_world[link_ixs], k=40
+    )
+    cc_cnts.append(0)
+
+    # Teleport to an approach pose, approach, the object and grasp.
+    if website and not gui:
+        # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)
+        best_flow_ix_id, contact = env.teleport(
+            best_points, best_flows, video_writer=writer, target_link=target_link
+        )
+    else:
+        # contact = env.teleport_and_approach(best_point, best_flow)
+        best_flow_ix_id, contact = env.teleport(
+            best_points, best_flows, target_link=target_link
+        )
+
+    best_flow = pred_flow[link_ixs][best_flow_ixs[best_flow_ix_id]]
+    best_point = P_world[link_ixs][best_flow_ixs[best_flow_ix_id]]
+
+    # For website demo
+    if analysis:
+        visual_all_points.append(P_world)
+        visual_link_ixs.append(link_ixs)
+        visual_grasp_points_idx.append(best_flow_ixs[best_flow_ix_id])
+        visual_grasp_points.append(best_point)
+        visual_flows.append(best_flow)
+
+    if website:
+        segmented_flow = np.zeros_like(pred_flow)
+        segmented_flow[link_ixs] = pred_flow[link_ixs]
+        segmented_flow = np.array(
+            normalize_trajectory(
+                torch.from_numpy(np.expand_dims(segmented_flow, 1))
+            ).squeeze()
+        )
+        animation.add_trace(
+            torch.as_tensor(P_world),
+            torch.as_tensor([P_world]),
+            torch.as_tensor([segmented_flow * 3]),
+            "red",
+        )
+
+    if not contact:
+        if website:
+            if gui:
+                p.stopStateLogging(log_id)
+            else:
+                # Write video
+                writer.close()
+                # videoWriter.release()
+
+        print("No contact!")
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        animation_results = None if not website else animation.animate()
+        return (
+            animation_results,
+            TrialResult(
+                success=False,
+                assertion=True,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory,
+        )
+
+    env.attach()
+    use_history = False
+    # gripper_tip_pos_before, _ = p.getBasePositionAndOrientation(env.gripper.base_id)
+    # points = p.getContactPoints(bodyA=env.gripper.body_id, bodyB=env.render_env.obj_id, linkIndexA=0)
+    # assert len(points)!=0, "Contact is None!!!!"
+    # gripper_tip_pos_before, _ = points[0][5], points[0][6]
+    gripper_tip_pos_before = best_point
+    gripper_object_contact_local = get_local_point(
+        env.render_env.obj_id,
+        env.render_env.link_name_to_index[target_link],
+        gripper_tip_pos_before,
+    )
+    # print(gripper_tip_pos_before, gripper_object_contact_local, get_world_point(env.render_env.obj_id, env.render_env.link_name_to_index[target_link], gripper_object_contact_local))
+    # env.pull(best_flow)
+    reset = env.pull_with_constraint(best_flow, target_link=target_link)
+    if not reset:
+        env.attach()
+        gripper_tip_pos_after = get_world_point(
+            env.render_env.obj_id,
+            env.render_env.link_name_to_index[target_link],
+            gripper_object_contact_local,
+        )
+
+        delta_gripper = np.array(gripper_tip_pos_after) - np.array(
+            gripper_tip_pos_before
+        )
+
+        last_step_grasp_point = best_point
+
+        if (
+            np.linalg.norm(delta_gripper) > initial_movement_thres
+        ):  # just to make sure it's not 0, 0, 0
+            correct_direction_stack.append(delta_gripper)
+        # Judge whether the movement is good - if it's good, update the history! (If not using history_filter, just update the history at every step!)
+        if not history_filter or np.linalg.norm(delta_gripper) > good_movement_thres:
+            use_history = True
+            prev_flow_pred = pred_flow.clone()  # History flow
+            prev_point_cloud = copy.deepcopy(P_world)  # History point cloud
+            update_history_step.append(1)
+
+    else:  # Need a reset because hit the lower boundary - definitely not a good step
+        if history_filter:
+            use_history = False
+            update_history_step.append(0)
+        else:  # no history filter: always update history
+            use_history = True
+            prev_flow_pred = pred_flow.clone()  # History flow
+            prev_point_cloud = copy.deepcopy(P_world)  # History point cloud
+            update_history_step.append(1)
+        last_step_grasp_point = None  # No contact anymore
+
+    # breakpoint()
+    global_step = 1
+    success, sim_trajectory[global_step] = env.detect_success(target_link)
+    # for i in range(n_steps):
+    while not success and global_step < n_steps:
+        pc_obs = env.render(
+            filter_nonobj_pts=True, n_pts=n_pts
+        )  # Render a new point cloud!  #
+        # Predict the flow on the observation.
+        if gt_model is None:  # GT Flow model
+            if use_history:
+                print("Using history!")
+                # Use history model
+                pred_trajectory = model_with_history(
+                    copy.deepcopy(pc_obs),
+                    copy.deepcopy(prev_point_cloud),
+                    copy.deepcopy(prev_flow_pred.numpy()),
+                )
+            else:
+                pred_trajectory = model(copy.deepcopy(pc_obs))
+        else:
+            movable_mask = gt_model.get_movable_mask(pc_obs)
+            # breakpoint()
+            pred_trajectory = model(pc_obs, movable_mask)
+            # pred_trajectory = model(pc_obs)
+        pred_trajectory = pred_trajectory.reshape(
+            pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]
+        )
+
+        pred_flow = pred_trajectory[:, 0, :]
+        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs
+
+        # Filter down just the points on the target link.
+        # breakpoint()
+        link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]
+        # assert link_ixs.any()
+        if not link_ixs.any():
+            if website:
+                if gui:
+                    p.stopStateLogging(log_id)
+                else:
+                    writer.close()
+                    # videoWriter.release()
+            p.disconnect(physicsClientId=env.render_env.client_id)
+            print("link_ixs finds no point")
+            animation_results = animation.animate() if website else None
+            return (
+                animation_results,
+                TrialResult(
+                    assertion=False,
+                    success=False,
+                    contact=False,
+                    init_angle=0,
+                    final_angle=0,
+                    now_angle=0,
+                    metric=0,
+                ),
+                sim_trajectory,
+            )
+
+        # Get the best direction.
+        # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()
+        # ------------DEBUG-------------
+        gt_model_debug = GTTrajectoryModel(raw_data, env, 1)
+        gt_flow = gt_model_debug.get_gt_force_vector(pc_obs, link_ixs)
+        if len(correct_direction_stack) != 0:
+            print(
+                "GT flow's cosine with the last consistent vector!!!!!",
+                np.dot(
+                    gt_flow.numpy(),
+                    correct_direction_stack[-1]
+                    / (np.linalg.norm(correct_direction_stack[-1]) + 1e-12),
+                ),
+            )
+        # ------------DEBUG-------------
+
+        # best_flow_ixs, best_flows, best_points = choose_grasp_points(
+        #     pred_flow[link_ixs],
+        #     P_world[link_ixs],
+        #     filter_edge=False,
+        #     k=40,
+        #     last_correct_direction=None
+        #     if len(correct_direction_stack) == 0
+        #     else correct_direction_stack[-1],
+        # )
+
+        # Density choosing
+        best_flow_ixs, best_flows, best_points = choose_grasp_points_density(
+            pred_flow[link_ixs],
+            P_world[link_ixs],
+            k=20,
+            last_correct_direction=None
+            if len(correct_direction_stack) == 0 or not consistency_check
+            else correct_direction_stack[-1],
+        )
+
+        have_to_execute_incorrect = False
+
+        if (
+            len(best_flows) == 0
+        ):  # All top 20 points are filtered out! - Not a good prediction - move on!
+            this_step_trial += 1
+            if (
+                this_step_trial > max_trial_per_step
+            ):  # To make the process go on, must make an action!
+                have_to_execute_incorrect = True
+                print("has to execute incorrect!!!")
+                # best_flow_ixs, best_flows, best_points = choose_grasp_points(
+                #     pred_flow[link_ixs],
+                #     P_world[link_ixs],
+                #     filter_edge=False,
+                #     k=20,
+                #     last_correct_direction=None,
+                # )
+
+                # Density choosing
+                best_flow_ixs, best_flows, best_points = choose_grasp_points_density(
+                    pred_flow[link_ixs],
+                    P_world[link_ixs],
+                    k=20,
+                    last_correct_direction=None,
+                )
+            else:
+                continue
+
+        cc_cnts.append(this_step_trial)
+
+        # (1) Strategy 1 - Don't change grasp point
+        # (2) Strategy 2 - Change grasp point when leverage difference is large
+        lev_diff_thres = 0.2
+        no_movement_thres = -1
+
+        # # Don't switch grasp point
+        # lev_diff_thres = 100
+        # no_movement_thres = -1
+        # good_movement_thres = 1000
+
+        if last_step_grasp_point is not None:  # Still grasping!
+            gripper_tip_pos, _ = p.getBasePositionAndOrientation(env.gripper.body_id)
+            pcd_dist = torch.tensor(P_world[link_ixs] - np.array(gripper_tip_pos)).norm(
+                dim=-1
+            )
+            grasp_point_id = pcd_dist.argmin()
+            print(grasp_point_id)
+            lev_diff = best_flows.norm(dim=-1) - pred_flow[link_ixs][
+                grasp_point_id
+            ].norm(dim=-1)
+
+        if (  # need to switch grasp point
+            last_step_grasp_point is None or lev_diff[0] > lev_diff_thres
+        ):
+            sgp_signals.append(1)
+            env.reset_gripper(target_link)
+            p.stepSimulation(
+                env.render_env.client_id
+            )  # Make sure the constraint is lifted
+
+            if website and not gui:
+                # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)
+                best_flow_ix_id, contact = env.teleport(
+                    best_points,
+                    best_flows,
+                    video_writer=writer,
+                    target_link=target_link,
+                )
+            else:
+                # contact = env.teleport_and_approach(best_point, best_flow)
+                best_flow_ix_id, contact = env.teleport(
+                    best_points, best_flows, target_link=target_link
+                )
+            best_flow = pred_flow[link_ixs][best_flow_ixs[best_flow_ix_id]]
+            best_point = P_world[link_ixs][best_flow_ixs[best_flow_ix_id]]
+            last_step_grasp_point = best_point  # Grasp a new point
+
+            # For website demo
+            if analysis:
+                visual_all_points.append(P_world)
+                visual_link_ixs.append(link_ixs)
+                visual_grasp_points_idx.append(best_flow_ixs[best_flow_ix_id])
+                visual_grasp_points.append(best_point)
+                visual_flows.append(best_flow)
+
+            if not contact:
+                if website:
+                    segmented_flow = np.zeros_like(pred_flow)
+                    segmented_flow[link_ixs] = pred_flow[link_ixs]
+                    segmented_flow = np.array(
+                        normalize_trajectory(
+                            torch.from_numpy(np.expand_dims(segmented_flow, 1))
+                        ).squeeze()
+                    )
+                    animation.add_trace(
+                        torch.as_tensor(P_world),
+                        torch.as_tensor([P_world]),
+                        torch.as_tensor([segmented_flow * 3]),
+                        "red",
+                    )
+                    if gui:
+                        p.stopStateLogging(log_id)
+                    else:
+                        # Write video
+                        writer.close()
+                        # videoWriter.release()
+
+                print("No contact!")
+                p.disconnect(physicsClientId=env.render_env.client_id)
+                animation_results = None if not website else animation.animate()
+                return (
+                    animation_results,
+                    TrialResult(
+                        success=False,
+                        assertion=True,
+                        contact=False,
+                        init_angle=0,
+                        final_angle=0,
+                        now_angle=0,
+                        metric=0,
+                    ),
+                    sim_trajectory,
+                )
+
+            env.attach()
+        else:  # Stick to the old grasp point
+            sgp_signals.append(0)
+            best_flow = pred_flow[link_ixs][best_flow_ixs[0]]
+            best_point = P_world[link_ixs][grasp_point_id]
+            last_step_grasp_point = (
+                best_point  # The original point - don't need to change
+            )
+            # print("same:", last_step_grasp_point)
+
+            # For website demo
+            if analysis:
+                visual_all_points.append(P_world)
+                visual_link_ixs.append(link_ixs)
+                visual_grasp_points_idx.append(grasp_point_id)
+                visual_grasp_points.append(best_point)
+                visual_flows.append(best_flow)
+
+        # Execute the step:
+        print(
+            "GT flow's cosine with the predicted vector!!!!!",
+            gt_flow,
+            best_flow / (np.linalg.norm(best_flow) + 1e-12),
+            np.dot(gt_flow.numpy(), best_flow / (np.linalg.norm(best_flow) + 1e-12)),
+        )
+        env.attach()
+        # gripper_tip_pos_before, _ = p.getBasePositionAndOrientation(env.gripper.base_id)
+        gripper_tip_pos_before = last_step_grasp_point
+        gripper_object_contact_local = get_local_point(
+            env.render_env.obj_id,
+            env.render_env.link_name_to_index[target_link],
+            gripper_tip_pos_before,
+        )
+        reset = env.pull_with_constraint(best_flow, target_link=target_link)
+        # -------DEBUG-------
+        # print(gt_flow)
+        # reset = env.pull_with_constraint(gt_flow, target_link=target_link)
+        # -------DEBUG-------
+        if not reset:
+            env.attach()
+            gripper_tip_pos_after = get_world_point(
+                env.render_env.obj_id,
+                env.render_env.link_name_to_index[target_link],
+                gripper_object_contact_local,
+            )
+
+            # Now with filter: we guarantee that every step is correct!!
+            delta_gripper = np.array(gripper_tip_pos_after) - np.array(
+                gripper_tip_pos_before
+            )
+            last_step_grasp_point = best_point
+
+            update_history_signal = False
+            # -----------Update the direction and history stack!!!!-----------
+            if len(correct_direction_stack) == 0:
+                # Update direction stack
+                if np.linalg.norm(delta_gripper) > initial_movement_thres:
+                    correct_direction_stack.append(
+                        delta_gripper / (np.linalg.norm(delta_gripper) + 1e-12)
+                    )
+                # Update history stack
+                if (
+                    not history_filter
+                    or np.linalg.norm(delta_gripper) > good_movement_thres
+                ):
+                    update_history_signal = True
+                    use_history = True
+                    prev_flow_pred = pred_flow.clone()  # History flow
+                    prev_point_cloud = copy.deepcopy(P_world)  # History point cloud
+            else:
+                # Update direction stack:
+                if np.dot(delta_gripper, correct_direction_stack[-1]) > 0:  # Consistent
+                    correct_direction_stack.append(
+                        delta_gripper / (np.linalg.norm(delta_gripper) + 1e-12)
+                    )
+                    if (
+                        not history_filter
+                        or np.linalg.norm(delta_gripper) > good_movement_thres
+                    ):
+                        update_history_signal = True
+                        prev_flow_pred = pred_flow.clone()  # History flow
+                        prev_point_cloud = copy.deepcopy(P_world)  # History point cloud
+            update_history_step.append(update_history_signal)
+        else:  # Reset
+            if history_filter:
+                use_history = False
+                update_history_step.append(0)
+            else:  # no history filter: always update history
+                use_history = True
+                update_history_step.append(1)
+                prev_flow_pred = pred_flow.clone()  # History flow
+                prev_point_cloud = copy.deepcopy(P_world)  # History point cloud
+            last_step_grasp_point = None
+        global_step += 1
+        this_step_trial = 0
+
+        if website:
+            # Add pcd to flow animation
+            segmented_flow = np.zeros_like(pred_flow)
+            segmented_flow[link_ixs] = pred_flow[link_ixs]
+            segmented_flow = np.array(
+                normalize_trajectory(
+                    torch.from_numpy(np.expand_dims(segmented_flow, 1))
+                ).squeeze()
+            )
+            animation.add_trace(
+                torch.as_tensor(P_world),
+                torch.as_tensor([P_world]),
+                torch.as_tensor([segmented_flow * 3]),
+                "red",
+            )
+
+            # Capture frame
+            width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+                width=frame_width,
+                height=frame_height,
+                viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+                    cameraTargetPosition=[0, 0, 0],
+                    distance=5,
+                    yaw=270,
+                    # yaw=90,
+                    pitch=-30,
+                    roll=0,
+                    upAxisIndex=2,
+                ),
+                projectionMatrix=p.computeProjectionMatrixFOV(
+                    fov=60,
+                    aspect=float(frame_width) / frame_height,
+                    nearVal=0.1,
+                    farVal=100.0,
+                ),
+            )
+            # rgbImgOpenCV = cv2.cvtColor(np.array(rgbImg), cv2.COLOR_RGB2BGR)
+            # videoWriter.write(rgbImgOpenCV)
+            image = np.array(rgbImg, dtype=np.uint8)
+            image = image[:, :, :3]
+
+            # Add the frame to the video
+            writer.append_data(image)
+
+        # breakpoint()
+        success, sim_trajectory[global_step] = env.detect_success(target_link)
+
+        if success:
+            break
+
+        # pc_obs = env.render(filter_nonobj_pts=True, n_pts=1200)   # Render a new point cloud!
+        # if len(correct_direction_stack) == 2:
+        #     breakpoint()
+
+    for left_step in range(global_step, 31):
+        sim_trajectory[left_step] = sim_trajectory[global_step]
+    # calculate the metrics
+    # if success:
+    #     metric = 1
+    # else:
+    #     curr_pos = env.get_joint_value(target_link)
+    #     metric = (curr_pos - init_angle) / (target_angle - init_angle)
+    #     metric = min(max(metric, 0), 1)
+    curr_pos = env.get_joint_value(target_link)
+    metric = (curr_pos - init_angle) / (target_angle - init_angle)
+    metric = min(max(metric, 0), 1)
+
+    if website:
+        if gui:
+            p.stopStateLogging(log_id)
+        else:
+            writer.close()
+            # videoWriter.release()
+
+    p.disconnect(physicsClientId=env.render_env.client_id)
+    animation_results = None if not website else animation.animate()
+    return (
+        animation_results,
+        TrialResult(  # Save the flow visuals
+            success=success,
+            contact=True,
+            assertion=True,
+            init_angle=init_angle,
+            final_angle=target_angle,
+            now_angle=curr_pos,
+            metric=metric,
+        ),
+        sim_trajectory
+        if not analysis
+        else [
+            sim_trajectory,
+            update_history_step,
+            cc_cnts,
+            sgp_signals,
+            visual_all_points,
+            visual_link_ixs,
+            visual_grasp_points_idx,
+            visual_grasp_points,
+            visual_flows,
+        ],
+    )
+
+
+def run_trial_with_switch_models(
+    env: PMSuctionSim,
+    raw_data: PMObject,
+    target_link: str,
+    model,
+    switch_model,
+    history_for_models,
+    gt_model=None,  # When we use mask_input_channel=True, this is the mask generator
+    n_steps: int = 30,
+    n_pts: int = 1200,
+    save_name: str = "unknown",
+    website: bool = False,
+    gui: bool = False,
+    return_switch_ids: bool = False,
+) -> TrialResult:
+    torch.manual_seed(42)
+    torch.set_printoptions(precision=10)  # Set higher precision for PyTorch outputs
+    np.set_printoptions(precision=10)
+    # p.setPhysicsEngineParameter(numSolverIterations=10)
+    # p.setPhysicsEngineParameter(contactBreakingThreshold=0.01, contactSlop=0.001)
+
+    sim_trajectory = [0.0] + [0] * (n_steps)  # start from 0.05
+    model_ids = [-1] + [0] * (n_steps)
+
+    if website:
+        # Flow animation
+        animation = FlowNetAnimation()
+
+    # First, reset the environment.
+    env.reset()
+    # Joint information
+    info = p.getJointInfo(
+        env.render_env.obj_id,
+        env.render_env.link_name_to_index[target_link],
+        env.render_env.client_id,
+    )
+    init_angle, target_angle = info[8], info[9]
+
+    # Sometimes doors collide with themselves. It's dumb.
+    if (
+        raw_data.category == "Door"
+        and raw_data.semantics.by_name(target_link).type == "hinge"
+    ):
+        env.set_joint_state(target_link, init_angle + 0.0 * (target_angle - init_angle))
+        # env.set_joint_state(target_link, 0.2)
+
+    if raw_data.semantics.by_name(target_link).type == "hinge":
+        env.set_joint_state(target_link, init_angle + 0.0 * (target_angle - init_angle))
+        # env.set_joint_state(target_link, 0.05)
+
+    # Predict the flow on the observation.
+    pc_obs = env.render(filter_nonobj_pts=True, n_pts=n_pts)
+    rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs
+
+    if init_angle == target_angle:  # Not movable
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        return (
+            None,
+            TrialResult(
+                success=False,
+                assertion=False,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory if not return_switch_ids else (sim_trajectory, model_ids),
+        )
+
+    # breakpoint()
+    if gt_model is None:  # No mask
+        # For the first step, never use history!!!
+        pred_trajectory = model(copy.deepcopy(pc_obs))
+    else:
+        movable_mask = gt_model.get_movable_mask(pc_obs)
+        pred_trajectory = model(copy.deepcopy(pc_obs), movable_mask)
+    pred_trajectory = pred_trajectory.reshape(
+        pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]
+    )
+    traj_len = pred_trajectory.shape[1]  # Trajectory length
+    print(f"Predicting {traj_len} length trajectories.")
+    pred_flow = pred_trajectory[:, 0, :]
+
+    # Filter down just the points on the target link.
+    link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]
+    # assert link_ixs.any()
+    if not link_ixs.any():
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        print("link_ixs finds no point")
+        animation_results = animation.animate() if website else None
+        return (
+            animation_results,
+            TrialResult(
+                success=False,
+                assertion=False,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory if not return_switch_ids else (sim_trajectory, model_ids),
+        )
+
+    if website:
+        if gui:
+            # Record simulation video
+            log_id = p.startStateLogging(
+                p.STATE_LOGGING_VIDEO_MP4,
+                f"./logs/simu_eval/video_assets/{save_name}.mp4",
+            )
+        else:
+            video_file = f"./logs/simu_eval/video_assets/{save_name}.mp4"
+            # # cv2 output videos won't show on website
+            frame_width = 640
+            frame_height = 480
+            # fps = 5
+            # fourcc = cv2.VideoWriter_fourcc(*'mp4v')
+            # videoWriter = cv2.VideoWriter(video_file, fourcc, fps, (frame_width, frame_height))
+            # videoWriter.write(rgbImgOpenCV)
+
+            # Camera param
+            writer = imageio.get_writer(video_file, fps=5)
+
+            # Capture image
+            width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+                width=frame_width,
+                height=frame_height,
+                viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+                    cameraTargetPosition=[0, 0, 0],
+                    distance=5,
+                    # yaw=180,
+                    yaw=270,
+                    pitch=-30,
+                    roll=0,
+                    upAxisIndex=2,
+                ),
+                projectionMatrix=p.computeProjectionMatrixFOV(
+                    fov=60,
+                    aspect=float(frame_width) / frame_height,
+                    nearVal=0.1,
+                    farVal=100.0,
+                ),
+            )
+            image = np.array(rgbImg, dtype=np.uint8)
+            image = image[:, :, :3]
+
+            # Add the frame to the video
+            writer.append_data(image)
+
+    # The attachment point is the point with the highest flow.
+    # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()
+    best_flow_ix, best_flows, best_points = choose_grasp_points(
+        pred_flow[link_ixs], P_world[link_ixs], filter_edge=False, k=20
+    )
+
+    # Teleport to an approach pose, approach, the object and grasp.
+    if website and not gui:
+        # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)
+        best_flow_ix, contact = env.teleport(
+            best_points, best_flows, video_writer=writer
+        )
+    else:
+        # contact = env.teleport_and_approach(best_point, best_flow)
+        best_flow_ix, contact = env.teleport(best_points, best_flows)
+
+    prev_flow_pred = pred_flow.clone()  # History flow
+    prev_point_cloud = copy.deepcopy(P_world)  # History point cloud
+
+    best_flow = pred_flow[link_ixs][best_flow_ix]
+    best_point = P_world[link_ixs][best_flow_ix]
+    last_step_grasp_point = best_point
+
+    if not contact:
+        if website:
+            segmented_flow = np.zeros_like(pred_flow)
+            segmented_flow[link_ixs] = pred_flow[link_ixs]
+            segmented_flow = np.array(
+                normalize_trajectory(
+                    torch.from_numpy(np.expand_dims(segmented_flow, 1))
+                ).squeeze()
+            )
+            animation.add_trace(
+                torch.as_tensor(P_world),
+                torch.as_tensor([P_world]),
+                torch.as_tensor([segmented_flow * 3]),
+                "red",
+            )
+            if gui:
+                p.stopStateLogging(log_id)
+            else:
+                # Write video
+                writer.close()
+                # videoWriter.release()
+
+        print("No contact!")
+        p.disconnect(physicsClientId=env.render_env.client_id)
+        animation_results = None if not website else animation.animate()
+        return (
+            animation_results,
+            TrialResult(
+                success=False,
+                assertion=True,
+                contact=False,
+                init_angle=0,
+                final_angle=0,
+                now_angle=0,
+                metric=0,
+            ),
+            sim_trajectory if not return_switch_ids else (sim_trajectory, model_ids),
+        )
+
+    env.attach()
+    # breakpoint()
+    pc_obs = env.render(filter_nonobj_pts=True, n_pts=n_pts)
+    success = False
+
+    should_switch_model = False
+
+    global_step = 0
+    # for i in range(n_steps):
+    while global_step < n_steps:
+        # Predict the flow on the observation.
+        if gt_model is None:  # GT Flow model
+            if should_switch_model:
+                print("Using model 2!")
+                # Use history model
+                if history_for_models[1]:
+                    pred_trajectory = switch_model(
+                        copy.deepcopy(pc_obs),
+                        copy.deepcopy(prev_point_cloud),
+                        copy.deepcopy(prev_flow_pred.numpy()),
+                    )
+                else:  # Switch model is not a history_based_model
+                    pred_trajectory = switch_model(copy.deepcopy(pc_obs))
+            else:
+                pred_trajectory = model(copy.deepcopy(pc_obs))
+        else:
+            movable_mask = gt_model.get_movable_mask(pc_obs)
+            # breakpoint()
+            pred_trajectory = model(pc_obs, movable_mask)
+            # pred_trajectory = model(pc_obs)
+        pred_trajectory = pred_trajectory.reshape(
+            pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]
+        )
+
+        for traj_step in range(pred_trajectory.shape[1]):
+            if global_step == n_steps:
+                break
+            global_step += 1
+            pred_flow = pred_trajectory[:, traj_step, :]
+            rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs
+
+            # Filter down just the points on the target link.
+            # breakpoint()
+            link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]
+            # assert link_ixs.any()
+            if not link_ixs.any():
+                if website:
+                    if gui:
+                        p.stopStateLogging(log_id)
+                    else:
+                        writer.close()
+                        # videoWriter.release()
+                p.disconnect(physicsClientId=env.render_env.client_id)
+                print("link_ixs finds no point")
+                animation_results = animation.animate() if website else None
+                return (
+                    animation_results,
+                    TrialResult(
+                        assertion=False,
+                        success=False,
+                        contact=False,
+                        init_angle=0,
+                        final_angle=0,
+                        now_angle=0,
+                        metric=0,
+                    ),
+                    sim_trajectory
+                    if not return_switch_ids
+                    else (sim_trajectory, model_ids),
+                )
+
+            # Get the best direction.
+            # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()
+            best_flow_ix, best_flows, best_points = choose_grasp_points(
+                pred_flow[link_ixs], P_world[link_ixs], filter_edge=False, k=20
+            )
+
+            # (1) Strategy 1 - Don't change grasp point
+            # (2) Strategy 2 - Change grasp point when leverage difference is large
+            lev_diff_thres = 0.2
+            no_movement_thres = -1
+
+            # # Don't use this policy
+            # lev_diff_thres = 100
+            # no_movement_thres = -1
+            # good_movement_thres = 1000
+
+            # Only change if the new point's leverage is a great increase
+            # gripper_tip_pos = p.getClosestPoints(
+            #     env.gripper.body_id, env.render_env.obj_id, distance=0.5, linkIndexA=0
+            # )[0][5]
+            # gripper_object_contact = p.getContactPoints(
+            #     env.gripper.body_id, env.render_env.obj_id, linkIndexA=0
+            # )[0]
+            # gripper_contact, object_contact = gripper_object_contact[5], gripper_object_contact[6]
+            gripper_tip_pos, _ = p.getBasePositionAndOrientation(env.gripper.body_id)
+            pcd_dist = torch.tensor(P_world[link_ixs] - np.array(gripper_tip_pos)).norm(
+                dim=-1
+            )
+            grasp_point_id = pcd_dist.argmin()
+            lev_diff = best_flows.norm(dim=-1) - pred_flow[link_ixs][
+                grasp_point_id
+            ].norm(dim=-1)
+
+            gripper_movement = torch.from_numpy(
+                P_world[link_ixs][grasp_point_id] - last_step_grasp_point
+            ).norm()
+            # print("gripper: ",gripper_movement)
+            # breakpoint()
+            if (
+                gripper_movement < no_movement_thres or lev_diff[0] > lev_diff_thres
+            ):  # pcd_dist < 0.05 -> didn't move much....
+                env.reset_gripper(target_link)
+                p.stepSimulation(
+                    env.render_env.client_id
+                )  # Make sure the constraint is lifted
+
+                if website and not gui:
+                    # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)
+                    best_flow_ix, contact = env.teleport(
+                        best_points, best_flows, video_writer=writer
+                    )
+                else:
+                    # contact = env.teleport_and_approach(best_point, best_flow)
+                    best_flow_ix, contact = env.teleport(best_points, best_flows)
+                best_flow = pred_flow[link_ixs][best_flow_ix]
+                best_point = P_world[link_ixs][best_flow_ix]
+                last_step_grasp_point = best_point  # Grasp a new point
+                # print("new!", last_step_grasp_point)
+
+                if not contact:
+                    if website:
+                        segmented_flow = np.zeros_like(pred_flow)
+                        segmented_flow[link_ixs] = pred_flow[link_ixs]
+                        segmented_flow = np.array(
+                            normalize_trajectory(
+                                torch.from_numpy(np.expand_dims(segmented_flow, 1))
+                            ).squeeze()
+                        )
+                        animation.add_trace(
+                            torch.as_tensor(P_world),
+                            torch.as_tensor([P_world]),
+                            torch.as_tensor([segmented_flow * 3]),
+                            "red",
+                        )
+                        if gui:
+                            p.stopStateLogging(log_id)
+                        else:
+                            # Write video
+                            writer.close()
+                            # videoWriter.release()
+
+                    print("No contact!")
+                    p.disconnect(physicsClientId=env.render_env.client_id)
+                    animation_results = None if not website else animation.animate()
+                    return (
+                        animation_results,
+                        TrialResult(
+                            success=False,
+                            assertion=True,
+                            contact=False,
+                            init_angle=0,
+                            final_angle=0,
+                            now_angle=0,
+                            metric=0,
+                        ),
+                        sim_trajectory
+                        if not return_switch_ids
+                        else (sim_trajectory, model_ids),
+                    )
+
+                env.attach()
+            else:
+                best_flow = pred_flow[link_ixs][best_flow_ix[0]]
+                last_step_grasp_point = P_world[link_ixs][
+                    grasp_point_id
+                ]  # The original point - don't need to change
+                # print("same:", last_step_grasp_point)
+
+            env.attach()
+            # Perform the pulling.
+            # if best_flow.sum() == 0:
+            #     continue
+            # print(best_flow)
+            env.pull(best_flow)
+            env.attach()
+
+            if website:
+                # Add pcd to flow animation
+                segmented_flow = np.zeros_like(pred_flow)
+                segmented_flow[link_ixs] = pred_flow[link_ixs]
+                segmented_flow = np.array(
+                    normalize_trajectory(
+                        torch.from_numpy(np.expand_dims(segmented_flow, 1))
+                    ).squeeze()
+                )
+                animation.add_trace(
+                    torch.as_tensor(P_world),
+                    torch.as_tensor([P_world]),
+                    torch.as_tensor([segmented_flow * 3]),
+                    "red",
+                )
+
+                # Capture frame
+                width, height, rgbImg, depthImg, segImg = p.getCameraImage(
+                    width=frame_width,
+                    height=frame_height,
+                    viewMatrix=p.computeViewMatrixFromYawPitchRoll(
+                        cameraTargetPosition=[0, 0, 0],
+                        distance=5,
+                        yaw=270,
+                        # yaw=180,
+                        pitch=-30,
+                        roll=0,
+                        upAxisIndex=2,
+                    ),
+                    projectionMatrix=p.computeProjectionMatrixFOV(
+                        fov=60,
+                        aspect=float(frame_width) / frame_height,
+                        nearVal=0.1,
+                        farVal=100.0,
+                    ),
+                )
+                # rgbImgOpenCV = cv2.cvtColor(np.array(rgbImg), cv2.COLOR_RGB2BGR)
+                # videoWriter.write(rgbImgOpenCV)
+                image = np.array(rgbImg, dtype=np.uint8)
+                image = image[:, :, :3]
+
+                # Add the frame to the video
+                writer.append_data(image)
+
+            success, sim_trajectory[global_step] = env.detect_success(target_link)
+            model_ids[global_step] = should_switch_model
+
+            if success:
+                for left_step in range(global_step, 31):
+                    sim_trajectory[left_step] = sim_trajectory[global_step]
+                    model_ids[left_step] = model_ids[global_step]
+                break
+
+            # Previous step
+            # # Policy - 1
+            # should_switch_model = True  # Always use history when there is history
+
+            # # Policy - 2
+            # should_switch_model = (  # If last step makes progress
+            #     sim_trajectory[global_step] - sim_trajectory[global_step - 1]
+            # ) > 0.01
+
+            # Policy - 3
+            should_switch_model = (sim_trajectory[global_step]) > 0.05  # If it's opened
+
+            # # Policy - 4
+            # should_switch_model = (  # If last step makes progress
+            #     sim_trajectory[global_step]
+            # ) > 0.1 and (sim_trajectory[global_step] - sim_trajectory[global_step - 1]) > 0.01
+
+            prev_flow_pred = pred_flow.clone()
+            prev_point_cloud = copy.deepcopy(pc_obs[4])
+            pc_obs = env.render(filter_nonobj_pts=True, n_pts=1200)
+
+        if success:
+            for left_step in range(global_step, 31):
+                sim_trajectory[left_step] = sim_trajectory[global_step]
+            break
+
+    # calculate the metrics
+    curr_pos = env.get_joint_value(target_link)
+    metric = (curr_pos - init_angle) / (target_angle - init_angle)
+    metric = min(max(metric, 0), 1)
+
+    if website:
+        if gui:
+            p.stopStateLogging(log_id)
+        else:
+            writer.close()
+            # videoWriter.release()
+
+    p.disconnect(physicsClientId=env.render_env.client_id)
+    animation_results = None if not website else animation.animate()
+    return (
+        animation_results,
+        TrialResult(  # Save the flow visuals
+            success=success,
+            contact=True,
+            assertion=True,
+            init_angle=init_angle,
+            final_angle=target_angle,
+            now_angle=curr_pos,
+            metric=metric,
+        ),
+        sim_trajectory if not return_switch_ids else (sim_trajectory, model_ids),
+    )
diff --git a/src/flowbothd/utils/script_utils.py b/src/flowbothd/utils/script_utils.py
new file mode 100644
index 0000000..12df169
--- /dev/null
+++ b/src/flowbothd/utils/script_utils.py
@@ -0,0 +1,114 @@
+import abc
+import os
+import pathlib
+from typing import Dict, List, Literal, Protocol, Sequence, Union, cast
+
+import lightning.pytorch as pl
+import numpy as np
+import torch
+import torch.utils._pytree as pytree
+import torch_geometric.data as tgd
+from lightning.pytorch import Callback
+from lightning.pytorch.loggers import WandbLogger
+
+PROJECT_ROOT = str(pathlib.Path(__file__).parent.parent.parent.parent.resolve())
+
+
+# This matching function
+def match_fn(dirs: Sequence[str], extensions: Sequence[str], root: str = PROJECT_ROOT):
+    def _match_fn(path: pathlib.Path):
+        in_dir = any([str(path).startswith(os.path.join(root, d)) for d in dirs])
+
+        if not in_dir:
+            return False
+
+        if not any([str(path).endswith(e) for e in extensions]):
+            return False
+
+        return True
+
+    return _match_fn
+
+
+TorchTree = Dict[str, Union[torch.Tensor, "TorchTree"]]
+
+
+def flatten_outputs(outputs: List[TorchTree]) -> TorchTree:
+    """Flatten a list of dictionaries into a single dictionary."""
+
+    # Concatenate all leaf nodes in the trees.
+    flattened_outputs = [pytree.tree_flatten(output) for output in outputs]
+    flattened_list = [o[0] for o in flattened_outputs]
+    flattened_spec = flattened_outputs[0][1]  # Spec definitely should be the same...
+    cat_flat = [torch.cat(x) for x in list(zip(*flattened_list))]
+    output_dict = pytree.tree_unflatten(cat_flat, flattened_spec)
+    return cast(TorchTree, output_dict)
+
+
+class CanMakePlots(Protocol):
+    @staticmethod
+    @abc.abstractmethod
+    def make_plots(preds, batch: tgd.Batch):
+        pass
+
+
+class LightningModuleWithPlots(pl.LightningModule, CanMakePlots):
+    pass
+
+
+class LogPredictionSamplesCallback(Callback):
+    def __init__(self, logger: WandbLogger, eval_per_n_epoch, eval_dataloader_lengths):
+        self.logger = logger
+        self.eval_per_n_epoch = eval_per_n_epoch
+        self.eval_dataloader_lengths = (
+            eval_dataloader_lengths  # [val_len, train_val_len, unseen_len]
+        )
+
+    @staticmethod
+    def eval_log_random_sample(
+        trainer: pl.Trainer,
+        pl_module: LightningModuleWithPlots,
+        outputs,
+        batch,
+        prefix: Literal["train", "val", "unseen"],
+    ):
+        preds = outputs["preds"]
+        cosine_cache = outputs["cosine_cache"]
+        random_id = np.random.randint(0, len(batch))
+        # preds = preds.reshape(
+        #     pl_module.batch_size, -1, preds.shape[-2], preds.shape[-1]
+        # )[random_id]
+        preds = preds.reshape(len(batch), -1, preds.shape[-2], preds.shape[-1])[
+            random_id
+        ]
+        data = batch.get_example(random_id)
+        plots = pl_module.make_plots(preds.cpu(), data.cpu(), cosine_cache)
+
+        assert trainer.logger is not None and isinstance(trainer.logger, WandbLogger)
+        trainer.logger.experiment.log(
+            {
+                **{
+                    f"{prefix}{'_wta' if plot_name == 'cosine_distribution_plot' else ''}/{plot_name}": plot
+                    for plot_name, plot in plots.items()
+                    if plot is not None
+                },
+                "global_step": trainer.global_step,
+            },
+            step=trainer.global_step,
+        )
+
+    def on_validation_batch_end(
+        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0
+    ):
+        """Called when the validation batch ends."""
+
+        # `outputs` comes from `LightningModule.validation_step`
+        # which corresponds to our model predictions in this case
+        if batch_idx != self.eval_dataloader_lengths[dataloader_idx] - 1:
+            # For the flow plots: only log one sample (a sample from the last batch)
+            # For the cosine distribution plot: only log at the end of this eval dataloader (The plot needs to be full)
+            return
+        dataloader_names = ["val", "train", "unseen"]
+        name = dataloader_names[dataloader_idx]
+        if (pl_module.current_epoch + 1) % self.eval_per_n_epoch == 0:
+            self.eval_log_random_sample(trainer, pl_module, outputs, batch, name)
