diff --git a/README.md b/README.md
index c37739f..ac9d06a 100644
--- a/README.md
+++ b/README.md
@@ -1,51 +1,106 @@
 # FlowBotHD
 
-Code for FlowBotHD: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation
+[FlowBotHD](https://flowbothd.github.io/) is a history-aware diffusion handling ambiguities (multi-modality and occlusion) in articulated objects.
 
-## Run Full-set Experiments
+![Alt text](imgs/teaser.jpg)
 
-### Train
 
-STEP-1: Specify the Config files:
+## Installation
+
+```{bash}
+conda create -n flowbothd python=3.9
+
+conda activate flowbothd
+
+pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
+
+pip install -r requirements.txt
+```
+
+> Note: Make sure you install pytorch with regard to your machine's CUDA version. If you want to change the cuda version, you also need to change the cuda version specified in requirements.txt
+
+
+Then we have to also install the flowbothd package itself. [Make sure you are directly under the flowbothd directory, where you can see pyproject.toml]
+
+```{bash}
+pip install .
+```
+
+## Demo
+
+In our `demo.ipynb`, we have prediction and simulation demo given a pretrained model.
+
+TODO
+
+## Train
+
+
+### STEP 0 : Set the config files
+
+0) configs/_logging.yaml:
+
+    Set the `wandb.entity` and `wandb.project` for wandb logging.
+
 1) configs/train.yaml: 
 
-    Choose dataset (single flow or trajectory) and model (original flowbot or diffuser)
-    - dataset: trajectory / flowbot
-    - model: artflownet / diffuser
+    Choose model
+    - `model`: diffuser_hispndit (FlowBotHD) / artflownet (FlowBot3D) / other structures we've tried whose names are listed in the configs/model/.
+
+2) configs/dataset/trajectory.yaml
+
+    `dataset_type`: full-dataset for training and evaluating on full partnet-mobility, doors-only for training and evaluating only on the most ambiguous category: doors.
 
-2) configs/training/{}_{}.yaml
+3) configs/training/trajectory_{model.name}.yaml
 
-    Change the corresponding detailed configs for the training process: learning_rate, batch_size, warmup_steps, etc.
+    Change the corresponding detailed configs for the training process: `learning_rate`, `batch_size`, `warmup_steps`.
 
-3) configs/model/{}.yaml
+4) configs/model/{model.name}.yaml
 
     Change the detailed configs for the model (Only needed for diffusion)
 
-    - num_train_timesteps: diffusion timestep
+    - `num_train_timesteps`: diffusion timestep
 
-STEP-2: Run training script
+### STEP 1 : Run training script
 ```
 python scripts/train.py
 ```
 
-### Eval
+## Eval and Simulation
+
+If you want to evaluate (calculate the basic cosine, mag, rmse metrics across dataset) and evaluate the simulation (calculate the success rate, normalized distance) of a trained model:
+
+### STEP 0 : Set the config files
+
+1. Specify the checkpoint / run you are evaluating:
+
+Set `checkpoint.run_id` and `wandb.group` with the run_id and the group name of the run to be evaluated.
 
-Basically the same procedure with training, only the config files are eval.yaml (metric evaluation), eval_sim.yaml (simulation evaluation).
+2. Set the same `model`, `dataset_type` etc as the training config files above.
 
-Need to specify:
-checkpoint/run_id: the run_id in wandb
-wandb/group: the group name in wandb
+### STEP 1 : Run evaluation / simulation
 
+For evaluating diffuser-based methods with history (that needs winner-take-all metric, meaning multiple samples and take the best metric):
+
+```{bash} 
+python scripts/eval_diffuser_wta.py   # For basic evaluation
+python scripts/eval_sim_diffuser_history.py  # For simulation
+```
+
+For evaluating diffuser-based methods without history:
+
+```{bash}
+python scripts/eval_history_diffuser_wta.py   # For basic evaluation
+python scripts/eval_sim_diffuser.py  # For simulation
 ```
+
+For evaluating regression-based method (flowbot3d):
+```{bash}
 python scripts/eval.py
-python scripts/eval_diffuser_wta.py
-python scripts/eval_diffuser.py
-python scripts/eval_sim.py  (Currently only for flowbot)
+python scripts/eval_sim.py
 ```
 
-## Run Small Dataset Experiment
 
-(This needs some changes in the `r-pad/partnet_mobility_utils` repo that are currently kept local. Should ask Yishu about it)
+## Run Specified Subset Experiment
 
 Need to change scripts/train.py for training and scripts/eval(_sim).py for evaluation:
 
@@ -53,8 +108,9 @@ When creating dataset, specify the arguments `special_req` and `toy_dataset`.
 
 1) special_req: 
 
-- "half-half"(Half of data fully closed, half of data randomly opened )
+- "half-half-01"(Part of data fully closed, part of data randomly opened, with 0 or 1-step history)
 - "fully-closed"(All of data fully closed)
+- "randomly-open"(All of the data randomly open)
 
 2) toy_dataset: a dict to specify a small dataset
 - id: the name for the toy dataset
@@ -72,8 +128,7 @@ datamodule = data_module_class[cfg.dataset.name](
     n_proc=cfg.resources.n_proc_per_worker,
     seed=cfg.seed,
     trajectory_len=trajectory_len, 
-    special_req="half-half"
-    # only for toy training
+    special_req="half-half-01"
     toy_dataset = {
         "id": "door-1",
         "train-train": ["8994", "9035"],
@@ -85,17 +140,16 @@ datamodule = data_module_class[cfg.dataset.name](
 
 Then run train and eval exactly like before.
 
-## Run Diffusion
+## Cite
 
-Currently For diffusion, most experiments are run with scripts under `src/flowbothd/models/diffusion`. (Although the above set of pipeline is also complete, I suggest currently run diffusion under this directory and with following commands)
+If you find this codebase useful in your research, please consider citing:
 
-Train: Under `src/flowbothd/models/diffusion/`
-```
-python diffuser.py
 ```
-Eval: Under `src/flowbothd/models/diffusion/`
-```
-python eval.py
+@inproceedings{liflowbothd,
+  title={FlowBotHD: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation},
+  author={Li, Yishu and Leng, Wen Hui and Fang, Yiming and Eisner, Ben and Held, David},
+  booktitle={8th Annual Conference on Robot Learning}
+}
 ```
 
-We can also use `inference.ipynb` to see the visualization results
+
diff --git a/configs/_logging.yaml b/configs/_logging.yaml
index 69c6329..18c4bc2 100644
--- a/configs/_logging.yaml
+++ b/configs/_logging.yaml
@@ -20,9 +20,8 @@ lightning:
   checkpoint_dir: ${output_dir}/checkpoints
 
 wandb:
-  entity: r-pad
-  # entity: leisure-thu-cv
-  project: flowbothd
+  entity: leisure-thu-cv # ???  # The wandb entity
+  project: flowbothd # ???  # The wandb project name
 
   # Group is for grouping runs together (i.e. a train run and an eval run).
   group: ???
diff --git a/configs/dataset/flowbot.yaml b/configs/dataset/flowbot.yaml
deleted file mode 100644
index 5d28c0b..0000000
--- a/configs/dataset/flowbot.yaml
+++ /dev/null
@@ -1,11 +0,0 @@
-name: flowbot
-data_dir: ${oc.env:HOME}/datasets/partnet-mobility
-
-dataset: umpnet
-model: flowbot
-# batch_size: 64
-lr: 1e-3
-mask_input_channel: True
-randomize_camera: True
-seed: 42
-n_points: 1200
diff --git a/configs/dataset/trajectory.yaml b/configs/dataset/trajectory.yaml
index 98ec7ac..06caea5 100644
--- a/configs/dataset/trajectory.yaml
+++ b/configs/dataset/trajectory.yaml
@@ -1,14 +1,12 @@
 name: trajectory
 data_dir: ${oc.env:HOME}/datasets/partnet-mobility
 
-dataset: umpnet
-model: trajectory
-# batch_size: 64
-# batch_size: 32
+dataset_type: "doors-only"  # "full-dataset"
+special_req: "half-half-01"  #"fully-closed", "randomly-open" (no special request)
 lr: 1e-3
 mask_input_channel: True
 randomize_camera: True
 randomize_size: False
-augmentation: True
+augmentation: False   # Turn this on with doors-only
 seed: 42
 n_points: 1200
diff --git a/configs/eval.yaml b/configs/eval.yaml
index 093fa90..d5f8292 100644
--- a/configs/eval.yaml
+++ b/configs/eval.yaml
@@ -5,11 +5,8 @@ job_type: ${mode}_${dataset.name}_${model.name} #_wta
 
 defaults:
   # Each of these have their own configuration parameters.
-  # - dataset: flowbot
   - dataset: trajectory
-  # - model: diffuser_pn++ #pn++
-  - model: diffuser_pndit
-  # - model: pn++
+  - model: diffuser_hispndit
 
   # A set of inference settings for the model. Note that these may be different
   # from / or a subset of the training settings. This is that we don't have to
@@ -28,52 +25,7 @@ seed: 42
 # like if you want multiple checkpoints simultaneously, etc.
 checkpoint:
   # If we want to load a model for a specific run, we can change that here.
-  # run_id: ???
-
-  # ArtflowNet:
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-
-  # Diffuser:
-  # run_id: 5pp284n8  # traj1
-  # run_id: aww6u0ye  # traj5
-  # run_id: f2abrctk  # traj10
-
-  # Flowbot no mask
-  # run_id: 4xb2c95k
-
-  # Flowbot half-half
-  # run_id: 302dfxe5
-  # Flowbot fully closed
-  # run_id: 8699d8oe
-  # Flowbot doors
-  # run_id: bjhgfn3n
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: ljpdwcna  # traj 7
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-
-  # run_id: 3hyzie1q  # door half-half diffusion pn++
-  # run_id: riwpizim  # door half-half diffusion dgdit
-  # run_id: 6ftiy79l  # door half-half diffusion dit
-  # run_id: 7be7n97m    # door half-half flowbot
-
-  # run_id: gyhracxt   # fullset half-half flowbot
-  # run_id: 1n4zm8rt   # fullset half-half diffusion dit
-  # run_id: 9v8h8jsn   # fullset half-half diffusion dgdit
-
-  # run_id: libhkzgt   # fullset half-half newsplit flowbot
-  # run_id: 9s85cqw7   # fullset half-half newsplit diffusion dit
-  # run_id: fj50vjmg     # fullset half-half newsplit diffusion pndit
-  run_id: lntyypny     # fullset half-half newsplit diffusion pndit wobn
-
+  run_id: ???
   reference: ${wandb.entity}/${wandb.project}/model-${checkpoint.run_id}:best
 
 resources:
@@ -85,22 +37,6 @@ resources:
 wandb:
   # The group ***should*** be the same as the training group (so it can be bundled)
   # nicely in the UI. But you might have a one-off eval or something.
-  # group: ???
-  # group: flowbot  # flowbot
-  # group: traj1  # traj 1 experiment-sb009k1f
-  # group: traj5  # traj 5 experiment-y2kdp0hq
-  # group: traj1
-  # group: diffuser1
-  # group: flowbot_nomask
-  # group: flowbot_mixed
-  # group: flowbot_heldout
-  # group: flowbot_closed
-  # reference: /home/yishu/flowbothd/logs/train_trajectory/2023-09-14/00-11-03/checkpoints/epoch=3819-step=3002520.ckpt  # diffuser1
-  # group: traj7  # traj 5 experiment-y2kdp0hq
-  # group: traj1
-
-  # group: door-half-half
-  # group: fullset-half-half
-  group: fullset-half-half-newsplit
+  group: ???
 
 metric_output_dir: './logs'
diff --git a/configs/eval_history.yaml b/configs/eval_history.yaml
index 0e47174..aa150a1 100644
--- a/configs/eval_history.yaml
+++ b/configs/eval_history.yaml
@@ -5,10 +5,8 @@ job_type: ${mode}_${dataset.name}_${model.name} #_wta
 
 defaults:
   # Each of these have their own configuration parameters.
-  # - dataset: flowbot
   - dataset: trajectory
-  - model: diffuser_hisdit
-  # - model: diffuser_hisdit
+  - model: diffuser_hispndit
 
   # A set of inference settings for the model. Note that these may be different
   # from / or a subset of the training settings. This is that we don't have to
@@ -27,57 +25,7 @@ seed: 42
 # like if you want multiple checkpoints simultaneously, etc.
 checkpoint:
   # If we want to load a model for a specific run, we can change that here.
-  # run_id: ???
-
-  # ArtflowNet:
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-
-  # Diffuser:
-  # run_id: 5pp284n8  # traj1
-  # run_id: aww6u0ye  # traj5
-  # run_id: f2abrctk  # traj10
-
-  # Flowbot no mask
-  # run_id: 4xb2c95k
-
-  # Flowbot half-half
-  # run_id: 302dfxe5
-  # Flowbot fully closed
-  # run_id: 8699d8oe
-  # Flowbot doors
-  # run_id: bjhgfn3n
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: ljpdwcna  # traj 7
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-
-  # run_id: 3hyzie1q  # door half-half diffusion pn++
-  # run_id: riwpizim  # door half-half diffusion dgdit
-  # run_id: 6ftiy79l  # door half-half diffusion dit
-  # run_id: 7be7n97m    # door half-half flowbot
-
-  # run_id: gyhracxt   # fullset half-half flowbot
-  # run_id: 1n4zm8rt   # fullset half-half diffusion dit
-  # run_id: 9v8h8jsn   # fullset half-half diffusion dgdit
-
-  # run_id: libhkzgt   # fullset half-half newsplit flowbot
-  # run_id: 9s85cqw7   # fullset half-half newsplit diffusion dit
-  # run_id: fj50vjmg     # fullset half-half newsplit diffusion pndit
-
-  # run_id: uphfwwg8        # door history with batch norm
-  # run_id: kttwciki         # door history w/o batch norm
-  # run_id: 5ora3vsz
-
-  # History
-  run_id: c7l4ss3d  # latent everywhere, learnable feature for 0 history
+  run_id: ???
 
   reference: ${wandb.entity}/${wandb.project}/model-${checkpoint.run_id}:best
 
@@ -90,25 +38,6 @@ resources:
 wandb:
   # The group ***should*** be the same as the training group (so it can be bundled)
   # nicely in the UI. But you might have a one-off eval or something.
-  # group: ???
-  # group: flowbot  # flowbot
-  # group: traj1  # traj 1 experiment-sb009k1f
-  # group: traj5  # traj 5 experiment-y2kdp0hq
-  # group: traj1
-  # group: diffuser1
-  # group: flowbot_nomask
-  # group: flowbot_mixed
-  # group: flowbot_heldout
-  # group: flowbot_closed
-  # reference: /home/yishu/flowbothd/logs/train_trajectory/2023-09-14/00-11-03/checkpoints/epoch=3819-step=3002520.ckpt  # diffuser1
-  # group: traj7  # traj 5 experiment-y2kdp0hq
-  # group: traj1
-
-  # group: door-half-half
-  # group: fullset-half-half
-  # group: fullset-half-half-newsplit
-  # group: door_history
-  # group: fullset_history
-  group: fullset-half-half-history
+  group: ???
 
 metric_output_dir: './logs'
diff --git a/configs/eval_sim.yaml b/configs/eval_sim.yaml
index bdcdc4c..717df43 100644
--- a/configs/eval_sim.yaml
+++ b/configs/eval_sim.yaml
@@ -5,10 +5,8 @@ job_type: ${mode}_${dataset.name}_${model.name}
 
 defaults:
   # Each of these have their own configuration parameters.
-  - dataset: trajectory  # flowbot
-  - model: diffuser_hispndit #pn++
-  # - model: diffuser_pndit
-  # - model: pn++
+  - dataset: trajectory
+  - model: diffuser_hispndit
 
   # A set of inference settings for the model. Note that these may be different
   # from / or a subset of the training settings. This is that we don't have to
@@ -35,48 +33,7 @@ history_filter: True # True
 # like if you want multiple checkpoints simultaneously, etc.
 checkpoint:
   # If we want to load a model for a specific run, we can change that here.
-  # run_id: ???
-
-  # ArtflowNet:
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-
-
-  # Flowbot half-half
-  # run_id: 302dfxe5
-  # run_id: 4xb2c95k
-
-  # Flowbot origianl:
-  # run_id: e0s0hxh1
-  # run_id: hyrdcsrg
-
-  # Diffuser:
-  # run_id: 5pp284n8  # traj1
-  # run_id: o69oit80  # traj5
-  # run_id: 136gqqta  # traj10
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: ljpdwcna  # traj 7
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-
-  # run_id: 3hyzie1q  # door half-half diffusion pn++
-  # run_id: riwpizim  # door half-half diffusion dgdit
-  # run_id: 6ftiy79l  # door half-half diffusion dit
-  # run_id: 7be7n97m    # door half-half flowbot
-
-  # run_id: libhkzgt   # fullset half-half newsplit flowbot
-  run_id: hyrdcsrg   # fullset randomly opened flowbot
-  # run_id: 9s85cqw7   # fullset half-half newsplit diffusion dit
-  # run_id: fj50vjmg   # fullset half-half newsplit diffusion pndit
-  # run_id: c7l4ss3d  # latent everywhere, learnable feature for 0 history
-  # run_id: cb8x0mc3  # latent everywhere, learnable feature for 0 history, 0/1 everystep
+  run_id: ???
 
   reference: ${wandb.entity}/${wandb.project}/model-${checkpoint.run_id}:best
 
@@ -89,24 +46,7 @@ resources:
 wandb:
   # The group ***should*** be the same as the training group (so it can be bundled)
   # nicely in the UI. But you might have a one-off eval or something.
-  # group: ???
-  # group: experiment-9uex8k1u  # flowbot
-  # group: traj1  # traj 1 experiment-sb009k1f
-  # group: traj5  # traj 5 experiment-y2kdp0hq
-  # group: traj20
-  # group: diffuser10
-  # group: flowbot_nomask
-  # group: flowbot_mixed
-  # group: flowbot  # flowbot
-  # group: traj1  # traj 1 experiment-sb009k1f
-  # group: traj20  # traj 5 experiment-y2kdp0hq
-  # group: traj7
-
-  # group: door-half-half
-  # group: fullset-half-half
-  # group: fullset-half-half-newsplit
-  group: fullset-half-half-history
-
+  group: ???
 
 
 metric_output_dir: './logs'
diff --git a/configs/eval_sim_gt.yaml b/configs/eval_sim_gt.yaml
deleted file mode 100644
index 5c14645..0000000
--- a/configs/eval_sim_gt.yaml
+++ /dev/null
@@ -1,55 +0,0 @@
-mode: sim_gt
-
-# This is somewhat arbitrary.
-job_type: ${mode}_${dataset.name}
-
-defaults:
-  # Each of these have their own configuration parameters.
-  - dataset: trajectory
-  - model: pn++
-
-  # A set of inference settings for the model. Note that these may be different
-  # from / or a subset of the training settings. This is that we don't have to
-  # provide, like, a learning rater or something to eval.
-  - inference: ${dataset}_${model}
-
-  # Simple shared imports.
-  - _logging
-
-  # Override.
-  - _self_
-
-seed: 42
-gui: False
-website: True
-website_port: None
-
-# This is the checkpoint that we're evaluating. You can change this to whatever you need,
-# like if you want multiple checkpoints simultaneously, etc.
-checkpoint:
-  # If we want to load a model for a specific run, we can change that here.
-  run_id: Null
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-  reference: ${wandb.entity}/${wandb.project}/model-${checkpoint.run_id}:best
-
-resources:
-  num_workers: 30
-  n_proc_per_worker: 2
-  gpus:
-    - 0
-
-wandb:
-  # The group ***should*** be the same as the training group (so it can be bundled)
-  # nicely in the UI. But you might have a one-off eval or something.
-  group: door-half-half
-  # group: flowbot  # flowbot
-  # group: traj1  # traj 1 experiment-sb009k1f
-  # group: traj5  # traj 5 experiment-y2kdp0hq
-  # group: traj5
-
-metric_output_dir: './logs'
diff --git a/configs/eval_sim_switch.yaml b/configs/eval_sim_switch.yaml
deleted file mode 100644
index 466f0fd..0000000
--- a/configs/eval_sim_switch.yaml
+++ /dev/null
@@ -1,102 +0,0 @@
-mode: sim_policies
-
-# This is somewhat arbitrary.
-job_type: ${mode}_${dataset.name}_${model.name}+${switch_model.name}
-
-defaults:
-  # Each of these have their own configuration parameters.
-  - dataset: trajectory  # flowbot
-  - model: diffuser_pndit #pn++
-  - switch_model: pn++ # diffuser_hisdit
-  # - model: pn++
-
-  # A set of inference settings for the model. Note that these may be different
-  # from / or a subset of the training settings. This is that we don't have to
-  # provide, like, a learning rater or something to eval.
-  - inference: ${dataset}_${model}
-  - switch_inference: ${dataset}_${switch_model}
-
-  # Simple shared imports.
-  - _logging
-
-  # Override.
-  - _self_
-
-seed: 42
-gui: False
-website: True
-website_port: 9002  # 9001, 9002, 9003
-
-# This is the checkpoint that we're evaluating. You can change this to whatever you need,
-# like if you want multiple checkpoints simultaneously, etc.
-checkpoint:
-  # If we want to load a model for a specific run, we can change that here.
-  # run_id: ???
-
-  # ArtflowNet:
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-
-
-  # Flowbot half-half
-  # run_id: 302dfxe5
-  # run_id: 4xb2c95k
-
-  # Diffuser:
-  # run_id: 5pp284n8  # traj1
-  # run_id: o69oit80  # traj5
-  # run_id: 136gqqta  # traj10
-  # run_id: nxe7uxh4  # flowbot
-  # run_id: ah26pym3  # traj 1
-  # run_id: l5stf32k  # traj 5
-  # run_id: ljpdwcna  # traj 7
-  # run_id: m2twtvm7   # traj 10
-  # run_id: yiel0na4   # traj 15
-  # run_id: zmnrssob  # traj 20
-
-  # run_id: 3hyzie1q  # door half-half diffusion pn++
-  # run_id: riwpizim  # door half-half diffusion dgdit
-  # run_id: 6ftiy79l  # door half-half diffusion dit
-  # run_id: 7be7n97m    # door half-half flowbot
-
-  # run_id: libhkzgt   # fullset half-half newsplit flowbot
-  run_id: hyrdcsrg   # fullset randomly opened flowbot
-  # run_id: 9s85cqw7   # fullset half-half newsplit diffusion dit
-  # run_id: fj50vjmg   # fullset half-half newsplit diffusion pndit
-  # run_id: c7l4ss3d  # latent everywhere, learnable feature for 0 history
-  # run_id: szvd1rh7  # latent everywhere, learnable feature for 0 history, 0/1 everystep
-
-  reference: ${wandb.entity}/${wandb.project}/model-${checkpoint.run_id}:best
-
-resources:
-  num_workers: 30
-  n_proc_per_worker: 2
-  gpus:
-    - 0
-
-wandb:
-  # The group ***should*** be the same as the training group (so it can be bundled)
-  # nicely in the UI. But you might have a one-off eval or something.
-  # group: ???
-  # group: experiment-9uex8k1u  # flowbot
-  # group: traj1  # traj 1 experiment-sb009k1f
-  # group: traj5  # traj 5 experiment-y2kdp0hq
-  # group: traj20
-  # group: diffuser10
-  # group: flowbot_nomask
-  # group: flowbot_mixed
-  # group: flowbot  # flowbot
-  # group: traj1  # traj 1 experiment-sb009k1f
-  # group: traj20  # traj 5 experiment-y2kdp0hq
-  # group: traj7
-
-  # group: door-half-half
-  group: fullset-half-half
-  # group: fullset-half-half-newsplit
-  # group: fullset-half-half-history
-
-metric_output_dir: './logs'
diff --git a/configs/inference/trajectory_diffuser_dgdit.yaml b/configs/inference/trajectory_diffuser_dgdit.yaml
index 77b9ee7..d1216f0 100644
--- a/configs/inference/trajectory_diffuser_dgdit.yaml
+++ b/configs/inference/trajectory_diffuser_dgdit.yaml
@@ -1,4 +1,3 @@
 name: trajectory_diffuser_dgdit
 batch_size: 64
 trajectory_len: 1
-mpc_step: 15
diff --git a/configs/inference/trajectory_diffuser_dit.yaml b/configs/inference/trajectory_diffuser_dit.yaml
index 2b619c8..8dfb175 100644
--- a/configs/inference/trajectory_diffuser_dit.yaml
+++ b/configs/inference/trajectory_diffuser_dit.yaml
@@ -1,4 +1,3 @@
 name: trajectory_diffuser_dit
 batch_size: 64
 trajectory_len: 1
-mpc_step: 15
diff --git a/configs/inference/trajectory_diffuser_hisdit.yaml b/configs/inference/trajectory_diffuser_hisdit.yaml
index 57e67c9..594cbed 100644
--- a/configs/inference/trajectory_diffuser_hisdit.yaml
+++ b/configs/inference/trajectory_diffuser_hisdit.yaml
@@ -1,4 +1,3 @@
 name: trajectory_diffuser_hisdit
 batch_size: 64
 trajectory_len: 1
-mpc_step: 15
diff --git a/configs/inference/trajectory_diffuser_hispndit.yaml b/configs/inference/trajectory_diffuser_hispndit.yaml
index ac2b091..228dd13 100644
--- a/configs/inference/trajectory_diffuser_hispndit.yaml
+++ b/configs/inference/trajectory_diffuser_hispndit.yaml
@@ -1,4 +1,3 @@
 name: trajectory_diffuser_hispndit
 batch_size: 64
 trajectory_len: 1
-mpc_step: 15
diff --git a/configs/inference/trajectory_diffuser_pn++.yaml b/configs/inference/trajectory_diffuser_pn++.yaml
index cf27947..8421c0b 100644
--- a/configs/inference/trajectory_diffuser_pn++.yaml
+++ b/configs/inference/trajectory_diffuser_pn++.yaml
@@ -1,4 +1,3 @@
 name: trajectory_diffuser_pn++
 batch_size: 64
 trajectory_len: 1
-mpc_step: 15
diff --git a/configs/inference/trajectory_diffuser_pndit.yaml b/configs/inference/trajectory_diffuser_pndit.yaml
index fd031e3..b68e6a9 100644
--- a/configs/inference/trajectory_diffuser_pndit.yaml
+++ b/configs/inference/trajectory_diffuser_pndit.yaml
@@ -1,4 +1,3 @@
 name: trajectory_diffuser_pndit
 batch_size: 64
 trajectory_len: 1
-mpc_step: 15
diff --git a/configs/switch_inference/trajectory_diffuser_hisdit.yaml b/configs/switch_inference/trajectory_diffuser_hisdit.yaml
deleted file mode 100644
index 57e67c9..0000000
--- a/configs/switch_inference/trajectory_diffuser_hisdit.yaml
+++ /dev/null
@@ -1,4 +0,0 @@
-name: trajectory_diffuser_hisdit
-batch_size: 64
-trajectory_len: 1
-mpc_step: 15
diff --git a/configs/switch_inference/trajectory_diffuser_hispndit.yaml b/configs/switch_inference/trajectory_diffuser_hispndit.yaml
deleted file mode 100644
index ac2b091..0000000
--- a/configs/switch_inference/trajectory_diffuser_hispndit.yaml
+++ /dev/null
@@ -1,4 +0,0 @@
-name: trajectory_diffuser_hispndit
-batch_size: 64
-trajectory_len: 1
-mpc_step: 15
diff --git a/configs/switch_inference/trajectory_pn++.yaml b/configs/switch_inference/trajectory_pn++.yaml
deleted file mode 100644
index bace490..0000000
--- a/configs/switch_inference/trajectory_pn++.yaml
+++ /dev/null
@@ -1,5 +0,0 @@
-name: trajectory_pn++
-batch_size: 64
-trajectory_len: 1
-# mask_input_channel: True
-mask_input_channel: False
diff --git a/configs/switch_model/diffuser_hisdit.yaml b/configs/switch_model/diffuser_hisdit.yaml
deleted file mode 100644
index de76189..0000000
--- a/configs/switch_model/diffuser_hisdit.yaml
+++ /dev/null
@@ -1,12 +0,0 @@
-# Diffuser params
-name: diffuser_hisdit
-history_model: encoder # encoder  # translator
-time_proj_dim: 64
-time_embed_dim: 64
-freq_shift: 0
-flip_sin_to_cos: True
-num_train_timesteps: 100
-num_inference_timesteps: 100
-history_len: 1
-history_dim: 128
-batch_norm: False
diff --git a/configs/switch_model/diffuser_hispndit.yaml b/configs/switch_model/diffuser_hispndit.yaml
deleted file mode 100644
index e9b2c2d..0000000
--- a/configs/switch_model/diffuser_hispndit.yaml
+++ /dev/null
@@ -1,12 +0,0 @@
-# Diffuser params
-name: diffuser_hispndit
-history_model: encoder # encoder  # translator
-time_proj_dim: 64
-time_embed_dim: 64
-freq_shift: 0
-flip_sin_to_cos: True
-num_train_timesteps: 100
-num_inference_timesteps: 100
-history_len: 1
-history_dim: 128
-batch_norm: True
diff --git a/configs/switch_model/pn++.yaml b/configs/switch_model/pn++.yaml
deleted file mode 100644
index 5fb9fe5..0000000
--- a/configs/switch_model/pn++.yaml
+++ /dev/null
@@ -1 +0,0 @@
-name: pn++
diff --git a/configs/train.yaml b/configs/train.yaml
index c402e4d..6244c92 100644
--- a/configs/train.yaml
+++ b/configs/train.yaml
@@ -6,12 +6,7 @@ job_type: ${mode}_${dataset.name}_${model.name}
 defaults:
   # Each of these have their own configuration parameters.
   - dataset: trajectory
-  # - model: diffuser_dit
-  - model: diffuser_hispndit
-  # - model: pn++ # diffuser_pn++
-
-  # History:
-  # - model: diffuser_hisdit
+  - model: diffuser_hispndit   # Or choose any other model that exists in the configs/model directory
 
   # We assume a different training config for each dataset/model pair.
   - training: ${dataset}_${model}
diff --git a/configs/train_synthetic.yaml b/configs/train_synthetic.yaml
deleted file mode 100644
index 2833ddb..0000000
--- a/configs/train_synthetic.yaml
+++ /dev/null
@@ -1,31 +0,0 @@
-mode: train_synthetic
-
-# This is somewhat arbitrary.
-job_type: ${mode}_${dataset.name}
-
-defaults:
-  # Each of these have their own configuration parameters.
-  - dataset: synthetic
-  - model: diffuser
-  # - model: artflownet
-
-  # We assume a different training config for each dataset/model pair.
-  - training: ${dataset}_${model}
-
-  # Simple shared imports.
-  - _logging
-
-  # Override.
-  - _self_
-
-seed: 42
-
-resources:
-  num_workers: 30
-  n_proc_per_worker: 2
-  gpus:
-    - 0
-
-wandb:
-  # Assume no group provided, we will create a default one.
-  group: Null
diff --git a/configs/training/synthetic_diffuser_pn++.yaml b/configs/training/synthetic_diffuser_pn++.yaml
deleted file mode 100644
index 89301b0..0000000
--- a/configs/training/synthetic_diffuser_pn++.yaml
+++ /dev/null
@@ -1,14 +0,0 @@
-name: synthetic_diffuser_pn++
-lr: 1e-4
-# lr_warmup_steps: 100
-# batch_size: 64
-# epochs: 5000
-# train_sample_number : 50300
-check_val_every_n_epoch: 5
-trajectory_len: 1
-mode: delta
-
-lr_warmup_steps: 5
-batch_size: 1
-epochs: 10000  # For toy
-train_sample_number : 2  # For toy
diff --git a/configs/training/synthetic_pn++.yaml b/configs/training/synthetic_pn++.yaml
deleted file mode 100644
index 8b1476a..0000000
--- a/configs/training/synthetic_pn++.yaml
+++ /dev/null
@@ -1,8 +0,0 @@
-name: synthetic_pn++
-lr: 1e-3
-batch_size: 1
-epochs: 10000
-check_val_every_n_epoch: 5
-trajectory_len: 1
-mode: "delta"
-mask_input_channel: False
diff --git a/configs/training/trajectory_diffuser_hispndit.yaml b/configs/training/trajectory_diffuser_hispndit.yaml
index dc1165a..b94d313 100644
--- a/configs/training/trajectory_diffuser_hispndit.yaml
+++ b/configs/training/trajectory_diffuser_hispndit.yaml
@@ -2,16 +2,10 @@ name: trajectory_diffuser_hispndit
 lr: 1e-4
 lr_warmup_steps: 100
 batch_size: 128
-epochs: 1000 # 500
+epochs: 1000
 train_sample_number : None # To be filled in train.py
 check_val_every_n_epoch: 5 # 20
 trajectory_len: 1
-# history_len: 1
 mode: delta
 wta: True
 wta_trial_times: 20
-
-# lr_warmup_steps: 5
-# batch_size: 1
-# epochs: 5000  # For toy
-# train_sample_number : 2  # For toy
diff --git a/docs/.gitignore b/docs/.gitignore
deleted file mode 100644
index 45ddf0a..0000000
--- a/docs/.gitignore
+++ /dev/null
@@ -1 +0,0 @@
-site/
diff --git a/docs/docs/index.md b/docs/docs/index.md
deleted file mode 100644
index c3fd49d..0000000
--- a/docs/docs/index.md
+++ /dev/null
@@ -1,5 +0,0 @@
-# flowbothd
-
-Some sample text for the website.
-
-Find documentation on [material-mkdocs here](https://squidfunk.github.io/mkdocs-material/).
diff --git a/docs/mkdocs.yml b/docs/mkdocs.yml
deleted file mode 100644
index afa49df..0000000
--- a/docs/mkdocs.yml
+++ /dev/null
@@ -1,13 +0,0 @@
-site_name: flowbothd
-theme:
-  name: material
-plugins:
-- search
-- mkdocstrings:
-    default_handler: python
-    handlers:
-      python:
-        options:
-          heading_level: 2
-          show_root_heading: True
-          show_source: False
diff --git a/pyproject.toml b/pyproject.toml
index cf7774d..84c69b4 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -1,30 +1,20 @@
 [project]
 name = "flowbothd"
 version = "0.1.0"
-description = "A Python Package Template"
+description = "FlowBotHD: History-Aware Diffusion Handling Ambiguities in Articulated Objects Manipulation"
 readme = "README.md"
 requires-python = ">=3.6"
 license = { file = "LICENSE.txt" }
 authors = [{ email = "yishul@andrew.cmu.edu", name = "Yishu Li" }]
 dependencies = [
-  "flowbot3d @ git+https://github.com/r-pad/flowbot3d.git",
   "hydra-core == 1.3.2",
   "lightning == 2.0.3",
   "omegaconf == 2.3.0",
   "pandas",
-  "rpad-pybullet-envs @ git+https://github.com/r-pad/pybullet_envs.git",
-  "rpad-pybullet-libs @ git+https://github.com/r-pad/pybullet_libs.git",
-  "rpad-partnet-mobility-utils[pybullet] @ git+https://github.com/r-pad/partnet_mobility_utils",
-  "rpad-pyg @ git+https://github.com/r-pad/pyg_libs",
-  # pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117
-  "torch == 1.13.1",       # CUDA 11.7
-  "torchmetrics",
-  "torchvision == 0.14.1", # CUDA 11.8
-  "wandb == 0.15.4",
 ]
 
 [build-system]
-requires = ["setuptools >= 62.3.2", "setuptools-scm", "wheel"]
+requires = ["setuptools >= 58.0.1", "setuptools-scm", "wheel"]
 build-backend = "setuptools.build_meta"
 
 [project.optional-dependencies]
@@ -53,7 +43,7 @@ profile = "black"
 known_third_party = "wandb"
 
 [tool.mypy]
-python_version = 3.8
+python_version = 3.9
 warn_return_any = true
 warn_unused_configs = true
 mypy_path = "src"
diff --git a/scripts/eval_diffuser.py b/scripts/eval_diffuser.py
deleted file mode 100644
index f5e69c3..0000000
--- a/scripts/eval_diffuser.py
+++ /dev/null
@@ -1,353 +0,0 @@
-# Diffuser evaluation scripts
-from pathlib import Path
-
-import hydra
-import lightning as L
-import omegaconf
-import pandas as pd
-import rpad.partnet_mobility_utils.dataset as rpd
-import rpad.pyg.nets.pointnet2 as pnp
-import torch
-import tqdm
-import wandb
-
-from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule
-from flowbothd.metrics.trajectory import (
-    flow_metrics,
-    normalize_trajectory,
-)
-from flowbothd.models.flow_diffuser_dgdit import (
-    FlowTrajectoryDiffuserInferenceModule_DGDiT,
-)
-from flowbothd.models.flow_diffuser_dit import (
-    FlowTrajectoryDiffuserInferenceModule_DiT,
-)
-from flowbothd.models.flow_trajectory_diffuser import (
-    FlowTrajectoryDiffuserInferenceModule_PN2,
-)
-from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn
-
-data_module_class = {
-    "trajectory": FlowTrajectoryDataModule,
-}
-
-inference_module_class = {
-    "trajectory_diffuser_pn++": FlowTrajectoryDiffuserInferenceModule_PN2,
-    "trajectory_diffuser_dgdit": FlowTrajectoryDiffuserInferenceModule_DGDiT,
-    "trajectory_diffuser_dit": FlowTrajectoryDiffuserInferenceModule_DiT,
-}
-
-
-@torch.no_grad()
-@hydra.main(config_path="../configs", config_name="eval", version_base="1.3")
-def main(cfg):
-    ######################################################################
-    # Torch settings.
-    ######################################################################
-
-    # Make deterministic + reproducible.
-    torch.backends.cudnn.deterministic = True
-    torch.backends.cudnn.benchmark = False
-
-    # Since most of us are training on 3090s+, we can use mixed precision.
-    torch.set_float32_matmul_precision("highest")
-
-    # Global seed for reproducibility.
-    L.seed_everything(42)
-
-    ######################################################################
-    # Create the datamodule.
-    # Should be the same one as in training, but we're gonna use val+test
-    # dataloaders.
-    ######################################################################
-    trajectory_len = cfg.inference.trajectory_len
-    # Create FlowBot dataset
-    datamodule = data_module_class[cfg.dataset.name](
-        root=cfg.dataset.data_dir,
-        batch_size=cfg.inference.batch_size,
-        num_workers=cfg.resources.num_workers,
-        n_proc=cfg.resources.n_proc_per_worker,
-        seed=cfg.seed,
-        trajectory_len=trajectory_len,  # Only used when inference trajectory model
-        toy_dataset={
-            "id": "door-full-new",
-            "train-train": [
-                "8877",
-                "8893",
-                "8897",
-                "8903",
-                "8919",
-                "8930",
-                "8961",
-                "8997",
-                "9016",
-                "9032",
-                "9035",
-                "9041",
-                "9065",
-                "9070",
-                "9107",
-                "9117",
-                "9127",
-                "9128",
-                "9148",
-                "9164",
-                "9168",
-                "9277",
-                "9280",
-                "9281",
-                "9288",
-                "9386",
-                "9388",
-                "9410",
-            ],
-            "train-test": [
-                "8877",
-                "8893",
-                "8897",
-                "8903",
-                "8919",
-                "8930",
-                "8961",
-                "8997",
-                "9016",
-                "9032",
-                "9035",
-                "9041",
-                "9065",
-                "9070",
-                "9107",
-                "9117",
-                "9127",
-                "9128",
-                "9148",
-                "9164",
-                "9168",
-                "9277",
-                "9280",
-                "9281",
-                "9288",
-                "9386",
-                "9388",
-                "9410",
-            ],
-            "test": [
-                "8877",
-                "8893",
-                "8897",
-                "8903",
-                "8919",
-                "8930",
-                "8961",
-                "8997",
-                "9016",
-                "9032",
-                "9035",
-                "9041",
-                "9065",
-                "9070",
-                "9107",
-                "9117",
-                "9127",
-                "9128",
-                "9148",
-                "9164",
-                "9168",
-                "9277",
-                "9280",
-                "9281",
-                "9288",
-                "9386",
-                "9388",
-                "9410",
-            ],
-        },
-    )
-
-    ######################################################################
-    # Set up logging in WandB.
-    # This is a different job type (eval), but we want it all grouped
-    # together. Notice that we use our own logging here (not lightning).
-    ######################################################################
-
-    # Create a run.
-    run = wandb.init(
-        entity=cfg.wandb.entity,
-        project=cfg.wandb.project,
-        dir=cfg.wandb.save_dir,
-        config=omegaconf.OmegaConf.to_container(
-            cfg, resolve=True, throw_on_missing=True
-        ),
-        job_type=cfg.job_type,
-        save_code=True,  # This just has the main script.
-        group=cfg.wandb.group,
-    )
-
-    # Log the code.
-    wandb.run.log_code(
-        root=PROJECT_ROOT,
-        include_fn=match_fn(
-            dirs=["configs", "scripts", "src"],
-            extensions=[".py", ".yaml"],
-        ),
-    )
-
-    ######################################################################
-    # Create the network(s) which will be evaluated (same as training).
-    # You might want to put this into a "create_network" function
-    # somewhere so train and eval can be the same.
-    #
-    # We'll also load the weights.
-    ######################################################################
-
-    in_channels = 3 * cfg.inference.trajectory_len + cfg.model.time_embed_dim
-    network = pnp.PN2Dense(
-        in_channels=in_channels,
-        out_channels=3 * trajectory_len,
-        p=pnp.PN2DenseParams(),
-    )
-
-    # Get the checkpoint file. If it's a wandb reference, download.
-    # Otherwise look to disk.
-    checkpoint_reference = cfg.checkpoint.reference
-    if checkpoint_reference.startswith(cfg.wandb.entity):
-        # download checkpoint locally (if not already cached)
-        artifact_dir = cfg.wandb.artifact_dir
-        artifact = run.use_artifact(checkpoint_reference, type="model")
-        ckpt_file = artifact.get_path("model.ckpt").download(root=artifact_dir)
-    else:
-        ckpt_file = checkpoint_reference
-    # ckpt_file = '/home/yishu/flowbothd/logs/train_trajectory/2023-09-11/19-08-26/checkpoints/epoch=5004-step=3933930.ckpt'
-
-    # # Load the network weights.
-    # ckpt = torch.load(ckpt_file)
-    # network.load_state_dict(
-    #     {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
-    # )
-
-    ######################################################################
-    # Create an inference module, which is basically just a bare-bones
-    # class which runs the model. In this example, we only implement
-    # the "predict_step" function, which may not be the blessed
-    # way to do it vis a vis lightning, but whatever.
-    #
-    # If this is a downstream application or something, you might
-    # want to implement a different interface (like with a "predict"
-    # function), so you can pass in un-batched observations from an
-    # environment, for instance.
-    ######################################################################
-
-    model = inference_module_class[cfg.dataset.name](
-        network, inference_cfg=cfg.inference, model_cfg=cfg.model
-    )
-    model.load_from_ckpt(ckpt_file)
-    model.eval()
-
-    ######################################################################
-    # Create the trainer.
-    # Bit of a misnomer here, we're not doing training. But we are gonna
-    # use it to set up the model appropriately and do all the batching
-    # etc.
-    #
-    # If this is a different kind of downstream eval, chuck this block.
-    ######################################################################
-
-    trainer = L.Trainer(
-        accelerator="gpu",
-        devices=cfg.resources.gpus,
-        precision="32-true",
-        logger=False,
-    )
-
-    ######################################################################
-    # Run the model on the train/val/test sets.
-    # This outputs a list of dictionaries, one for each batch. This
-    # is annoying to work with, so later we'll flatten.
-    #
-    # If a downstream eval, you can swap it out with whatever the eval
-    # function is.
-    ######################################################################
-
-    dataloaders = [
-        (datamodule.train_val_dataloader(), "train"),
-        (datamodule.val_dataloader(), "val"),
-        (datamodule.unseen_dataloader(), "test"),
-    ]
-
-    all_objs = (
-        rpd.UMPNET_TRAIN_TRAIN_OBJS + rpd.UMPNET_TRAIN_TEST_OBJS + rpd.UMPNET_TEST_OBJS
-    )
-    id_to_obj_class = {obj_id: obj_class for obj_id, obj_class in all_objs}
-
-    for loader, name in dataloaders:
-        metrics = []
-        outputs = trainer.predict(
-            model,
-            dataloaders=[loader],
-        )
-
-        for batch, preds in zip(tqdm.tqdm(loader), outputs):
-            st = 0
-            for data in batch.to_data_list():
-                f_pred = preds[st : st + data.num_nodes]
-                f_pred = f_pred.reshape(f_pred.shape[0], -1, 3)
-
-                # Ignore nan predictions for now...
-                if torch.isnan(f_pred).sum() != 0:
-                    continue
-
-                f_ix = data.mask.bool()
-                if cfg.dataset.name == "trajectory":
-                    f_target = data.delta
-                else:
-                    f_target = data.flow
-                    f_target = f_target.reshape(f_target.shape[0], -1, 3)
-
-                f_pred = normalize_trajectory(f_pred)
-                f_target = normalize_trajectory(f_target)
-                rmse, cos_dist, mag_error = flow_metrics(f_pred[f_ix], f_target[f_ix])
-
-                metrics.append(
-                    {
-                        "id": data.id,
-                        "obj_class": id_to_obj_class[data.id],
-                        "metrics": {
-                            "rmse": rmse.cpu().item(),
-                            "cos_dist": cos_dist.cpu().item(),
-                            "mag_error": mag_error.cpu().item(),
-                        },
-                    }
-                )
-
-                st += data.num_nodes
-
-        rows = [
-            (
-                m["id"],
-                m["obj_class"],
-                m["metrics"]["rmse"],
-                m["metrics"]["cos_dist"],
-                m["metrics"]["mag_error"],
-            )
-            for m in metrics
-        ]
-        raw_df = pd.DataFrame(
-            rows, columns=["id", "category", "rmse", "cos_dist", "mag_error"]
-        )
-        df = raw_df.groupby("category").mean(numeric_only=True)
-        df.loc["unweighted_mean"] = raw_df.mean(numeric_only=True)
-        df.loc["class_mean"] = df.mean()
-
-        out_file = Path(cfg.log_dir) / f"{cfg.dataset.name}_{trajectory_len}_{name}.csv"
-        print(out_file)
-        # if out_file.exists():
-        #     raise ValueError(f"{out_file} already exists...")
-        df.to_csv(out_file, float_format="%.3f")
-
-        # Log the metrics + table to wandb.
-        table = wandb.Table(dataframe=df.reset_index())
-        run.log({f"{name}_metric_table": table})
-
-
-if __name__ == "__main__":
-    main()
diff --git a/scripts/eval_diffuser_wta.py b/scripts/eval_diffuser_wta.py
deleted file mode 100644
index e1f92b1..0000000
--- a/scripts/eval_diffuser_wta.py
+++ /dev/null
@@ -1,372 +0,0 @@
-# Diffuser evaluation scripts
-
-import hydra
-import lightning as L
-import omegaconf
-import pandas as pd
-import rpad.pyg.nets.pointnet2 as pnp
-import torch
-import wandb
-
-from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule
-from flowbothd.models.flow_diffuser_dgdit import (
-    FlowTrajectoryDiffuserInferenceModule_DGDiT,
-)
-from flowbothd.models.flow_diffuser_dit import (
-    FlowTrajectoryDiffuserInferenceModule_DiT,
-)
-from flowbothd.models.flow_diffuser_pndit import (
-    FlowTrajectoryDiffuserInferenceModule_PNDiT,
-)
-from flowbothd.models.flow_trajectory_diffuser import (
-    FlowTrajectoryDiffuserInferenceModule_PN2,
-)
-from flowbothd.models.modules.dit_models import DGDiT, DiT, PN2DiT
-from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn
-
-data_module_class = {
-    "trajectory": FlowTrajectoryDataModule,
-}
-
-inference_module_class = {
-    "diffuser_pn++": FlowTrajectoryDiffuserInferenceModule_PN2,
-    "diffuser_dgdit": FlowTrajectoryDiffuserInferenceModule_DGDiT,
-    "diffuser_dit": FlowTrajectoryDiffuserInferenceModule_DiT,
-    "diffuser_pndit": FlowTrajectoryDiffuserInferenceModule_PNDiT,
-    # "trajectory_diffuser_pndit": FlowTrajectoryDiffusionModule_PNDiT,
-}
-
-
-@torch.no_grad()
-@hydra.main(config_path="../configs", config_name="eval", version_base="1.3")
-def main(cfg):
-    ######################################################################
-    # Torch settings.
-    ######################################################################
-
-    # Make deterministic + reproducible.
-    torch.backends.cudnn.deterministic = True
-    torch.backends.cudnn.benchmark = False
-
-    # Since most of us are training on 3090s+, we can use mixed precision.
-    torch.set_float32_matmul_precision("highest")
-
-    # Global seed for reproducibility.
-    L.seed_everything(42)
-
-    ######################################################################
-    # Create the datamodule.
-    # Should be the same one as in training, but we're gonna use val+test
-    # dataloaders.
-    ######################################################################
-    trajectory_len = cfg.inference.trajectory_len
-    toy_dataset = {
-        "id": "door-full-new",
-        "train-train": [
-            "8877",
-            "8893",
-            "8897",
-            "8903",
-            "8919",
-            "8930",
-            "8961",
-            "8997",
-            "9016",
-            "9032",
-            "9035",
-            "9041",
-            "9065",
-            "9070",
-            "9107",
-            "9117",
-            "9127",
-            "9128",
-            "9148",
-            "9164",
-            "9168",
-            "9277",
-            "9280",
-            "9281",
-            "9288",
-            "9386",
-            "9388",
-            "9410",
-        ],
-        "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-        "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-    }
-    # Create FlowBot dataset
-    fully_closed_datamodule = FlowTrajectoryDataModule(
-        root="/home/yishu/datasets/partnet-mobility",
-        batch_size=1,
-        num_workers=30,
-        n_proc=2,
-        seed=42,
-        trajectory_len=1,  # Only used when training trajectory model
-        special_req="fully-closed",
-        # toy_dataset = {
-        #     "id": "door-1",
-        #     "train-train": ["8994", "9035"],
-        #     "train-test": ["8994", "9035"],
-        #     "test": ["8867"],
-        #     # "train-train": ["8867"],
-        #     # "train-test": ["8867"],
-        #     # "test": ["8867"],
-        # }
-        # toy_dataset=toy_dataset,   # 1) Eval on doors
-        toy_dataset=None,  # 2) Eval on all
-    )
-    randomly_opened_datamodule = FlowTrajectoryDataModule(
-        root="/home/yishu/datasets/partnet-mobility",
-        batch_size=1,
-        num_workers=30,
-        n_proc=2,
-        seed=42,
-        trajectory_len=1,  # Only used when training trajectory model
-        special_req=None,
-        # toy_dataset = {
-        #     "id": "door-1",
-        #     "train-train": ["8994", "9035"],
-        #     "train-test": ["8994", "9035"],
-        #     "test": ["8867"],
-        #     # "train-train": ["8867"],
-        #     # "train-test": ["8867"],
-        #     # "test": ["8867"],
-        # }
-        # toy_dataset=toy_dataset,   # 1) Eval on doors
-        toy_dataset=None,  # 2) Eval on all
-    )
-
-    ######################################################################
-    # Set up logging in WandB.
-    # This is a different job type (eval), but we want it all grouped
-    # together. Notice that we use our own logging here (not lightning).
-    ######################################################################
-
-    # Create a run.
-    run = wandb.init(
-        entity=cfg.wandb.entity,
-        project=cfg.wandb.project,
-        dir=cfg.wandb.save_dir,
-        config=omegaconf.OmegaConf.to_container(
-            cfg, resolve=True, throw_on_missing=True
-        ),
-        job_type=cfg.job_type,
-        save_code=True,  # This just has the main script.
-        group=cfg.wandb.group,
-    )
-
-    # Log the code.
-    wandb.run.log_code(
-        root=PROJECT_ROOT,
-        include_fn=match_fn(
-            dirs=["configs", "scripts", "src"],
-            extensions=[".py", ".yaml"],
-        ),
-    )
-
-    ######################################################################
-    # Create the network(s) which will be evaluated (same as training).
-    # You might want to put this into a "create_network" function
-    # somewhere so train and eval can be the same.
-    #
-    # We'll also load the weights.
-    ######################################################################
-
-    if "diffuser" in cfg.model.name:
-        if "pn++" in cfg.model.name:
-            in_channels = 3 * cfg.inference.trajectory_len + cfg.model.time_embed_dim
-        else:
-            in_channels = (
-                3 * cfg.inference.trajectory_len
-            )  # Will add 3 as input channel in diffuser
-    else:
-        in_channels = 1 if cfg.inference.mask_input_channel else 0
-
-    if "pn++" in cfg.model.name:
-        network = pnp.PN2Dense(
-            in_channels=in_channels,
-            out_channels=3 * trajectory_len,
-            p=pnp.PN2DenseParams(),
-        ).cuda()
-    elif "dgdit" in cfg.model.name:
-        network = DGDiT(
-            in_channels=in_channels,
-            depth=5,
-            hidden_size=128,
-            patch_size=1,
-            num_heads=4,
-            n_points=cfg.dataset.n_points,
-        ).cuda()
-    elif "pndit" in cfg.model.name:
-        network = PN2DiT(
-            in_channels=in_channels,
-            depth=5,
-            hidden_size=128,
-            patch_size=1,
-            num_heads=4,
-            n_points=cfg.dataset.n_points,
-        ).cuda()
-    elif "dit" in cfg.model.name:
-        network = DiT(
-            in_channels=in_channels + 3,
-            # depth=5,
-            # hidden_size=128,
-            # num_heads=4,
-            depth=12,
-            hidden_size=384,
-            num_heads=6,
-            learn_sigma=True,
-        ).cuda()
-
-    # Get the checkpoint file. If it's a wandb reference, download.
-    # Otherwise look to disk.
-    # checkpoint_reference = cfg.checkpoint.reference
-    # if checkpoint_reference.startswith(cfg.wandb.entity):
-    #     # download checkpoint locally (if not already cached)
-    #     artifact_dir = cfg.wandb.artifact_dir
-    #     artifact = run.use_artifact(checkpoint_reference, type="model")
-    #     ckpt_file = artifact.get_path("model.ckpt").download(root=artifact_dir)
-    # else:
-    #     ckpt_file = checkpoint_reference
-
-    # ckpt_file = '/home/yishu/flowbothd/logs/train_trajectory_diffuser_dit/2024-03-23/02-47-04/checkpoints/epoch=299-step=235800-val_loss=0.00-weights-only.ckpt'
-    # ckpt_file = '/home/yishu/flowbothd/logs/train_trajectory_diffuser_dgdit/2024-03-23/02-45-56/checkpoints/epoch=259-step=408720-val_loss=0.00-weights-only.ckpt'
-    # ckpt_file = '/home/yishu/flowbothd/logs/train_trajectory_diffuser_dit/2024-03-30/07-12-41/checkpoints/epoch=359-step=199080-val_loss=0.00-weights-only.ckpt'
-    # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_pndit/2024-04-23/05-01-44/checkpoints/epoch=469-step=1038700-val_loss=0.00-weights-only.ckpt"
-    # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_dit/2024-05-02/12-35-27/checkpoints/epoch=109-step=243100-val_loss=0.00-weights-only.ckpt"
-    # ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_pndit/2024-05-13/05-20-34/checkpoints/epoch=469-step=1038700.ckpt"
-    ckpt_file = "/home/yishu/flowbothd/logs/train_trajectory_diffuser_pndit/2024-05-13/05-20-34/checkpoints/epoch=129-step=287300-val_loss=0.00-weights-only.ckpt"
-
-    # # Load the network weights.
-    # ckpt = torch.load(ckpt_file)
-    # network.load_state_dict(
-    #     {k.partition(".")[2]: v for k, v, in ckpt["state_dict"].items()}
-    # )
-
-    ######################################################################
-    # Create an inference module, which is basically just a bare-bones
-    # class which runs the model. In this example, we only implement
-    # the "predict_step" function, which may not be the blessed
-    # way to do it vis a vis lightning, but whatever.
-    #
-    # If this is a downstream application or something, you might
-    # want to implement a different interface (like with a "predict"
-    # function), so you can pass in un-batched observations from an
-    # environment, for instance.
-    ######################################################################
-
-    model = inference_module_class[cfg.model.name](
-        network, inference_cfg=cfg.inference, model_cfg=cfg.model
-    )
-    model.load_from_ckpt(ckpt_file)
-    model.eval()
-    model.cuda()
-
-    ######################################################################
-    # Create the trainer.
-    # Bit of a misnomer here, we're not doing training. But we are gonna
-    # use it to set up the model appropriately and do all the batching
-    # etc.
-    #
-    # If this is a different kind of downstream eval, chuck this block.
-    ######################################################################
-
-    # trainer = L.Trainer(
-    #     accelerator="gpu",
-    #     devices=cfg.resources.gpus,
-    #     precision="32-true",
-    #     logger=False,
-    # )
-
-    ######################################################################
-    # Run the model on the train/val/test sets.
-    # This outputs a list of dictionaries, one for each batch. This
-    # is annoying to work with, so later we'll flatten.
-    #
-    # If a downstream eval, you can swap it out with whatever the eval
-    # function is.
-    ######################################################################
-
-    dataloaders = [
-        # # (datamodule.train_val_dataloader(), "train"),
-        # (fully_closed_datamodule.train_dataloader(bsz=1), "closed_door"),
-        # (randomly_opened_datamodule.unseen_dataloader(bsz=1), "open_door"),
-        # Test means door only
-        # Fullset closed
-        (fully_closed_datamodule.val_dataloader(bsz=1), "val_closed"),
-        (fully_closed_datamodule.unseen_dataloader(bsz=1), "test_closed"),
-        # Fullset open
-        (randomly_opened_datamodule.val_dataloader(bsz=1), "val_open"),
-        (randomly_opened_datamodule.unseen_dataloader(bsz=1), "test_open"),
-        # # Train set
-        # (fully_closed_datamodule.train_val_dataloader(bsz=1), "train_closed"),
-        # (randomly_opened_datamodule.train_val_dataloader(bsz=1), "train_opened"),
-    ]
-
-    trial_time = 50
-
-    all_metrics = []
-    all_directions = []
-    sample_cnt = 0
-    for loader, name in dataloaders:
-        sample_cnt += len(loader)
-
-        metrics, directions = model.predict_wta(
-            dataloader=loader, mode="delta", trial_times=trial_time
-        )
-        print(f"{name} metric:")
-        print(metrics)
-
-        all_metrics.append(metrics)
-        all_directions += directions
-
-    # # Scatter plot
-    # ys = [d.item() for d in all_directions]
-    # xs = sorted(list(range(sample_cnt)) * trial_time)
-    # xs = [f"{x}" for x in xs]
-    # colors = sorted(["red", "blue", "purple"] * trial_time) * sample_cnt
-    # import matplotlib.pyplot as plt
-
-    # plt.figure()
-    # plt.scatter(xs, ys, s=5, c=colors[: len(xs)])
-    # plt.savefig(f"./{cfg.model.name}_cos_stats.jpeg")
-    eval_set_names = [loader[1] for loader in dataloaders]
-    rows = [
-        (
-            id,
-            m["rmse"],
-            m["cosine_similarity"],
-            m["mag_error"],
-            m["multimodal"],
-            m["pos@0.7"],
-            m["neg@0.7"],
-        )
-        for id, m in zip(eval_set_names, all_metrics)
-    ]
-    df = pd.DataFrame(
-        rows,
-        columns=[
-            "category",
-            "rmse",
-            "cos_similarity",
-            "mag_error",
-            "multimodal",
-            "pos@0.7",
-            "neg@0.7",
-        ],
-    )
-
-    # out_file = Path(cfg.log_dir) / f"{cfg.dataset.name}_{trajectory_len}_{name}.csv"
-    # print(out_file)
-    # # if out_file.exists():
-    # #     raise ValueError(f"{out_file} already exists...")
-    # df.to_csv(out_file, float_format="%.3f")
-
-    # Log the metrics + table to wandb.
-    table = wandb.Table(dataframe=df)
-    run.log({f"eval_wta_metric_table": table})
-
-
-if __name__ == "__main__":
-    main()
diff --git a/scripts/eval_sim_diffuser.py b/scripts/eval_sim_diffuser.py
index e2d4840..89640b6 100644
--- a/scripts/eval_sim_diffuser.py
+++ b/scripts/eval_sim_diffuser.py
@@ -59,8 +59,6 @@ def load_obj_id_to_category(toy_dataset=None):
 
 
 def load_obj_and_link(id_to_cat):
-    # with open("./scripts/umpnet_object_list.json", "r") as f:
-    # with open(f"{PROJECT_ROOT}/scripts/movable_links_005.json", "r") as f:
     with open(f"{PROJECT_ROOT}/scripts/movable_links_fullset_000.json", "r") as f:
         object_link_json = json.load(f)
     for id in id_to_cat.keys():
diff --git a/scripts/eval_sim_gt.py b/scripts/eval_sim_gt.py
deleted file mode 100644
index 99bf63f..0000000
--- a/scripts/eval_sim_gt.py
+++ /dev/null
@@ -1,279 +0,0 @@
-# The evaluation script that runs a rollout for each object in the eval-ed dataset and calculates:
-# - success : 90% open
-# - distance to open
-import json
-import os
-
-import hydra
-import lightning as L
-import numpy as np
-import omegaconf
-import pandas as pd
-import plotly.graph_objects as go
-import torch
-import tqdm
-import wandb
-from rpad.visualize_3d import html
-
-from flowbothd.simulations.simulation import trial_gt_trajectory
-from flowbothd.utils.script_utils import PROJECT_ROOT, match_fn
-
-
-def load_obj_id_to_category(toy_dataset=None):
-    id_to_cat = {}
-    if toy_dataset is None:
-        # Extract existing classes.
-        with open(f"{PROJECT_ROOT}/scripts/umpnet_data_split.json", "r") as f:
-            data = json.load(f)
-
-        for _, category_dict in data.items():
-            for category, split_dict in category_dict.items():
-                for _, id_list in split_dict.items():
-                    for id in id_list:
-                        id_to_cat[id] = category
-
-    else:
-        with open(f"{PROJECT_ROOT}/scripts/umpnet_object_list.json", "r") as f:
-            data = json.load(f)
-        for split in ["train-train", "train-test"]:
-            for id in toy_dataset[split]:
-                id_to_cat[id] = split
-    return id_to_cat
-
-
-def load_obj_and_link(id_to_cat):
-    with open("./scripts/umpnet_object_list.json", "r") as f:
-        object_link_json = json.load(f)
-    # with open(f"{PROJECT_ROOT}/scripts/movable_links_005.json", "r") as f:
-    #     object_link_json = json.load(f)
-    for id in id_to_cat.keys():
-        if id not in object_link_json.keys():
-            object_link_json[id] = []
-    return object_link_json
-
-
-toy_dataset = {
-    "id": "door-full-new",
-    "train-train": [
-        "8877",
-        "8893",
-        "8897",
-        "8903",
-        "8919",
-        "8930",
-        "8961",
-        "8997",
-        "9016",
-        "9032",
-        "9035",
-        "9041",
-        "9065",
-        "9070",
-        "9107",
-        "9117",
-        "9127",
-        "9128",
-        "9148",
-        "9164",
-        "9168",
-        "9277",
-        "9280",
-        "9281",
-        "9288",
-        "9386",
-        "9388",
-        "9410",
-    ],
-    "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-    "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-}
-# id_to_cat = load_obj_id_to_category(toy_dataset)
-id_to_cat = load_obj_id_to_category()  # Full dataset
-object_to_link = load_obj_and_link(id_to_cat)
-
-
-@hydra.main(config_path="../configs", config_name="eval_sim_gt", version_base="1.3")
-def main(cfg):
-    ######################################################################
-    # Torch settings.
-    ######################################################################
-
-    # Make deterministic + reproducible.
-    torch.backends.cudnn.deterministic = True
-    torch.backends.cudnn.benchmark = False
-
-    # Since most of us are training on 3090s+, we can use mixed precision.
-    torch.set_float32_matmul_precision("medium")
-
-    # Global seed for reproducibility.
-    L.seed_everything(42)
-
-    ######################################################################
-    # Create the datamodule.
-    # Should be the same one as in training, but we're gonna use val+test
-    # dataloaders.
-    ######################################################################
-    # datamodule = FlowBotDataModule(
-    #     root=cfg.dataset.data_dir,
-    #     batch_size=cfg.inference.batch_size,
-    #     num_workers=cfg.resources.num_workers,
-    #     n_proc=cfg.resources.n_proc_per_worker,  # Add n_proc
-    # )
-
-    ######################################################################
-    # Set up logging in WandB.
-    # This is a different job type (eval), but we want it all grouped
-    # together. Notice that we use our own logging here (not lightning).
-    ######################################################################
-    id = wandb.util.generate_id()
-    group = "experiment-" + id
-    # if cfg.wandb.group is None:
-    #     id = wandb.util.generate_id()
-    #     group = "experiment-" + id
-    # else:
-    #     group = cfg.wandb.group
-
-    # Create a run.
-    run = wandb.init(
-        entity=cfg.wandb.entity,
-        project=cfg.wandb.project,
-        dir=cfg.wandb.save_dir,
-        config=omegaconf.OmegaConf.to_container(
-            cfg, resolve=True, throw_on_missing=True
-        ),
-        job_type=cfg.job_type,
-        save_code=True,  # This just has the main script.
-        group=group,
-    )
-
-    # Log the code.
-    wandb.run.log_code(
-        root=PROJECT_ROOT,
-        include_fn=match_fn(
-            dirs=["configs", "scripts", "src"],
-            extensions=[".py", ".yaml"],
-        ),
-    )
-
-    ######################################################################
-    # Create the network(s) which will be evaluated (same as training).
-    # You might want to put this into a "create_network" function
-    # somewhere so train and eval can be the same.
-    #
-    # We'll also load the weights.
-    ######################################################################
-
-    # Simulation and results.
-    print("Simulating")
-
-    if cfg.website:
-        # Visualization html
-        os.makedirs("./logs/simu_eval/video_assets/")
-        doc = html.PlotlyWebsiteBuilder("Simulation Visualizations")
-
-    obj_cats = list(set(id_to_cat.values()))
-    metric_df = pd.DataFrame(
-        np.zeros((len(set(id_to_cat.values())), 3)),
-        index=obj_cats,
-        columns=["count", "success_rate", "norm_dist"],
-    )
-    category_counts = {}
-    movable_link = {}
-    sim_trajectories = []
-    link_names = []
-    # for obj_id, obj_cat in tqdm.tqdm(list(id_to_cat.items())):
-    for obj_id, available_links in tqdm.tqdm(list(object_to_link.items())):
-        if obj_id not in id_to_cat.keys():
-            continue
-        # if len(available_links) == 0:
-        #     continue
-        obj_cat = id_to_cat[obj_id]
-        if not os.path.exists(f"/home/yishu/datasets/partnet-mobility/raw/{obj_id}"):
-            continue
-        print(f"OBJ {obj_id} of {obj_cat}")
-        # trial_figs, trial_results = trial_flow(
-        #     obj_id=obj_id,
-        #     n_steps=30,
-        #     all_joint=True,
-        #     available_joints=available_links,
-        #     gui=cfg.gui,
-        #     website=cfg.website,
-        # )
-        (
-            trial_figs,
-            trial_results,
-            this_movable_links,
-            sim_trajectory,
-        ) = trial_gt_trajectory(
-            obj_id=obj_id,
-            traj_len=cfg.inference.trajectory_len,
-            n_steps=30,
-            all_joint=True,
-            # available_joints=available_links,
-            gui=cfg.gui,
-            website=cfg.website,
-        )
-        sim_trajectories += sim_trajectory
-        link_names += [f"link_{i}" for i in range(len(sim_trajectory))]
-        # link_names += [f"{obj_id}_{link}" for link in available_links]
-        movable_link[obj_id] = this_movable_links
-
-        # Wandb table
-        if obj_cat not in category_counts.keys():
-            category_counts[obj_cat] = 0
-        category_counts[obj_cat] += len(trial_results)
-        for result in trial_results:
-            metric_df.loc[obj_cat]["success_rate"] += result.success
-            metric_df.loc[obj_cat]["norm_dist"] += result.metric
-
-        if cfg.website:
-            # Website visualization
-            for id, (joint_name, fig) in enumerate(trial_figs.items()):
-                tag = f"{obj_id}_{joint_name}"
-                doc.add_plot(obj_cat, tag, fig)
-                doc.add_video(
-                    obj_cat,
-                    f"{tag}{'_NO CONTACT' if not trial_results[id].contact else ''}",
-                    f"http://128.2.178.238:9000/video_assets/{tag}.mp4",
-                )
-            # print(trial_results)
-            doc.write_site("./logs/simu_eval")
-
-        if category_counts[obj_cat] == 0:
-            continue
-        wandb_df = metric_df.copy(deep=True)
-        for obj_cat in category_counts.keys():
-            wandb_df.loc[obj_cat]["success_rate"] /= category_counts[obj_cat]
-            wandb_df.loc[obj_cat]["norm_dist"] /= category_counts[obj_cat]
-            wandb_df.loc[obj_cat]["count"] = category_counts[obj_cat]
-
-        table = wandb.Table(dataframe=wandb_df.reset_index())
-        run.log({f"simulation_metric_table": table})
-
-    with open(
-        "/home/yishu/flowbothd/scripts/movable_links_fullset_000.json",
-        "w",
-    ) as f:
-        json.dump(movable_link, f)
-    # for obj_cat in category_counts.keys():
-    #     metric_df.loc[obj_cat]["success_rate"] /= category_counts[obj_cat]
-    #     metric_df.loc[obj_cat]["norm_dist"] /= category_counts[obj_cat]
-    #     metric_df.loc[obj_cat]["category"] = obj_cat
-
-    # table = wandb.Table(dataframe=metric_df)
-    # run.log({f"simulation_metric_table": table})
-
-    traces = []
-    xs = list(range(31))
-    for id, sim_trajectory in enumerate(sim_trajectories):
-        traces.append(
-            go.Scatter(x=xs, y=sim_trajectory, mode="lines", name=link_names[id])
-        )
-
-    layout = go.Layout(title="Simulation Trajectory Figure")
-    fig = go.Figure(data=traces, layout=layout)
-    wandb.log({"sim_traj_figure": wandb.Plotly(fig)})
-
-
-if __name__ == "__main__":
-    main()
diff --git a/scripts/movable_links_001.json b/scripts/movable_links_001.json
deleted file mode 100644
index dce9230..0000000
--- a/scripts/movable_links_001.json
+++ /dev/null
@@ -1 +0,0 @@
-{"9016": ["link_1", "link_2"], "9164": [], "9041": ["link_0"], "9410": [], "9388": [], "9107": [], "9070": ["link_0"], "9386": [], "9168": [], "8867": [], "8893": ["link_2"], "8897": ["link_1"], "8903": ["link_2"], "8919": ["link_1", "link_2"], "8930": ["link_4"], "8961": ["link_1", "link_2"], "8983": [], "8997": ["link_0", "link_1"], "9003": ["link_1"], "9065": ["link_1"], "9117": [], "9128": ["link_1", "link_2"], "9280": [], "9281": [], "9288": [], "8877": ["link_1"], "9032": [], "9035": ["link_0"], "9127": [], "9148": [], "9277": [], "8994": ["link_2"], "9263": ["link_2"], "9393": ["link_1"]}
diff --git a/scripts/movable_links_005.json b/scripts/movable_links_005.json
deleted file mode 100644
index 29e2503..0000000
--- a/scripts/movable_links_005.json
+++ /dev/null
@@ -1 +0,0 @@
-{"9016": ["link_1", "link_2"], "9164": ["link_1"], "9041": ["link_0"], "9410": ["link_1"], "9388": ["link_0"], "9107": [], "9070": ["link_0"], "9386": ["link_1"], "9168": ["link_0", "link_1"], "8867": ["link_1"], "8893": ["link_2"], "8897": ["link_1"], "8903": ["link_2"], "8919": ["link_1", "link_2"], "8930": ["link_3", "link_4"], "8961": ["link_1", "link_2"], "8983": ["link_1"], "8997": ["link_0", "link_1"], "9003": ["link_1"], "9065": ["link_1"], "9117": ["link_1"], "9128": ["link_1", "link_2"], "9280": ["link_2"], "9281": [], "9288": ["link_0"], "8877": ["link_1", "link_2"], "9032": [], "9035": ["link_0"], "9127": ["link_1"], "9148": [], "9277": ["link_2"], "8994": ["link_2"], "9263": [], "9393": ["link_1"]}
diff --git a/scripts/movable_links_fullset_001.json b/scripts/movable_links_fullset_001.json
deleted file mode 100644
index 79f393b..0000000
--- a/scripts/movable_links_fullset_001.json
+++ /dev/null
@@ -1 +0,0 @@
-{"100015": ["link_0"], "100017": ["link_0"], "100021": ["link_0"], "100023": ["link_0"], "100025": ["link_0"], "100028": ["link_0"], "100032": ["link_0"], "100033": ["link_0"], "100038": ["link_0"], "100040": ["link_0"], "100045": ["link_0"], "100047": ["link_0"], "100051": ["link_0"], "100054": ["link_0"], "100055": [], "100056": ["link_0"], "100057": ["link_0"], "100058": ["link_0"], "100060": ["link_0"], "100613": ["link_0"], "100619": ["link_0"], "100623": ["link_0"], "100693": ["link_0"], "102080": ["link_0"], "102085": ["link_0"], "19179": ["link_0", "link_1"], "19855": ["link_0"], "19898": ["link_4", "link_1", "link_2", "link_3", "link_5", "link_6", "link_7"], "20043": ["link_1", "link_2"], "20411": ["link_0"], "20555": ["link_0"], "20745": ["link_0", "link_1", "link_2"], "20985": ["link_1"], "22241": ["link_0"], "22301": ["link_0", "link_1", "link_2"], "22339": ["link_0"], "22367": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5", "link_6", "link_7"], "22508": ["link_0"], "23372": ["link_2", "link_3", "link_0", "link_1"], "23511": ["link_0"], "24644": ["link_0"], "24931": ["link_1"], "25308": ["link_0", "link_1"], "25913": ["link_0", "link_1", "link_2"], "26503": ["link_1"], "26525": ["link_0"], "26652": ["link_1"], "26657": ["link_2"], "26670": ["link_0"], "26806": ["link_0"], "27044": ["link_0"], "27189": ["link_0"], "28164": ["link_0", "link_1"], "28668": ["link_0"], "29921": ["link_1"], "30238": ["link_1", "link_2"], "30341": ["link_1"], "30666": ["link_0", "link_1", "link_2", "link_4", "link_5", "link_6", "link_7", "link_8"], "30739": [], "31249": ["link_3", "link_4", "link_0", "link_1"], "31601": ["link_1", "link_2"], "32052": [], "32086": ["link_0", "link_1"], "32174": ["link_1", "link_0"], "32259": ["link_0", "link_1", "link_2"], "32324": ["link_0", "link_1", "link_2"], "32566": ["link_0"], "32601": ["link_2", "link_3"], "32625": ["link_0", "link_1", "link_2", "link_3"], "32761": ["link_0", "link_1", "link_2", "link_3"], "32932": ["link_0", "link_1"], "33116": ["link_0", "link_2"], "33457": ["link_0", "link_2"], "33810": ["link_0", "link_2", "link_3", "link_4"], "33930": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "34178": ["link_2", "link_3", "link_0", "link_1"], "34610": ["link_0", "link_1", "link_2", "link_3", "link_4"], "34617": ["link_1", "link_2"], "7290": [], "7220": [], "7120": ["link_1", "link_2"], "7179": ["link_3", "link_4"], "7187": ["link_1"], "7201": ["link_3", "link_4"], "7332": ["link_4"], "101773": [], "101808": ["link_0", "link_1"], "101908": ["link_0", "link_1", "link_2"], "101909": [], "101917": ["link_0"], "101924": [], "101930": ["link_0", "link_6"], "101931": ["link_0"], "101940": ["link_0"], "101943": ["link_0", "link_5"], "101946": ["link_0", "link_4"], "101947": ["link_0", "link_1", "link_4"], "101971": ["link_0"], "102001": ["link_5"], "102018": ["link_0"], "102019": [], "102044": [], "12536": [], "12617": [], "12560": ["link_0"], "12597": ["link_0"], "12552": ["link_0"], "12654": ["link_0"], "12530": [], "12565": [], "12563": ["link_0"], "12414": ["link_1"], "12558": ["link_0"], "12594": ["link_1"], "12579": [], "12621": ["link_0"], "11622": ["link_0"], "11661": ["link_1"], "11700": ["link_0"], "11826": ["link_2"], "12065": ["link_2"], "12092": ["link_0"], "12259": ["link_0"], "12349": ["link_0"], "12428": ["link_4"], "12480": ["link_1"], "12484": ["link_2", "link_3"], "12531": ["link_0"], "12540": ["link_0"], "12543": ["link_0"], "12553": [], "12559": ["link_0"], "12561": ["link_0"], "12562": ["link_0"], "12580": ["link_0"], "12583": ["link_1"], "12587": ["link_0"], "12590": ["link_0"], "12592": [], "12596": ["link_0"], "12605": ["link_0"], "12606": ["link_1"], "12614": ["link_1"], "9016": ["link_1", "link_2"], "9164": [], "9041": [], "9410": ["link_1"], "9388": [], "9107": [], "9070": ["link_0"], "9386": [], "9168": [], "8867": [], "8893": ["link_2"], "8897": ["link_1", "link_2"], "8903": [], "8919": ["link_1", "link_2"], "8930": ["link_4"], "8961": ["link_1", "link_2"], "8983": [], "8997": ["link_0", "link_1"], "9003": ["link_1"], "9065": ["link_1", "link_2"], "9117": [], "9128": ["link_1", "link_2"], "9280": [], "9281": [], "9288": ["link_0"], "102423": ["link_0"], "102278": ["link_0", "link_1", "link_4", "link_9", "link_13"], "102389": ["link_16", "link_0", "link_1"], "102418": ["link_0", "link_3", "link_4", "link_5", "link_8"], "101363": ["link_0"], "101564": ["link_13", "link_1", "link_11"], "101579": ["link_3"], "101584": ["link_0", "link_1"], "101591": ["link_4", "link_5", "link_2"], "101593": ["link_0", "link_1"], "101594": ["link_0"], "101599": ["link_14"], "101603": ["link_0", "link_2"], "101604": ["link_13"], "101605": [], "101611": ["link_0"], "101612": ["link_0"], "101619": ["link_0"], "102301": ["link_0"], "102309": ["link_0", "link_3"], "102311": ["link_1", "link_2"], "102316": ["link_0"], "102318": ["link_0"], "102380": ["link_2"], "102381": ["link_0", "link_2"], "102384": ["link_0"], "102387": ["link_0"], "47645": ["link_0"], "48492": ["link_2"], "100129": ["link_1"], "100141": ["link_0"], "100174": ["link_0"], "100189": ["link_0"], "100214": ["link_0"], "100243": ["link_0"], "100247": [], "100664": ["link_0"], "102377": ["link_0", "link_1"], "102379": ["link_0"], "102456": ["link_0"], "100438": ["link_0"], "100444": ["link_0"], "100454": ["link_0"], "100470": ["link_0"], "100473": ["link_0"], "102358": ["link_0"], "103350": ["link_14"], "103593": ["link_0"], "103886": ["link_0"], "26875": ["link_0", "link_1"], "100282": ["link_0"], "100283": ["link_0"], "103351": ["link_0"], "103361": ["link_0"], "103369": ["link_0"], "103425": ["link_0", "link_8"], "103452": ["link_0"], "103480": ["link_6", "link_7"], "103490": [], "103508": [], "103518": ["link_0"], "103521": ["link_0", "link_2", "link_3"], "103528": [], "103775": ["link_0"], "103776": [], "103778": ["link_0"], "22433": ["link_1"], "23782": ["link_1"], "26899": [], "27267": [], "101315": ["link_0"], "46966": ["link_0"], "11231": ["link_1", "link_2"], "102257": ["link_0"], "103297": ["link_1"], "102634": ["link_0", "link_1"], "48379": ["link_0", "link_1"], "103236": ["link_0", "link_1"], "12050": ["link_0", "link_2"], "41086": ["link_0"], "10586": ["link_1", "link_2"], "46859": ["link_0", "link_1"], "103669": ["link_0"], "45949": ["link_0", "link_1", "link_2"], "102177": [], "47808": ["link_0", "link_1"], "47853": ["link_0", "link_1", "link_2"], "103789": ["link_1"], "47180": ["link_1"], "103015": [], "12248": ["link_0", "link_1"], "45790": ["link_0", "link_1"], "45238": ["link_0", "link_1", "link_2", "link_3"], "41085": ["link_0", "link_1", "link_2", "link_3"], "47651": ["link_1"], "45378": ["link_0", "link_1"], "102707": ["link_2"], "101377": [], "45354": ["link_0", "link_1"], "45756": ["link_0", "link_1", "link_2"], "45779": ["link_1"], "45940": ["link_0", "link_1", "link_2"], "45503": ["link_0", "link_1", "link_2"], "103234": ["link_1"], "3971": ["link_0"], "47578": ["link_0", "link_1", "link_2", "link_3"], "11178": ["link_0", "link_1"], "45922": ["link_1"], "47315": ["link_0"], "46408": ["link_0"], "45159": ["link_0", "link_1"], "46616": ["link_0"], "46981": ["link_0", "link_1", "link_2"], "100982": ["link_0"], "100521": ["link_0"], "102984": ["link_0", "link_1"], "103307": ["link_1"], "45622": ["link_0", "link_1", "link_2"], "47281": ["link_0"], "101305": ["link_0"], "45594": ["link_0", "link_1"], "46134": ["link_0", "link_1"], "47235": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "102697": ["link_2", "link_3"], "49038": ["link_1", "link_2"], "46655": ["link_0", "link_1"], "100970": ["link_0"], "9968": ["link_1"], "48013": ["link_0", "link_1"], "102801": ["link_1"], "45689": ["link_0", "link_1"], "46825": ["link_0", "link_1"], "47648": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "10626": ["link_0"], "100561": ["link_0"], "9912": ["link_1"], "45676": ["link_0", "link_1"], "47817": ["link_1"], "10270": ["link_1"], "46893": ["link_0", "link_1", "link_2", "link_3"], "10211": ["link_1"], "45853": ["link_0"], "102667": ["link_2", "link_3"], "102244": ["link_0", "link_1"], "48700": ["link_0", "link_1"], "102720": ["link_0"], "10697": ["link_1"], "10143": ["link_0", "link_2"], "101320": ["link_0"], "103008": ["link_0"], "47529": ["link_0", "link_1"], "47021": ["link_0"], "41510": ["link_0", "link_1"], "11712": ["link_0", "link_1"], "11242": [], "12038": ["link_1"], "45573": ["link_0", "link_1"], "100599": ["link_0"], "45178": [], "46230": ["link_0", "link_1", "link_2"], "102715": ["link_0"], "47443": ["link_0", "link_1"], "103635": ["link_0"], "103299": [], "46874": ["link_0", "link_1", "link_2"], "47207": ["link_0", "link_1", "link_2", "link_3"], "46641": ["link_0", "link_1", "link_2"], "103276": ["link_1"], "102675": ["link_0", "link_1"], "7265": ["link_0"], "48746": ["link_0"], "46180": ["link_0", "link_1"], "100885": [], "47944": ["link_0", "link_1", "link_2"], "7304": ["link_1"], "45130": ["link_0"], "10280": ["link_0"], "103321": [], "12477": ["link_0"], "102977": ["link_0"], "11304": ["link_0", "link_1"], "103056": [], "47585": ["link_0", "link_1", "link_2", "link_3", "link_5", "link_6", "link_19"], "45855": ["link_0"], "46699": ["link_0", "link_1"], "45323": ["link_0"], "102165": ["link_1"], "46179": ["link_0"], "45176": ["link_0"], "11945": ["link_1"], "103052": ["link_0"], "102985": ["link_1"], "103013": ["link_0"], "49062": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45623": ["link_0"], "49140": ["link_1", "link_2", "link_3", "link_4"], "100590": [], "46549": ["link_0", "link_1", "link_2", "link_3"], "100031": ["link_0"], "103222": ["link_0"], "102773": ["link_0"], "101313": [], "102714": [], "102724": ["link_0"], "102726": ["link_1"], "102732": ["link_0"], "102736": ["link_0"], "102739": ["link_1"], "102753": ["link_0"], "102761": ["link_0"], "102763": ["link_0"], "102765": ["link_0"], "102768": ["link_0"], "102786": ["link_0"], "103201": ["link_0"], "103207": ["link_0"], "103208": ["link_0"], "100920": [], "102839": ["link_0", "link_4"], "102860": [], "102812": ["link_0"], "102856": ["link_6"], "103540": ["link_2"], "103319": ["link_0", "link_1"], "103070": [], "103063": ["link_4", "link_5"], "103077": [], "103148": ["link_0"], "102798": ["link_0"], "102802": ["link_0", "link_1"], "102803": ["link_0"], "102804": ["link_1"], "102805": ["link_1", "link_2"], "102896": ["link_0", "link_1"], "102903": ["link_0", "link_1"], "102905": ["link_1"], "102906": ["link_0", "link_1"], "102981": ["link_0", "link_1"], "103032": [], "103042": ["link_1"], "103044": ["link_1", "link_2"], "103050": ["link_0", "link_1"], "103058": [], "103150": ["link_0"], "103235": ["link_0"], "103238": [], "103242": ["link_0"], "103253": ["link_2"], "103255": ["link_0", "link_1"], "103268": ["link_0"], "103316": [], "103318": ["link_1"], "103320": [], "103323": ["link_0"], "103325": [], "103329": [], "103332": ["link_0", "link_1"], "103333": ["link_1"], "103339": ["link_0"], "103340": ["link_0", "link_1"], "103684": ["link_1", "link_2"], "40147": ["link_0", "link_1"], "40417": ["link_0", "link_1", "link_4", "link_5", "link_2", "link_3"], "41083": ["link_0", "link_1", "link_2", "link_3"], "44781": ["link_0", "link_1", "link_2"], "44817": ["link_0", "link_1", "link_2", "link_3"], "44826": ["link_0", "link_1"], "44853": ["link_0", "link_1", "link_2"], "44962": ["link_0", "link_1", "link_2"], "45092": ["link_0", "link_1", "link_2", "link_3"], "45132": ["link_0", "link_1", "link_2"], "45135": ["link_0", "link_1", "link_2"], "45146": ["link_1", "link_0"], "45162": ["link_0", "link_1"], "45168": ["link_1", "link_0"], "45194": ["link_0", "link_1", "link_2", "link_3"], "45219": ["link_0", "link_1", "link_2", "link_3"], "45235": ["link_0", "link_1"], "45248": ["link_0"], "45261": ["link_0", "link_1", "link_2", "link_3", "link_4"], "45262": ["link_1", "link_2", "link_3"], "45271": ["link_2", "link_3", "link_4", "link_5", "link_0", "link_1"], "45290": ["link_0", "link_1", "link_2"], "45374": ["link_0", "link_1", "link_2", "link_3"], "45413": ["link_0"], "45427": ["link_0", "link_1", "link_2"], "45575": ["link_0", "link_1"], "45612": ["link_0", "link_1", "link_2", "link_3", "link_4"], "45620": ["link_0"], "45632": ["link_0", "link_1", "link_2"], "45636": ["link_0", "link_1", "link_2", "link_3"], "45642": ["link_1", "link_2"], "45661": ["link_0", "link_1", "link_2"], "45677": ["link_0", "link_1", "link_2", "link_3"], "45687": ["link_0", "link_1"], "45694": ["link_0", "link_1"], "45710": ["link_0", "link_1", "link_2", "link_3"], "45746": ["link_1", "link_2"], "45759": ["link_0", "link_1", "link_2", "link_3"], "45784": ["link_0", "link_1"], "45801": ["link_0", "link_1", "link_2", "link_3"], "45822": ["link_0"], "45841": ["link_1", "link_2", "link_3"], "45910": ["link_0"], "45948": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45984": ["link_0", "link_1"], "46014": ["link_1", "link_2", "link_3", "link_4"], "46045": ["link_0", "link_1"], "46060": ["link_0", "link_1", "link_2", "link_3"], "46084": ["link_0", "link_1", "link_2", "link_3"], "46107": ["link_1"], "46109": ["link_1", "link_2", "link_3", "link_4", "link_5"], "46123": ["link_0", "link_1", "link_2"], "46127": ["link_1"], "46130": ["link_0", "link_1", "link_2"], "46132": [], "46145": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "46172": ["link_2", "link_4", "link_5", "link_6", "link_7", "link_8", "link_9", "link_10", "link_11"], "46236": ["link_0", "link_1"], "46334": ["link_1", "link_2"], "46380": ["link_1", "link_2", "link_3", "link_4"], "46439": ["link_0"], "46440": ["link_0", "link_1", "link_2"], "46443": ["link_0", "link_1", "link_2", "link_3"], "46452": ["link_0", "link_2"], "46462": ["link_0", "link_1", "link_2"], "46466": ["link_0", "link_1", "link_2", "link_3"], "46537": ["link_0", "link_1", "link_2"], "46544": ["link_1", "link_2"], "46556": ["link_0"], "46598": ["link_0", "link_1", "link_2", "link_3"], "46653": ["link_1", "link_2", "link_3"], "46741": ["link_0", "link_1"], "46762": ["link_0", "link_1", "link_2", "link_3"], "46768": ["link_0", "link_1"], "46839": ["link_0", "link_1", "link_2", "link_3"], "46856": ["link_0", "link_1", "link_2", "link_3"], "47024": ["link_0", "link_1"], "47089": ["link_0", "link_1", "link_2", "link_3"], "47168": ["link_0"], "47178": ["link_0", "link_1", "link_2"], "47183": ["link_0", "link_1", "link_2"], "47185": ["link_0", "link_1"], "47233": ["link_0", "link_1", "link_2"], "47252": ["link_0", "link_1", "link_2"], "47254": ["link_0", "link_1"], "47296": ["link_0", "link_1", "link_2"], "47391": ["link_1"], "47438": ["link_0", "link_1", "link_2"], "47565": ["link_1", "link_2"], "47570": ["link_0", "link_1"], "47711": ["link_1", "link_2", "link_3"], "47926": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "47963": ["link_1"], "48010": ["link_0", "link_1", "link_2"], "48051": ["link_1", "link_3", "link_4", "link_5"], "48063": ["link_1", "link_2"], "48169": ["link_0", "link_1", "link_2", "link_3", "link_5"], "48253": ["link_0", "link_1", "link_2"], "48258": ["link_0", "link_1", "link_2", "link_3"], "48263": ["link_1", "link_2", "link_3", "link_4"], "48491": ["link_1", "link_2", "link_3"], "48513": ["link_0", "link_1"], "48517": ["link_0", "link_1", "link_2"], "48740": ["link_0", "link_1", "link_2"], "48855": ["link_1", "link_2", "link_0"], "48876": ["link_0", "link_1", "link_2"], "48878": ["link_1", "link_0"], "11818": ["link_0"], "102996": ["link_0"], "11229": [], "11259": ["link_0"], "100520": [], "100523": [], "100526": ["link_0"], "100531": ["link_0"], "100532": ["link_0"], "100568": [], "100579": ["link_0"], "100586": ["link_0"], "100600": ["link_0"], "100609": ["link_0"], "100611": ["link_0"], "100616": ["link_0"], "102255": [], "102263": [], "102314": ["link_0"], "102333": [], "9748": ["link_0"], "9960": ["link_0"], "9992": [], "9996": ["link_0"], "10098": [], "10101": ["link_1"], "10125": ["link_1"], "10213": ["link_1"], "10238": ["link_0"], "10239": ["link_0"], "10243": [], "10248": ["link_1"], "10269": [], "10289": [], "10305": ["link_0"], "10306": ["link_1"], "10383": ["link_1"], "10707": ["link_1"], "11030": [], "11141": ["link_1"], "11156": ["link_1"], "11395": ["link_1"], "11405": ["link_0"], "11406": ["link_1"], "11778": ["link_0"], "11429": ["link_1"], "11477": ["link_1"], "11888": ["link_0"], "10885": ["link_0"], "11854": ["link_0"], "10915": ["link_0"], "11586": ["link_1"], "11581": ["link_0"], "11876": ["link_1"], "10036": ["link_1", "link_2"], "10068": ["link_1"], "10144": ["link_0"], "10347": ["link_0", "link_1"], "10373": ["link_0"], "10489": ["link_1", "link_2"], "10612": ["link_1", "link_2"], "10620": ["link_1", "link_2"], "10638": ["link_1", "link_2"], "10627": ["link_1", "link_2"], "10655": ["link_1", "link_2"], "10685": ["link_1", "link_2"], "10751": ["link_1", "link_2"], "10797": ["link_1"], "10849": ["link_0"], "10867": ["link_0", "link_2"], "10900": ["link_0", "link_1"], "10905": ["link_0"], "10944": ["link_0"], "11211": ["link_0"], "11299": ["link_1", "link_2"], "11550": ["link_0", "link_1"], "11709": ["link_0", "link_2"], "12036": ["link_0", "link_1"], "12042": ["link_1"], "12043": ["link_1", "link_2"], "12054": ["link_0"], "12059": ["link_0", "link_1"], "12066": ["link_1", "link_2"], "12249": ["link_0"], "12250": ["link_1"], "12252": ["link_0"], "102620": ["link_0"], "102621": ["link_1", "link_2", "link_3"], "102622": ["link_0"], "102630": ["link_0"], "102645": ["link_2"], "102648": ["link_2", "link_3"], "102651": ["link_0"], "102652": ["link_0"], "102654": ["link_1", "link_2"], "102658": ["link_0"], "102663": ["link_1", "link_2", "link_4"], "102666": ["link_1"], "102668": ["link_2", "link_3"], "102669": ["link_0"], "102670": ["link_2", "link_3"], "102676": ["link_2", "link_3", "link_1"], "102677": ["link_2"], "102687": ["link_0"], "102689": ["link_0", "link_1", "link_2", "link_3"], "102692": ["link_1"], "102694": ["link_0"], "102699": [], "102701": ["link_0", "link_1"], "102703": ["link_2"], "102708": ["link_1", "link_2"], "103646": [], "102252": ["link_0", "link_1"], "103007": [], "103012": [], "102158": ["link_2"], "101384": [], "102163": [], "103634": ["link_0"], "103647": ["link_1"], "102219": ["link_0", "link_1"], "102992": ["link_1"], "103633": ["link_0"], "102229": ["link_0"], "103639": [], "10584": ["link_0"], "11124": ["link_0"], "11279": ["link_0"], "11361": ["link_0"], "12447": [], "12483": ["link_0"], "101378": ["link_0"], "102153": ["link_2"], "102154": ["link_2"], "102155": ["link_2"], "102156": ["link_2"], "102173": [], "102181": ["link_1"], "102182": [], "102186": ["link_1"], "102189": ["link_2"], "102192": [], "102259": ["link_2"], "7236": ["link_0"], "7263": ["link_0"], "7292": ["link_0"], "7310": ["link_0"], "7366": ["link_1"], "7167": ["link_0", "link_2"], "7128": ["link_0"], "7349": ["link_1"], "102990": ["link_1"], "103095": ["link_1"], "103099": [], "103100": [], "103104": ["link_1"], "103111": ["link_1"], "103113": ["link_1"], "103271": ["link_1"], "103273": ["link_1"], "103280": ["link_1"], "103283": ["link_0", "link_1"], "103292": ["link_1"], "103293": ["link_1"], "103301": [], "103303": ["link_1"], "103305": ["link_1"], "103792": [], "100849": ["link_0"], "100965": ["link_0"], "100980": ["link_1", "link_2"], "103040": [], "103135": ["link_0"], "35059": ["link_0"], "38516": ["link_0"], "41003": ["link_0", "link_1", "link_2", "link_3"], "41004": ["link_0"], "41452": ["link_0"], "41529": ["link_0"], "45001": ["link_0", "link_1"], "45007": ["link_1"], "45087": ["link_1"], "45091": ["link_1"], "45134": ["link_1"], "45164": ["link_0"], "45166": [], "45173": ["link_0"], "45177": [], "45189": ["link_0", "link_1", "link_2", "link_3"], "45203": ["link_0"], "45212": ["link_0"], "45244": ["link_0"], "45247": ["link_1"], "45249": ["link_0"], "45267": ["link_0"], "45297": [], "45305": ["link_0", "link_1"], "45372": ["link_0"], "45384": ["link_0"], "45385": ["link_1"], "45387": ["link_0", "link_1", "link_2", "link_3"], "45397": ["link_0", "link_1"], "45415": ["link_0"], "45420": ["link_0", "link_1"], "45423": ["link_0", "link_1"], "45443": ["link_1"], "45444": ["link_0", "link_1"], "45448": ["link_0"], "45463": ["link_0", "link_1"], "45504": ["link_1"], "45505": ["link_0", "link_1"], "45516": [], "45523": ["link_1", "link_2"], "45524": ["link_0"], "45526": ["link_0"], "45600": ["link_1"], "45606": ["link_0"], "45621": ["link_0"], "45633": ["link_1"], "45638": ["link_0"], "45645": ["link_0"], "45662": ["link_0", "link_1"], "45667": ["link_1"], "45670": ["link_0", "link_1"], "45671": ["link_1"], "45690": ["link_0"], "45691": ["link_0"], "45693": ["link_1"], "45696": ["link_0", "link_1", "link_2", "link_3"], "45699": ["link_1"], "45717": [], "45747": ["link_0", "link_1"], "45749": ["link_0", "link_1", "link_2", "link_3"], "45767": ["link_0", "link_1"], "45776": ["link_0", "link_1"], "45780": ["link_0", "link_1"], "45783": ["link_0"], "45850": ["link_1"], "45908": ["link_1"], "45915": ["link_1"], "45916": ["link_0"], "45936": ["link_0"], "45950": ["link_0"], "45961": ["link_0"], "45963": ["link_0", "link_1"], "45964": ["link_0"], "46002": ["link_0", "link_1", "link_2", "link_3"], "46019": ["link_0", "link_1"], "46029": ["link_0"], "46033": ["link_0"], "46037": ["link_0", "link_1"], "46044": ["link_1"], "46057": ["link_1"], "46092": ["link_0"], "46108": ["link_2", "link_3"], "46117": ["link_1"], "46166": ["link_1", "link_2"], "46197": ["link_0"], "46277": ["link_0", "link_1"], "46401": ["link_0"], "46417": ["link_0"], "46427": ["link_0"], "46430": ["link_1"], "46437": ["link_0", "link_1"], "46456": ["link_0", "link_1"], "46480": ["link_0", "link_1"], "46481": ["link_0", "link_1"], "46490": ["link_0", "link_1"], "46563": ["link_0", "link_1"], "46700": ["link_0", "link_1"], "46732": ["link_0", "link_1"], "46744": ["link_1"], "46787": ["link_1"], "46801": ["link_0", "link_1"], "46889": ["link_0"], "46906": ["link_1"], "46922": ["link_0"], "46944": ["link_0"], "46955": ["link_0", "link_1", "link_2", "link_3"], "47099": ["link_0", "link_1"], "47133": ["link_1"], "47182": ["link_0"], "47187": ["link_0"], "47227": ["link_0", "link_1", "link_2", "link_3"], "47278": ["link_0", "link_1"], "47290": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "47388": ["link_0"], "47419": ["link_1"], "47514": ["link_1"], "47577": ["link_0", "link_1"], "47595": ["link_0", "link_1", "link_2", "link_3"], "47601": ["link_1"], "47613": ["link_0", "link_1"], "47632": ["link_1"], "47669": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "47686": ["link_0"], "47701": ["link_0", "link_1", "link_2", "link_3"], "47729": ["link_1"], "47742": ["link_0"], "47747": ["link_0", "link_1"], "47976": ["link_0", "link_1", "link_2", "link_3"], "48018": ["link_0", "link_1", "link_2", "link_3"], "48023": ["link_1"], "48036": ["link_1"], "48167": ["link_0"], "48177": ["link_0", "link_1"], "48243": ["link_1"], "48271": ["link_0"], "48356": ["link_0", "link_1", "link_2", "link_3"], "48381": ["link_0", "link_2"], "48413": ["link_1"], "48452": ["link_0"], "48467": ["link_1"], "48490": ["link_1"], "48519": ["link_1"], "48623": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "48721": ["link_0"], "49025": ["link_0", "link_1", "link_2", "link_3"], "49042": ["link_0", "link_1"], "49132": ["link_0", "link_1"], "49133": ["link_0", "link_1"], "49182": ["link_0", "link_1"], "49188": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45403": ["link_1"], "45419": ["link_1"], "45725": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5", "link_6", "link_7", "link_8", "link_9", "link_10", "link_11", "link_12", "link_13"], "47088": ["link_0", "link_1", "link_2"], "12055": ["link_0"], "100732": ["link_1"], "45213": ["link_0"], "46847": ["link_0", "link_2"], "45332": ["link_0", "link_1"], "45243": ["link_0", "link_1", "link_2", "link_3"], "46120": ["link_0", "link_1"], "46896": ["link_0", "link_1"], "46199": ["link_0", "link_1", "link_4", "link_5"], "101613": [], "103781": [], "26073": ["link_0"], "26387": [], "23472": ["link_0", "link_1"], "20453": ["link_0"], "23807": ["link_0", "link_1", "link_2", "link_3"], "21467": [], "20279": ["link_0"], "32354": ["link_0", "link_1", "link_2"], "19836": ["link_0", "link_1", "link_2"], "19825": ["link_0", "link_1"], "25493": ["link_0", "link_1", "link_2"], "22692": ["link_0", "link_1"], "29133": ["link_0", "link_1"], "29557": ["link_0"], "26608": ["link_4", "link_5", "link_6", "link_7"], "33914": ["link_0"], "27619": ["link_0", "link_1"], "30869": ["link_0"], "30663": ["link_0", "link_1", "link_2"], "8877": ["link_1"], "8936": []}
diff --git a/scripts/movable_links_fullset_005.json b/scripts/movable_links_fullset_005.json
deleted file mode 100644
index d8cb695..0000000
--- a/scripts/movable_links_fullset_005.json
+++ /dev/null
@@ -1 +0,0 @@
-{"100015": ["link_0"], "100017": ["link_0"], "100021": ["link_0"], "100023": ["link_0"], "100025": ["link_0"], "100028": ["link_0"], "100032": ["link_0"], "100033": ["link_0"], "100038": ["link_0"], "100040": ["link_0"], "100045": ["link_0"], "100047": ["link_0"], "100051": ["link_0"], "100054": ["link_0"], "100055": [], "100056": ["link_0"], "100057": ["link_0"], "100058": ["link_0"], "100060": ["link_0"], "100613": ["link_0"], "100619": ["link_0"], "100623": ["link_0"], "100693": ["link_0"], "102080": ["link_0"], "102085": ["link_0"], "19179": ["link_0", "link_1"], "19855": ["link_0"], "19898": ["link_1", "link_2", "link_3", "link_5", "link_6", "link_7"], "20043": ["link_1", "link_2"], "20411": ["link_0"], "20555": ["link_0"], "20745": ["link_0", "link_1", "link_2"], "20985": ["link_1"], "22241": ["link_0"], "22301": ["link_0", "link_1", "link_2"], "22339": ["link_0"], "22367": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_6", "link_7"], "22508": ["link_0"], "23372": ["link_2", "link_3", "link_0", "link_1"], "23511": ["link_0"], "24644": ["link_0"], "24931": ["link_1"], "25308": ["link_0", "link_1"], "25913": ["link_0", "link_1"], "26503": ["link_1"], "26525": ["link_0"], "26652": ["link_1"], "26657": ["link_1", "link_2"], "26670": ["link_0"], "26806": ["link_0"], "27044": ["link_0"], "27189": ["link_0"], "28164": ["link_0", "link_1"], "28668": ["link_0"], "29921": ["link_1"], "30238": ["link_1", "link_2"], "30341": ["link_1"], "30666": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5", "link_6", "link_7", "link_8"], "30739": [], "31249": ["link_3", "link_4", "link_0", "link_1"], "31601": ["link_1", "link_2"], "32052": [], "32086": ["link_0", "link_1"], "32174": ["link_1", "link_0"], "32259": ["link_0", "link_1", "link_2"], "32324": ["link_0", "link_1", "link_2"], "32566": ["link_0"], "32601": ["link_2", "link_3"], "32625": ["link_0", "link_1", "link_2", "link_3"], "32761": ["link_0", "link_1", "link_2", "link_3"], "32932": ["link_0", "link_1"], "33116": ["link_2"], "33457": ["link_0", "link_2"], "33810": ["link_0", "link_2", "link_3", "link_4", "link_5"], "33930": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "34178": ["link_2", "link_3", "link_0", "link_1"], "34610": ["link_0", "link_1", "link_2", "link_3", "link_4"], "34617": ["link_1", "link_2"], "7290": [], "7220": ["link_0", "link_1"], "7120": ["link_1", "link_2"], "7179": ["link_3", "link_4"], "7187": ["link_0", "link_1"], "7201": ["link_3", "link_4"], "7332": ["link_4"], "101773": ["link_1"], "101808": ["link_0", "link_1"], "101908": ["link_0", "link_1"], "101909": ["link_0"], "101917": [], "101924": [], "101930": ["link_0"], "101931": ["link_0", "link_1"], "101940": ["link_0"], "101943": ["link_0", "link_1", "link_5", "link_6"], "101946": ["link_0"], "101947": ["link_0", "link_1", "link_3"], "101971": ["link_0"], "102001": ["link_3"], "102018": ["link_0", "link_5"], "102019": ["link_0", "link_4"], "102044": ["link_0"], "12536": [], "12617": [], "12560": ["link_0"], "12597": ["link_0"], "12552": ["link_0"], "12654": ["link_0"], "12530": [], "12565": [], "12563": ["link_0"], "12414": ["link_1"], "12558": ["link_0"], "12594": ["link_1"], "12579": [], "12621": ["link_0"], "11622": ["link_0"], "11661": ["link_1"], "11700": ["link_0"], "11826": ["link_2"], "12065": ["link_2"], "12092": ["link_0"], "12259": ["link_0"], "12349": ["link_0"], "12428": ["link_4"], "12480": ["link_1"], "12484": ["link_2", "link_3"], "12531": ["link_0"], "12540": ["link_0"], "12543": ["link_0"], "12553": [], "12559": ["link_0"], "12561": ["link_0"], "12562": ["link_0"], "12580": ["link_0"], "12583": ["link_1"], "12587": ["link_0"], "12590": ["link_0"], "12592": [], "12596": ["link_0"], "12605": ["link_0"], "12606": ["link_1"], "12614": ["link_1"], "9016": ["link_1", "link_2"], "9164": ["link_1"], "9041": ["link_0"], "9410": ["link_1"], "9388": ["link_0"], "9107": [], "9070": ["link_0"], "9386": ["link_1"], "9168": ["link_0", "link_1"], "8867": ["link_1"], "8893": ["link_2"], "8897": ["link_1"], "8903": ["link_2"], "8919": ["link_1", "link_2"], "8930": ["link_3", "link_4"], "8961": ["link_1", "link_2"], "8983": ["link_1"], "8997": ["link_0", "link_1"], "9003": ["link_1"], "9065": ["link_1"], "9117": ["link_1"], "9128": ["link_1", "link_2"], "9280": ["link_2"], "9281": [], "9288": ["link_0"], "102423": ["link_0"], "102278": ["link_0", "link_2", "link_8", "link_12"], "102389": ["link_16"], "102418": ["link_0", "link_3", "link_4", "link_7"], "101363": ["link_0"], "101564": ["link_13", "link_0", "link_11"], "101579": ["link_3"], "101584": ["link_0"], "101591": ["link_5", "link_1"], "101593": ["link_0", "link_1", "link_2"], "101594": ["link_0"], "101599": ["link_14"], "101603": ["link_0", "link_2"], "101604": ["link_0", "link_13"], "101605": ["link_0"], "101611": ["link_0"], "101612": ["link_0", "link_1"], "101619": ["link_0"], "102301": ["link_0", "link_1"], "102309": ["link_0"], "102311": ["link_1", "link_2"], "102316": ["link_0", "link_1"], "102318": ["link_0"], "102380": ["link_2"], "102381": [], "102384": ["link_0"], "102387": ["link_0"], "47645": ["link_0"], "48492": ["link_2"], "100129": ["link_1"], "100141": ["link_0"], "100174": ["link_0"], "100189": ["link_0"], "100214": ["link_0"], "100243": ["link_0"], "100247": [], "100664": ["link_0"], "102377": ["link_0", "link_1"], "102379": ["link_0"], "102456": ["link_0"], "100438": ["link_0"], "100444": ["link_0"], "100454": ["link_0"], "100470": ["link_0"], "100473": ["link_0"], "102358": ["link_0"], "103350": ["link_14"], "103593": ["link_0"], "103886": ["link_0"], "26875": ["link_0", "link_1"], "100282": ["link_0"], "100283": ["link_0"], "103351": ["link_0", "link_4"], "103361": ["link_0"], "103369": ["link_0"], "103425": ["link_0"], "103452": ["link_0"], "103480": ["link_6"], "103490": ["link_0"], "103508": ["link_0"], "103518": ["link_0"], "103521": ["link_0", "link_2", "link_3"], "103528": ["link_0"], "103775": ["link_0"], "103776": ["link_0"], "103778": ["link_0"], "22433": ["link_1"], "23782": ["link_1"], "26899": ["link_0"], "27267": ["link_0"], "101315": ["link_0"], "46966": ["link_0"], "11231": ["link_1", "link_2"], "102257": ["link_0"], "103297": ["link_1"], "102634": ["link_0", "link_1"], "48379": ["link_0", "link_1"], "103236": ["link_0", "link_1"], "12050": ["link_0", "link_2"], "41086": ["link_0"], "10586": ["link_1", "link_2"], "46859": ["link_0", "link_1"], "103669": ["link_0"], "45949": ["link_0", "link_1", "link_2"], "102177": [], "47808": ["link_0", "link_1"], "47853": ["link_0", "link_1", "link_2"], "103789": ["link_1"], "47180": ["link_1"], "103015": [], "12248": ["link_0", "link_1"], "45790": ["link_0", "link_1"], "45238": ["link_0", "link_1", "link_2", "link_3"], "41085": ["link_0", "link_1", "link_2", "link_3"], "47651": ["link_1"], "45378": ["link_0", "link_1"], "102707": ["link_2", "link_3"], "101377": [], "45354": ["link_0", "link_1"], "45756": ["link_0", "link_2"], "45779": ["link_1"], "45940": ["link_0", "link_1", "link_2"], "45503": ["link_0", "link_1", "link_2"], "103234": ["link_1"], "3971": ["link_0"], "47578": ["link_0", "link_1", "link_2", "link_3"], "11178": ["link_0", "link_1"], "45922": ["link_1"], "47315": ["link_0"], "46408": ["link_0"], "45159": ["link_0", "link_1"], "46616": ["link_0"], "46981": ["link_0", "link_1", "link_2"], "100982": ["link_0"], "100521": ["link_0"], "102984": ["link_0", "link_1"], "103307": ["link_1"], "45622": ["link_0", "link_1", "link_2"], "47281": ["link_0"], "101305": ["link_0"], "45594": ["link_0", "link_1"], "46134": ["link_0", "link_1"], "47235": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "102697": ["link_2", "link_3"], "49038": ["link_1", "link_2"], "46655": ["link_0", "link_1"], "100970": [], "9968": ["link_1"], "48013": ["link_0", "link_1"], "102801": [], "45689": ["link_0", "link_1"], "46825": ["link_0", "link_1"], "47648": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "10626": ["link_0"], "100561": ["link_0"], "9912": ["link_1"], "45676": ["link_0", "link_1"], "47817": ["link_1"], "10270": ["link_1"], "46893": ["link_0", "link_1", "link_2", "link_3"], "10211": ["link_1"], "45853": ["link_0"], "102667": ["link_2", "link_3"], "102244": ["link_0", "link_1"], "48700": ["link_0", "link_1"], "102720": ["link_0"], "10697": ["link_1"], "10143": ["link_0", "link_2"], "101320": ["link_0"], "103008": ["link_0"], "47529": ["link_0", "link_1"], "47021": ["link_0"], "41510": ["link_0", "link_1"], "11712": ["link_0", "link_1"], "11242": [], "12038": ["link_1"], "45573": ["link_0", "link_1"], "100599": ["link_0"], "45178": ["link_1"], "46230": ["link_0", "link_1", "link_2"], "102715": ["link_0"], "47443": ["link_0", "link_1"], "103635": ["link_0"], "103299": [], "46874": ["link_0", "link_1", "link_2"], "47207": ["link_0", "link_1", "link_2", "link_3"], "46641": ["link_0", "link_1", "link_2"], "103276": ["link_1"], "102675": ["link_0", "link_1"], "7265": ["link_0"], "48746": ["link_0"], "46180": ["link_0", "link_1"], "100885": ["link_0", "link_1", "link_2", "link_3"], "47944": ["link_0", "link_1", "link_2"], "7304": ["link_1"], "45130": ["link_0"], "10280": ["link_0"], "103321": [], "12477": ["link_0"], "102977": ["link_0"], "11304": ["link_0", "link_1"], "103056": [], "47585": ["link_0", "link_1", "link_2", "link_3", "link_5", "link_6", "link_19"], "45855": ["link_0"], "46699": ["link_0", "link_1"], "45323": ["link_0"], "102165": ["link_1"], "46179": ["link_0"], "45176": ["link_0"], "11945": ["link_1"], "103052": ["link_0"], "102985": ["link_1"], "103013": ["link_0"], "49062": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45623": ["link_0"], "49140": ["link_1", "link_2", "link_3", "link_4"], "100590": [], "46549": ["link_0", "link_1", "link_2", "link_3"], "100031": [], "103222": ["link_0"], "102773": ["link_0"], "101313": ["link_0"], "102714": ["link_0"], "102724": ["link_0"], "102726": ["link_1", "link_0"], "102732": ["link_0"], "102736": ["link_0"], "102739": ["link_1", "link_0"], "102753": ["link_0"], "102761": ["link_0"], "102763": ["link_0"], "102765": ["link_0"], "102768": ["link_0"], "102786": ["link_0"], "103201": ["link_0"], "103207": [], "103208": [], "100920": ["link_0", "link_1"], "102839": ["link_0", "link_4"], "102860": ["link_0"], "102812": ["link_0"], "102856": ["link_4", "link_6"], "103540": ["link_2"], "103319": ["link_0", "link_1"], "103070": ["link_2"], "103063": ["link_2", "link_4", "link_5"], "103077": [], "103148": [], "102798": [], "102802": ["link_0", "link_1"], "102803": ["link_0"], "102804": ["link_1"], "102805": ["link_1", "link_2"], "102896": ["link_0", "link_1"], "102903": ["link_1"], "102905": ["link_1"], "102906": ["link_0", "link_1"], "102981": ["link_0", "link_1"], "103032": [], "103042": ["link_1"], "103044": ["link_0", "link_1"], "103050": ["link_0", "link_1"], "103058": ["link_1"], "103150": ["link_0"], "103235": ["link_0"], "103238": ["link_0"], "103242": ["link_0"], "103253": ["link_2"], "103255": ["link_0", "link_1"], "103268": ["link_0"], "103316": [], "103318": ["link_1"], "103320": [], "103323": ["link_0"], "103325": [], "103329": ["link_0"], "103332": ["link_0", "link_1"], "103333": ["link_1"], "103339": ["link_0"], "103340": ["link_0", "link_1"], "103684": ["link_1", "link_2"], "40147": ["link_0", "link_1"], "40417": ["link_0", "link_1", "link_4", "link_5", "link_2", "link_3"], "41083": ["link_0", "link_1", "link_2", "link_3"], "44781": ["link_0", "link_1", "link_2"], "44817": ["link_0", "link_1", "link_2", "link_3"], "44826": ["link_0", "link_1"], "44853": ["link_0", "link_1", "link_2"], "44962": ["link_0", "link_1", "link_2"], "45092": ["link_0", "link_1", "link_2", "link_3"], "45132": ["link_0", "link_1", "link_2"], "45135": ["link_0", "link_1", "link_2"], "45146": ["link_1", "link_0"], "45162": ["link_0", "link_1"], "45168": ["link_1", "link_0"], "45194": ["link_0", "link_1", "link_2", "link_3"], "45219": ["link_0", "link_1", "link_2", "link_3"], "45235": ["link_0", "link_1"], "45248": ["link_0"], "45261": ["link_0", "link_1", "link_2", "link_3", "link_4"], "45262": ["link_0", "link_1", "link_2", "link_3"], "45271": ["link_2", "link_3", "link_4", "link_5", "link_0", "link_1"], "45290": ["link_0", "link_1", "link_2"], "45374": ["link_0", "link_1", "link_2", "link_3"], "45413": ["link_0"], "45427": ["link_0", "link_1", "link_2"], "45575": ["link_0", "link_1"], "45612": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45620": ["link_0"], "45632": ["link_0", "link_1", "link_2"], "45636": ["link_0", "link_1", "link_2", "link_3"], "45642": ["link_1", "link_2"], "45661": ["link_0", "link_1", "link_2"], "45677": ["link_0", "link_1", "link_2", "link_3"], "45687": ["link_0", "link_1"], "45694": ["link_0", "link_1"], "45710": ["link_0", "link_1", "link_2", "link_3"], "45746": ["link_1", "link_2"], "45759": ["link_0", "link_1", "link_2", "link_3"], "45784": ["link_0", "link_1"], "45801": ["link_0", "link_1", "link_2", "link_3"], "45822": ["link_0"], "45841": ["link_1", "link_2", "link_3"], "45910": ["link_0"], "45948": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45984": ["link_0", "link_1"], "46014": ["link_1", "link_2", "link_3", "link_4"], "46045": ["link_0", "link_1"], "46060": ["link_0", "link_1", "link_2", "link_3"], "46084": ["link_0", "link_1", "link_2", "link_3"], "46107": ["link_1"], "46109": ["link_1", "link_2", "link_3", "link_4", "link_5"], "46123": ["link_0", "link_1", "link_2"], "46127": ["link_1"], "46130": ["link_0", "link_1", "link_2"], "46132": ["link_1"], "46145": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "46172": ["link_1", "link_2", "link_3", "link_4", "link_5", "link_6", "link_7", "link_8", "link_9", "link_10", "link_11"], "46236": ["link_0", "link_1"], "46334": ["link_0", "link_1", "link_2"], "46380": ["link_1", "link_2", "link_3", "link_4"], "46439": ["link_0"], "46440": ["link_0", "link_1", "link_2"], "46443": ["link_0", "link_1", "link_2", "link_3"], "46452": ["link_0", "link_2"], "46462": ["link_0", "link_1", "link_2"], "46466": ["link_0", "link_1", "link_2", "link_3"], "46537": ["link_0", "link_1", "link_2"], "46544": ["link_0", "link_1", "link_2"], "46556": ["link_0"], "46598": ["link_0", "link_1", "link_2", "link_3"], "46653": ["link_1", "link_2", "link_3"], "46741": ["link_0", "link_1"], "46762": ["link_0", "link_1", "link_2", "link_3"], "46768": ["link_1"], "46839": ["link_0", "link_1", "link_2", "link_3"], "46856": ["link_0", "link_1", "link_2", "link_3"], "47024": ["link_0", "link_1"], "47089": ["link_0", "link_1", "link_2", "link_3"], "47168": ["link_0"], "47178": ["link_0", "link_1", "link_2"], "47183": ["link_0", "link_1", "link_2"], "47185": ["link_0", "link_1"], "47233": ["link_0", "link_1", "link_2"], "47252": ["link_0", "link_1", "link_2"], "47254": ["link_0", "link_1"], "47296": ["link_0", "link_1", "link_2"], "47391": ["link_1"], "47438": ["link_0", "link_1", "link_2"], "47565": ["link_1", "link_2"], "47570": ["link_0", "link_1"], "47711": ["link_1", "link_2", "link_3"], "47926": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "47963": ["link_1"], "48010": ["link_0", "link_1", "link_2"], "48051": ["link_1", "link_2", "link_3", "link_4", "link_5"], "48063": ["link_0", "link_1", "link_2"], "48169": ["link_0", "link_1", "link_2", "link_3", "link_5"], "48253": ["link_0", "link_1", "link_2"], "48258": ["link_0", "link_1", "link_2", "link_3"], "48263": ["link_1", "link_2", "link_3", "link_4"], "48491": ["link_1", "link_2", "link_3"], "48513": ["link_0", "link_1"], "48517": ["link_0", "link_1", "link_2"], "48740": ["link_0", "link_1", "link_2"], "48855": ["link_1", "link_2", "link_0"], "48876": ["link_0", "link_1", "link_2"], "48878": ["link_1", "link_0"], "11818": ["link_0"], "102996": ["link_0"], "11229": ["link_0"], "11259": [], "100520": [], "100523": [], "100526": ["link_0"], "100531": ["link_0"], "100532": [], "100568": [], "100579": ["link_0"], "100586": ["link_0"], "100600": ["link_0"], "100609": ["link_0"], "100611": ["link_0"], "100616": ["link_0"], "102255": [], "102263": [], "102314": ["link_0"], "102333": [], "9748": ["link_0"], "9960": ["link_0"], "9992": [], "9996": ["link_0"], "10098": [], "10101": ["link_1"], "10125": ["link_1"], "10213": ["link_1"], "10238": ["link_0"], "10239": ["link_0"], "10243": [], "10248": ["link_1"], "10269": [], "10289": [], "10305": ["link_0"], "10306": ["link_1"], "10383": ["link_1"], "10707": ["link_1"], "11030": [], "11141": ["link_1"], "11156": ["link_1"], "11395": ["link_1"], "11405": ["link_0"], "11406": ["link_1"], "11778": ["link_0"], "11429": ["link_1"], "11477": ["link_1"], "11888": ["link_0"], "10885": ["link_0"], "11854": ["link_0"], "10915": ["link_0"], "11586": ["link_1"], "11581": ["link_0"], "11876": ["link_1"], "10036": ["link_1", "link_2"], "10068": ["link_1", "link_2"], "10144": ["link_0"], "10347": ["link_0", "link_1"], "10373": ["link_0"], "10489": ["link_1", "link_2"], "10612": ["link_1", "link_2"], "10620": ["link_1", "link_2"], "10638": ["link_1", "link_2"], "10627": ["link_1", "link_2"], "10655": ["link_1", "link_2"], "10685": ["link_1", "link_2"], "10751": ["link_1", "link_2"], "10797": ["link_1"], "10849": ["link_0"], "10867": ["link_0", "link_2"], "10900": ["link_0", "link_1"], "10905": ["link_0"], "10944": ["link_0"], "11211": ["link_0"], "11299": ["link_1", "link_2"], "11550": ["link_0", "link_1"], "11709": ["link_0", "link_1", "link_2"], "12036": ["link_0", "link_1"], "12042": ["link_1"], "12043": ["link_1", "link_2"], "12054": ["link_0"], "12059": ["link_0", "link_1"], "12066": ["link_1", "link_2"], "12249": ["link_0"], "12250": ["link_1"], "12252": ["link_0"], "102620": ["link_0"], "102621": ["link_1", "link_2", "link_3"], "102622": ["link_0"], "102630": ["link_0", "link_1"], "102645": ["link_2", "link_3"], "102648": ["link_2", "link_3"], "102651": ["link_0"], "102652": ["link_0"], "102654": ["link_1", "link_2"], "102658": ["link_0", "link_1"], "102663": ["link_1", "link_2", "link_0"], "102666": ["link_1"], "102668": ["link_2", "link_3"], "102669": ["link_0"], "102670": ["link_2", "link_3"], "102676": ["link_2", "link_3", "link_1"], "102677": ["link_2", "link_3"], "102687": ["link_0", "link_1"], "102689": ["link_0", "link_2"], "102692": ["link_2", "link_1"], "102694": ["link_0"], "102699": ["link_0"], "102701": ["link_0", "link_1"], "102703": ["link_2", "link_3", "link_1"], "102708": ["link_1", "link_2"], "103646": [], "102252": ["link_0", "link_1"], "103007": [], "103012": [], "102158": ["link_2"], "101384": [], "102163": [], "103634": ["link_0"], "103647": ["link_1"], "102219": ["link_0", "link_1"], "102992": ["link_1"], "103633": ["link_0"], "102229": ["link_0"], "103639": [], "10584": ["link_0"], "11124": ["link_0"], "11279": ["link_0"], "11361": ["link_0"], "12447": [], "12483": ["link_0"], "101378": ["link_0"], "102153": ["link_2"], "102154": ["link_2"], "102155": ["link_2"], "102156": ["link_2"], "102173": [], "102181": ["link_1"], "102182": [], "102186": ["link_1"], "102189": ["link_2"], "102192": [], "102259": ["link_2"], "7236": ["link_0"], "7263": ["link_0"], "7292": ["link_0"], "7310": ["link_0"], "7366": ["link_1"], "7167": ["link_0"], "7128": ["link_0"], "7349": ["link_1"], "102990": ["link_1"], "103095": ["link_1"], "103099": [], "103100": [], "103104": ["link_1"], "103111": [], "103113": ["link_1"], "103271": ["link_1"], "103273": ["link_1"], "103280": ["link_1"], "103283": ["link_0", "link_1"], "103292": ["link_1"], "103293": ["link_1"], "103301": ["link_1"], "103303": ["link_1"], "103305": ["link_1"], "103792": [], "100849": ["link_0"], "100965": ["link_0"], "100980": ["link_1", "link_2"], "103040": [], "103135": ["link_0"], "35059": ["link_0"], "38516": ["link_0"], "41003": ["link_0", "link_1", "link_2", "link_3"], "41004": ["link_0"], "41452": ["link_0"], "41529": ["link_0"], "45001": ["link_0", "link_1"], "45007": ["link_1"], "45087": ["link_1"], "45091": ["link_1"], "45134": ["link_1"], "45164": ["link_0"], "45166": [], "45173": ["link_0"], "45177": ["link_1"], "45189": ["link_0", "link_1", "link_2", "link_3"], "45203": ["link_0"], "45212": ["link_0"], "45244": ["link_0"], "45247": ["link_1"], "45249": ["link_0"], "45267": ["link_0"], "45297": [], "45305": ["link_0", "link_1"], "45372": ["link_0"], "45384": ["link_0"], "45385": ["link_1"], "45387": ["link_0", "link_1", "link_2", "link_3"], "45397": ["link_0", "link_1"], "45415": ["link_0"], "45420": ["link_0", "link_1"], "45423": ["link_0", "link_1"], "45443": ["link_1"], "45444": ["link_0", "link_1"], "45448": ["link_0"], "45463": ["link_0", "link_1"], "45504": ["link_1"], "45505": ["link_0", "link_1"], "45516": ["link_1"], "45523": ["link_1", "link_2"], "45524": ["link_0"], "45526": ["link_0"], "45600": ["link_1"], "45606": ["link_0"], "45621": ["link_0"], "45633": ["link_1"], "45638": ["link_0"], "45645": ["link_0"], "45662": ["link_0", "link_1"], "45667": ["link_1"], "45670": ["link_0", "link_1"], "45671": ["link_1"], "45690": ["link_0"], "45691": ["link_0"], "45693": ["link_1"], "45696": ["link_0", "link_1", "link_2", "link_3"], "45699": ["link_1"], "45717": [], "45747": ["link_0", "link_1"], "45749": ["link_0", "link_1", "link_2", "link_3"], "45767": ["link_0", "link_1"], "45776": ["link_0", "link_1"], "45780": ["link_0", "link_1"], "45783": ["link_0"], "45850": ["link_1"], "45908": ["link_1"], "45915": ["link_1"], "45916": ["link_0"], "45936": ["link_0"], "45950": ["link_0"], "45961": ["link_0"], "45963": ["link_0", "link_1"], "45964": ["link_0"], "46002": ["link_0", "link_1", "link_2", "link_3"], "46019": ["link_0", "link_1"], "46029": ["link_0"], "46033": ["link_0"], "46037": ["link_0", "link_1"], "46044": ["link_1"], "46057": ["link_1"], "46092": ["link_0"], "46108": ["link_2", "link_3"], "46117": ["link_1"], "46166": ["link_1", "link_2"], "46197": ["link_0"], "46277": ["link_0", "link_1"], "46401": ["link_0"], "46417": ["link_0"], "46427": ["link_0"], "46430": ["link_1"], "46437": ["link_0", "link_1"], "46456": ["link_0", "link_1"], "46480": ["link_0", "link_1"], "46481": ["link_0", "link_1"], "46490": ["link_0", "link_1"], "46563": ["link_0", "link_1"], "46700": ["link_0", "link_1"], "46732": ["link_0", "link_1"], "46744": ["link_1"], "46787": ["link_1"], "46801": ["link_0", "link_1"], "46889": ["link_0"], "46906": ["link_1"], "46922": ["link_0"], "46944": ["link_0"], "46955": ["link_0", "link_1", "link_2", "link_3"], "47099": ["link_0", "link_1"], "47133": ["link_1"], "47182": ["link_0"], "47187": ["link_0"], "47227": ["link_0", "link_1", "link_2", "link_3"], "47278": ["link_0", "link_1"], "47290": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "47388": ["link_0"], "47419": ["link_1"], "47514": ["link_1"], "47577": ["link_0", "link_1"], "47595": ["link_0", "link_1", "link_2", "link_3"], "47601": ["link_1"], "47613": ["link_0", "link_1"], "47632": ["link_1"], "47669": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "47686": ["link_0"], "47701": ["link_0", "link_1", "link_2", "link_3"], "47729": ["link_1"], "47742": ["link_0"], "47747": ["link_0", "link_1"], "47976": ["link_0", "link_1", "link_2", "link_3"], "48018": ["link_0", "link_1", "link_2", "link_3"], "48023": ["link_1"], "48036": ["link_1"], "48167": ["link_0"], "48177": ["link_0", "link_1"], "48243": ["link_1"], "48271": ["link_0"], "48356": ["link_0", "link_1", "link_2", "link_3"], "48381": ["link_0", "link_2"], "48413": ["link_1"], "48452": ["link_0"], "48467": ["link_1"], "48490": ["link_1"], "48519": ["link_1"], "48623": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "48721": ["link_0"], "49025": ["link_0", "link_1", "link_2", "link_3"], "49042": ["link_0", "link_1"], "49132": ["link_0", "link_1"], "49133": ["link_0", "link_1"], "49182": ["link_0", "link_1"], "49188": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5"], "45403": ["link_1"], "45419": ["link_1"], "45725": ["link_0", "link_1", "link_2", "link_3", "link_4", "link_5", "link_6", "link_7", "link_8", "link_9", "link_10", "link_11", "link_12", "link_13"], "47088": ["link_0", "link_1", "link_2"], "12055": ["link_0"], "100732": ["link_1"], "45213": ["link_0"], "46847": ["link_0", "link_2"], "45332": ["link_0", "link_1"], "45243": ["link_0", "link_1", "link_2", "link_3"], "46120": ["link_0", "link_1"], "46896": ["link_0", "link_1"], "46199": ["link_0", "link_1", "link_5"], "101613": ["link_0"], "103781": ["link_0"], "26073": ["link_0"], "26387": [], "23472": ["link_0", "link_1"], "20453": ["link_0"], "23807": ["link_0", "link_1", "link_2", "link_3"], "21467": [], "20279": ["link_0"], "32354": ["link_0", "link_1", "link_2"], "19836": ["link_0", "link_1", "link_2"], "19825": ["link_0", "link_1"], "25493": ["link_0", "link_1", "link_2"], "22692": ["link_0", "link_1"], "29133": ["link_0", "link_1"], "29557": ["link_0"], "26608": ["link_4", "link_5", "link_6", "link_7"], "33914": ["link_0"], "27619": ["link_0", "link_1"], "30869": ["link_0"], "30663": ["link_0", "link_1", "link_2"], "8877": ["link_1", "link_2"], "8936": []}
diff --git a/scripts/train.py b/scripts/train.py
index 2f75761..fc04d1c 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -46,7 +46,6 @@ from flowbothd.models.modules.dit_models import (
     PN2HisDiT,
 )
 from flowbothd.models.modules.history_encoder import HistoryEncoder
-from flowbothd.models.modules.history_translator import HistoryTranslator
 from flowbothd.utils.script_utils import (
     PROJECT_ROOT,
     LogPredictionSamplesCallback,
@@ -70,7 +69,6 @@ training_module_class = {
 }
 history_network_class = {
     "encoder": HistoryEncoder,
-    "translator": HistoryTranslator,
 }
 
 
@@ -107,65 +105,49 @@ def main(cfg):
     # or with an if statement, or by using hydra.instantiate.
     ######################################################################
 
-    trajectory_len = 1 if cfg.dataset.name == "flowbot" else cfg.training.trajectory_len
-    # 1) toy_dataset = None
-    # 2) toy_dataset = {
-    #     "id": "door-1",
-    #     "train-train": ["8994", "9035"],
-    #     "train-test": ["8994", "9035"],
-    #     "test": ["8867"],
-    #     # "train-train": ["8867"],
-    #     # "train-test": ["8867"],
-    #     # "test": ["8867"],
-    # }
-    # 3) toy_dataset = {
-    #     "id": "door-full",
-    #     "train-train": ["8867", "8877", "8893", "8897", "8903", "8919", "8930", "8936", "8961", "8983", "8994", "8997", "9003", "9016", "9032", "9035", "9041", "9065", "9070", "9107", "9117", "9127", "9128", "9148", "9164", "9168", "9263", "9277", "9280", "9281", "9288", "9386", "9388", "9393", "9410"],
-    #     "train-test": ["8994"],
-    #     "test": ["9035"],
-    # }
-
-    # # Full dataset
-    # toy_dataset = None
-    # Door dataset
-    toy_dataset = {
-        "id": "door-full-new-noslide",
-        "train-train": [
-            "8877",
-            "8893",
-            "8897",
-            "8903",
-            "8919",
-            "8930",
-            "8961",
-            "8997",
-            "9016",
-            # "9032",   # has slide
-            "9035",
-            "9041",
-            "9065",
-            "9070",
-            "9107",
-            "9117",
-            "9127",
-            "9128",
-            "9148",
-            "9164",
-            "9168",
-            "9277",
-            "9280",
-            "9281",
-            "9288",
-            "9386",
-            "9388",
-            "9410",
-        ],
-        "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-        "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-    }
-    # special_req = "half-half"  # "fully-closed"
-    special_req = "half-half-01"
-    # special_req = None
+    trajectory_len = cfg.training.trajectory_len
+    special_req = cfg.dataset.special_req if cfg.dataset.special_req != "randomly-open" else None
+    if cfg.dataset.dataset_type == "full-dataset":
+        # Full dataset
+        toy_dataset = None
+    else:
+        # Door dataset
+        toy_dataset = {
+            "id": "door-full-new-noslide",
+            "train-train": [
+                "8877",
+                "8893",
+                "8897",
+                "8903",
+                "8919",
+                "8930",
+                "8961",
+                "8997",
+                "9016",
+                # "9032",   # has slide
+                "9035",
+                "9041",
+                "9065",
+                "9070",
+                "9107",
+                "9117",
+                "9127",
+                "9128",
+                "9148",
+                "9164",
+                "9168",
+                "9277",
+                "9280",
+                "9281",
+                "9288",
+                "9386",
+                "9388",
+                "9410",
+            ],
+            "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+            "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
+        }
+
 
     # Create flow dataset
     datamodule = data_module_class[cfg.dataset.name](
@@ -182,7 +164,6 @@ def main(cfg):
         n_repeat=200
         if special_req == "half-half-01"
         else (50 if special_req is None else 100),
-        # # TODO: only for toy training!!!!!
         toy_dataset=toy_dataset,
     )
     train_loader = datamodule.train_dataloader()
@@ -442,10 +423,11 @@ def main(cfg):
     # Train the model.
     ######################################################################
 
-    # trainer.fit(model, train_loader, [val_loader, train_val_loader, unseen_loader], ckpt_path='/home/yishu/flowbothd/logs/train_trajectory/2023-09-11/19-01-57/checkpoints/last.ckpt')
-    # trainer.fit(model, train_loader, [val_loader, train_val_loader, unseen_loader])
     trainer.fit(model, train_loader, [val_loader, unseen_loader])
 
+    # If we want to resume training
+    # trainer.fit(model, train_loader, [val_loader, unseen_loader], ckpt_path='/home/yishu/flowbothd/logs/train_trajectory/2023-09-11/19-01-57/checkpoints/last.ckpt')
+
 
 if __name__ == "__main__":
     main()
diff --git a/scripts/umpnet_obj_splits/gen_simu_list.ipynb b/scripts/umpnet_obj_splits/gen_simu_list.ipynb
deleted file mode 100644
index 5bd52db..0000000
--- a/scripts/umpnet_obj_splits/gen_simu_list.ipynb
+++ /dev/null
@@ -1,72 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Generate the simulation object and links"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "splits = ['test_test', 'train_test', 'train_train']"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "object_lists = {}\n",
-    "for split in splits:\n",
-    "    with open(f'./{split}_split.txt', 'r') as f:\n",
-    "        split_list = f.readlines()\n",
-    "    for sample in split_list:\n",
-    "        sample_seg = sample.replace('-v0\\n', '').split('_')\n",
-    "        object_id = sample_seg[1]\n",
-    "        link = '_'.join(sample_seg[2:])\n",
-    "        \n",
-    "        if object_id not in object_lists.keys():\n",
-    "            object_lists[object_id] = []\n",
-    "        object_lists[object_id].append(link)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import json\n",
-    "with open('../umpnet_object_list.json', 'w') as f:\n",
-    "    json.dump(object_lists, f)"
-   ]
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "openany",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.9.17"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
-}
diff --git a/setup.py b/setup.py
deleted file mode 100644
index 6068493..0000000
--- a/setup.py
+++ /dev/null
@@ -1,3 +0,0 @@
-from setuptools import setup
-
-setup()
diff --git a/src/flowbothd/models/diffusion/diffuser.py b/src/flowbothd/models/diffusion/diffuser.py
deleted file mode 100644
index aa9c7fc..0000000
--- a/src/flowbothd/models/diffusion/diffuser.py
+++ /dev/null
@@ -1,722 +0,0 @@
-## Diffusion model
-from dataclasses import dataclass
-
-import lightning as L
-import matplotlib.pyplot as plt
-import torch
-import torch.nn.functional as F
-import torch_geometric.data as tgd
-
-# import rpad.pyg.nets.pointnet2 as pnp
-import tqdm
-import wandb
-from diffusers import DDPMScheduler
-from diffusers.optimization import get_cosine_schedule_with_warmup
-from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation
-
-from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule
-from flowbothd.metrics.trajectory import (
-    artflownet_loss,
-    flow_metrics,
-    normalize_trajectory,
-)
-from flowbothd.models.diffusion.model import PNDiffuser
-
-# import flowbothd.models.diffusion.module as pnp
-
-
-@dataclass
-class TrainingConfig:
-    device = "cuda"
-
-    image_size = 128  # the generated image resolution
-    batch_size = 16
-    # train_batch_size = 16
-    # eval_batch_size = 16  # how many images to sample during evaluation
-    num_epochs = 1201
-    # num_epochs = 10
-    gradient_accumulation_steps = 1
-    learning_rate = 1e-4
-    lr_warmup_steps = 1000
-    save_image_epochs = 10
-    save_model_epochs = 30
-    mixed_precision = "no"  # `no` for float32, `fp16` for automatic mixed precision
-    # output_dir = "ddpm-butterflies-128"  # the model name locally and on the HF Hub
-    train_sample_number = 1
-
-    traj_len = 1
-    # Diffuser params
-    num_train_timesteps = 100
-    seed = 0
-    sample_size = [1, 1200]
-    in_channels = 3
-    out_channels = 3
-    cross_attention_dim = 3
-    block_out_channels = [128, 256, 512, 512]
-    attention_head_dim = 3
-
-    # ckpt params
-    read_ckpt_path = "./door_diffusion_ckpt.pth"
-    save_train_ckpt_path = "./door_diffusion_ckpt_trainbest.pth"
-    save_val_ckpt_path = "./door_diffusion_ckpt_valbest.pth"
-
-
-class TrajDiffuser:
-    def __init__(self, config, train_batch_num):
-        self.config = config
-        self.traj_len = config.traj_len
-        self.device = config.device
-
-        # self.model = UNet2DConditionModel(
-        #     sample_size=config.sample_size,
-        #     in_channels=config.in_channels,
-        #     out_channels=config.out_channels,
-        #     cross_attention_dim=config.cross_attention_dim,
-        #     block_out_channels=config.block_out_channels,
-        #     attention_head_dim=config.attention_head_dim,
-        # ).to(config.device)
-
-        # self.model = DGCNN(
-        #          in_channels=3 * config.traj_len,
-        #          sample_size=1200,
-        #          time_embed_dim=64,
-        #          emb_dims=3).to(config.device)
-
-        self.model = PNDiffuser(
-            in_channels=3 * config.traj_len,
-            # sample_size=1200,
-            traj_len=config.traj_len,
-            time_embed_dim=64,
-            # emb_dims=3
-        ).to(config.device)
-
-        # self.model = pnp.PN2Dense(
-        #     in_channels=3 * (self.traj_len + 1),   # noise concatenated with condition signal
-        #     out_channels=3 * self.traj_len,
-        #     p=pnp.PN2DenseParams(),
-        # )
-
-        self.noise_scheduler = DDPMScheduler(
-            num_train_timesteps=config.num_train_timesteps
-        )
-        self.optimizer = torch.optim.AdamW(
-            self.model.parameters(), lr=config.learning_rate
-        )
-        self.lr_scheduler = get_cosine_schedule_with_warmup(
-            optimizer=self.optimizer,
-            num_warmup_steps=config.lr_warmup_steps,
-            num_training_steps=(train_batch_num * config.num_epochs),
-            # num_training_steps=((config.train_sample_number // config.batch_size) * config.num_epochs),
-            # num_training_steps=(config.num_epochs),
-        )
-
-    def load_model(self, ckpt_path="./diffusion_best_ckpt.pth"):
-        self.model.load_state_dict(torch.load(ckpt_path))
-
-    def train(
-        self, train_dataloader, train_val_dataloader, val_dataloader, unseen_dataloader
-    ):  # TODO:currently only support overfit
-        ## Train loop
-        losses = []
-        min_val_flow_loss = 1e9
-        min_train_flow_loss = 1e9
-        global_step = 0
-        # clean_flow = torch.tensor(sample.delta.transpose(0, 2).unsqueeze(0)).to(
-        #     self.device
-        # )
-        # condition = torch.tensor(sample.pos.unsqueeze(0)).to(self.device)
-        self.model.train()
-        for epoch in range(config.num_epochs):
-            print(f"Epoch: {epoch}")
-            for step, batch in tqdm.tqdm(enumerate(train_dataloader)):
-                global_step += 1
-
-                clean_flow = batch.delta
-                clean_flow = clean_flow.reshape(
-                    -1, 1200, clean_flow.shape[1], clean_flow.shape[2]
-                ).to(self.device)
-                condition = batch.pos
-                condition = condition.reshape(-1, 1200, condition.shape[1]).to(
-                    self.device
-                )
-                # breakpoint()
-
-                # Random permutation
-                # perm = torch.randperm(1200).to(self.device)
-                # clean_flow = clean_flow[:, perm]
-                # condition = condition[:, perm]
-
-                clean_flow = clean_flow.transpose(1, 3)
-
-                # breakpoint()
-
-                # Sample noise to add to the images
-                noise = torch.randn(clean_flow.shape).to(clean_flow.device)
-                bs = clean_flow.shape[0]
-
-                # Sample a random timestep for each image
-                timesteps = torch.randint(
-                    0,
-                    self.noise_scheduler.config.num_train_timesteps,
-                    (bs,),
-                    device=clean_flow.device,
-                ).long()
-
-                # Add noise to the clean images according to the noise magnitude at each timestep
-                # (this is the forward diffusion process)
-                noisy_images = self.noise_scheduler.add_noise(
-                    clean_flow, noise, timesteps
-                )
-
-                # Predict the noise residual
-                # noise_pred = self.model(
-                #     noisy_images,
-                #     encoder_hidden_states=condition,
-                #     timestep=timesteps,
-                #     return_dict=False,
-                # )[0]
-                # model_input.pos = condition
-                noise_pred = self.model(
-                    noisy_images, timesteps, context=batch, return_dict=False
-                )[0]
-                loss = F.mse_loss(noise_pred, noise)
-
-                if global_step % 1 == 0:
-                    wandb.log({"train_loss/loss": loss}, step=global_step)
-                    wandb.log(
-                        {"train_loss/lr": self.lr_scheduler.get_lr()}, step=global_step
-                    )
-
-                loss.backward()
-                self.optimizer.step()
-                self.lr_scheduler.step()
-                self.optimizer.zero_grad()
-
-                loss = loss.detach().item()
-                losses.append(loss)
-                # print("loss", loss.detach().item(), "lr", lr_scheduler.get_last_lr()[0], "step", epoch)
-
-            # Wandb
-            if epoch % 2 == 0:
-                # TODO: multimodal eval
-                # Trainset val Metric
-                # metric = self.predict(train_val_dataloader, vis=False)
-                metric = self.predict_wta(train_val_dataloader, trial_times=20)
-                for metric_name in metric.keys():
-                    if metric_name == "all_directions":
-                        continue
-                    wandb.log({f"train/{metric_name}": metric[metric_name]})
-
-                if metric["best_flow_loss"] < min_train_flow_loss:
-                    min_train_flow_loss = metric["best_flow_loss"]
-                    torch.save(
-                        self.model.state_dict(), self.config.save_train_ckpt_path
-                    )
-
-                # Validation Metric
-                # metric = self.predict(val_dataloader, vis=False)
-                metric = self.predict_wta(val_dataloader, trial_times=20)
-                for metric_name in metric.keys():
-                    if metric_name == "all_directions":
-                        continue
-                    wandb.log({f"val/{metric_name}": metric[metric_name]})
-
-                if metric["best_flow_loss"] < min_val_flow_loss:
-                    min_val_flow_loss = metric["best_flow_loss"]
-                    torch.save(self.model.state_dict(), self.config.save_val_ckpt_path)
-                # # Test Metric
-                # # metric = self.predict(unseen_dataloader, vis=False)
-                # metric = self.predict_wta(unseen_dataloader, trial_times=20)
-                # for metric_name in metric.keys():
-                #     if metric_name == "all_directions":
-                #         continue
-                #     wandb.log({f"test/{metric_name}": metric[metric_name]}, step=global_step)
-
-        # Visualize loss
-        plt.figure()
-        plt.plot(losses[::50])
-
-    def predict_wta(self, val_dataloader, trial_times=20):
-        self.model.eval()
-
-        valid_sample_count = 0
-
-        all_rmse = 0
-        all_cos_dist = 0
-        all_mag_error = 0
-        all_flow_loss = 0
-        all_multimodal = 0
-        all_pos_cosine = 0
-        all_neg_cosine = 0
-
-        all_directions = []  # dataloader * trial_times
-
-        # Eval every dataloader
-        with torch.no_grad():
-            for id, orig_sample in tqdm.tqdm(enumerate(val_dataloader)):
-                # breakpoint()
-                batch_size = orig_sample.pos.shape[0] // 1200
-                assert batch_size == 1, f"batch size should be 1, now is {batch_size}"
-
-                # batch every sample into bsz of trial_times
-                data_list = orig_sample.to_data_list() * trial_times
-                sample = tgd.Batch.from_data_list(data_list)
-                batch_size = trial_times
-
-                # breakpoint()
-
-                noisy_input = (
-                    torch.randn((batch_size, 3, self.traj_len, 1200))
-                    .float()
-                    .to(self.device)
-                )
-
-                condition = sample.pos
-                # breakpoint()
-                condition = condition.reshape(-1, 1200, condition.shape[1]).to(
-                    self.device
-                )
-
-                mask = sample.mask == 1
-                if torch.sum(mask) == 0:  # Skip those unmovable samples
-                    continue
-                valid_sample_count += 1
-
-                mask = mask.reshape(-1, 1200).to(self.device)
-                # sample.mask = mask.repeat(batch_size, 1)
-
-                flow_gt = sample.delta.to(self.device)
-                flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-                # flow_gt = flow_gt.repeat(batch_size, 1, 1, 1)
-                # breakpoint()
-
-                # # Random permutation
-                # perm = torch.randperm(1200).to(self.device)
-                # mask = mask[:, perm]
-                # flow_gt = flow_gt[:, perm]
-                # condition = condition[:, perm]
-
-                flow_gt = normalize_trajectory(
-                    torch.flatten(flow_gt, start_dim=0, end_dim=1)
-                )
-                masked_flow_gt = flow_gt.reshape(
-                    -1, 1200, flow_gt.shape[1], flow_gt.shape[2]
-                )
-                masked_flow_gt = masked_flow_gt[mask == 1]
-
-                for t in self.noise_scheduler.timesteps:
-                    # model_output = self.model(
-                    #     noisy_input, encoder_hidden_states=condition, timestep=t
-                    # ).sample
-                    # breakpoint()
-                    model_output = self.model(noisy_input, t, context=sample).sample
-
-                    noisy_input = self.noise_scheduler.step(
-                        model_output, t, noisy_input
-                    ).prev_sample
-
-                flow_prediction = noisy_input.transpose(1, 3)
-                flow_prediction = normalize_trajectory(
-                    torch.flatten(flow_prediction, start_dim=0, end_dim=1)
-                )
-                masked_flow_prediction = flow_prediction.reshape(
-                    -1, 1200, flow_prediction.shape[1], flow_prediction.shape[2]
-                )
-                masked_flow_prediction = masked_flow_prediction[mask == 1]
-                n_nodes = torch.as_tensor([d.num_nodes for d in sample.to_data_list()]).to(self.device)  # type: ignore
-
-                flow_loss = artflownet_loss(
-                    flow_prediction, flow_gt, n_nodes, reduce=False
-                )
-
-                # Compute some metrics on flow-only regions.
-                rmse, cos_dist, mag_error = flow_metrics(
-                    masked_flow_prediction, masked_flow_gt, reduce=False
-                )
-
-                # Aggregate the results
-                # Choose the one with smallest flow loss
-                flow_loss = flow_loss.reshape(batch_size, -1).mean(-1)
-                rmse = rmse.reshape(batch_size, -1).mean(-1)
-                cos_dist = cos_dist.reshape(batch_size, -1).mean(-1)
-                all_directions += list(cos_dist)
-
-                mag_error = mag_error.reshape(batch_size, -1).mean(-1)
-
-                pos_cosine = torch.sum((cos_dist - 0.7) > 0)
-                neg_cosine = torch.sum((cos_dist + 0.7) < 0)
-                multimodal = 1 if (pos_cosine != 0 and neg_cosine != 0) else 0
-
-                # breakpoint()
-                chosen_id = torch.min(flow_loss, 0)[1]  # index
-                # chosen_direction = cos_dist[chosen_id]
-                # if chosen_direction > 0:
-                #     multimodal = torch.sum((cos_dist + 0.7) < 0) != 0  # < -0.7
-                # else:
-                #     multimodal = torch.sum((cos_dist - 0.7) > 0) != 0  # > 0.7
-
-                # print(multimodal, rmse[chosen_id], cos_dist[chosen_id], mag_error[chosen_id], flow_loss[chosen_id])
-                all_pos_cosine += pos_cosine.item() / trial_times
-                all_neg_cosine += neg_cosine.item() / trial_times
-                all_multimodal += multimodal  # .item()
-                all_rmse += rmse[chosen_id].item()
-                all_cos_dist += cos_dist[chosen_id].item()
-                all_mag_error += mag_error[chosen_id].item()
-                all_flow_loss += flow_loss[chosen_id].item()
-
-                # print(all_rmse)
-
-        return {
-            "best_rmse": all_rmse / valid_sample_count,
-            "best_cosine": all_cos_dist / valid_sample_count,
-            "best_mag": all_mag_error / valid_sample_count,
-            "best_flow_loss": all_flow_loss / valid_sample_count,
-            "multimodal_ratio": all_multimodal / valid_sample_count,
-            "pos_rate": all_pos_cosine / valid_sample_count,
-            "neg_rate": all_neg_cosine / valid_sample_count,
-            "all_directions": all_directions,
-        }
-
-    def predict(self, val_dataloader, vis=False):
-        self.model.eval()
-
-        all_rmse = 0
-        all_cos_dist = 0
-        all_mag_error = 0
-        all_flow_loss = 0
-
-        with torch.no_grad():
-            for id, sample in enumerate(val_dataloader):
-                # breakpoint()
-                batch_size = sample.pos.shape[0] // 1200
-                noisy_input = (
-                    torch.randn((batch_size, 3, self.traj_len, 1200))
-                    .float()
-                    .to(self.device)
-                )
-                # condition = condition
-                # breakpoint()
-                condition = sample.pos
-                condition = condition.reshape(-1, 1200, condition.shape[1]).to(
-                    self.device
-                )
-
-                mask = sample.mask == 1
-                mask = mask.reshape(-1, 1200).to(self.device)
-
-                flow_gt = sample.delta.to(self.device)
-                flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-
-                # breakpoint()
-
-                # # Random permutation
-                # perm = torch.randperm(1200).to(self.device)
-                # mask = mask[:, perm]
-                # flow_gt = flow_gt[:, perm]
-                # condition = condition[:, perm]
-
-                flow_gt = normalize_trajectory(
-                    torch.flatten(flow_gt, start_dim=0, end_dim=1)
-                )
-                masked_flow_gt = flow_gt.reshape(
-                    -1, 1200, flow_gt.shape[1], flow_gt.shape[2]
-                )
-                masked_flow_gt = masked_flow_gt[mask == 1]
-
-                if vis:
-                    animation = FlowNetAnimation()
-                    pcd = sample.pos.numpy()
-
-                for t in self.noise_scheduler.timesteps:
-                    # model_output = self.model(
-                    #     noisy_input, encoder_hidden_states=condition, timestep=t
-                    # ).sample
-                    # breakpoint()
-                    model_output = self.model(noisy_input, t, context=sample).sample
-
-                    noisy_input = self.noise_scheduler.step(
-                        model_output, t, noisy_input
-                    ).prev_sample
-
-                    print(model_output)
-
-                    if vis:
-                        # print(noisy_input.shape)
-                        # print(torch.nn.functional.normalize(noisy_input, p=2, dim=1)
-                        #     .squeeze().permute(1, 0).shape)
-                        # print(torch.flatten(noisy_input.transpose(1, 3), start_dim=0, end_dim=1).shape)
-                        if t % 5 == 0 or t == 99:
-                            flow = (
-                                # torch.nn.functional.normalize(noisy_input, p=2, dim=1)
-                                # .squeeze()
-                                # .permute(1, 0)
-                                normalize_trajectory(
-                                    torch.flatten(
-                                        noisy_input.transpose(1, 3),
-                                        start_dim=0,
-                                        end_dim=1,
-                                    )
-                                )[:, 0, :]
-                            )
-                            animation.add_trace(
-                                torch.as_tensor(pcd),
-                                torch.as_tensor(
-                                    [pcd[mask.squeeze().detach().cpu().numpy()]]
-                                ),
-                                torch.as_tensor(
-                                    [
-                                        flow[mask.detach().cpu().numpy()]
-                                        .detach()
-                                        .cpu()
-                                        .numpy()
-                                    ]
-                                ),
-                                "red",
-                            )
-
-                flow_prediction = noisy_input.transpose(1, 3)
-                flow_prediction = normalize_trajectory(
-                    torch.flatten(flow_prediction, start_dim=0, end_dim=1)
-                )
-                masked_flow_prediction = flow_prediction.reshape(
-                    -1, 1200, flow_prediction.shape[1], flow_prediction.shape[2]
-                )
-                # flow_prediction = torch.nn.functional.normalize(
-                #     flow_prediction, p=2, dim=-1
-                # )
-                # largest_mag: float = torch.linalg.norm(
-                #     flow_prediction, ord=2, dim=-1
-                # ).max()
-                # flow_prediction = flow_prediction / (largest_mag + 1e-6)
-                # flow_prediction = flow_prediction.permute(1, 2, 0)
-                masked_flow_prediction = masked_flow_prediction[mask == 1]
-                # breakpoint()
-
-                # mean_dist = (masked_flow_prediction - flow_gt).norm(p=2, dim=-1).mean()
-                # cos_sim = torch.cosine_similarity(
-                #     masked_flow_prediction, flow_gt, dim=-1
-                # ).mean()
-                n_nodes = torch.as_tensor([d.num_nodes for d in sample.to_data_list()]).to(self.device)  # type: ignore
-                # breakpoint()
-                flow_loss = artflownet_loss(flow_prediction, flow_gt, n_nodes)
-
-                # Compute some metrics on flow-only regions.
-                rmse, cos_dist, mag_error = flow_metrics(
-                    masked_flow_prediction, masked_flow_gt
-                )
-
-                all_rmse += rmse.item()
-                all_cos_dist = cos_dist.item()
-                all_mag_error = mag_error.item()
-                all_flow_loss = flow_loss.item()
-
-        if vis:
-            # fig = animation.animate()
-            # fig.show()
-
-            return {
-                "animation": animation,
-                "rmse": all_rmse / len(val_dataloader),
-                "cos_dist": all_cos_dist / len(val_dataloader),
-                "mag_error": all_mag_error / len(val_dataloader),
-                "flow_loss": all_flow_loss / len(val_dataloader),
-            }
-        else:
-            return {
-                "rmse": all_rmse / len(val_dataloader),
-                "cos_dist": all_cos_dist / len(val_dataloader),
-                "mag_error": all_mag_error / len(val_dataloader),
-                "flow_loss": all_flow_loss / len(val_dataloader),
-            }
-
-    def predict_step(self, sample):  # For a single sample
-        batch_size = sample.pos.shape[0] // 1200
-        noisy_input = (
-            torch.randn((batch_size, 3, self.traj_len, 1200)).float().to(self.device)
-        )
-
-        condition = sample.pos
-        condition = condition.reshape(-1, 1200, condition.shape[1]).to(self.device)
-
-        for t in self.noise_scheduler.timesteps:
-            model_output = self.model(noisy_input, t, context=sample).sample
-
-            noisy_input = self.noise_scheduler.step(
-                model_output, t, noisy_input
-            ).prev_sample
-
-        flow_prediction = noisy_input.transpose(1, 3)
-
-        # # Metric
-
-        # new_flow_prediction = normalize_trajectory(
-        #         torch.flatten(flow_prediction, start_dim=0, end_dim=1)
-        #     )
-        # new_flow_prediction = new_flow_prediction.reshape(
-        #     -1, 1200, new_flow_prediction.shape[1], new_flow_prediction.shape[2]
-        # )
-
-        # flow_gt = sample.delta.to(self.device)
-        # flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-
-        # flow_gt = normalize_trajectory(
-        #     torch.flatten(flow_gt, start_dim=0, end_dim=1)
-        # )
-        # flow_gt = flow_gt.reshape(
-        #     -1, 1200, flow_gt.shape[1], flow_gt.shape[2]
-        # )
-
-        # n_nodes = torch.as_tensor([d.num_nodes for d in sample.to_data_list()]).to(self.device)  # type: ignore
-        # # breakpoint()
-        # flow_loss = artflownet_loss(flow_prediction, flow_gt, n_nodes)
-
-        # # Compute some metrics on flow-only regions.
-        # rmse, cos_dist, mag_error = flow_metrics(
-        #     new_flow_prediction, flow_gt
-        # )
-
-        # print("flow_loss: ", flow_loss)
-        # print("rmse, cos_dist, mag_error: ", rmse, cos_dist, mag_error)
-        return flow_prediction
-
-
-class TrajDiffuserSimWrapper(L.LightningModule):
-    def __init__(self, diffuser):
-        super().__init__()
-        self.diffuser = diffuser
-
-    def forward(self, data):
-        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
-        data = tgd.Data(
-            pos=torch.from_numpy(P_world).float().cuda(),
-            # mask=torch.ones(P_world.shape[0]).float(),
-        )
-        batch = tgd.Batch.from_data_list([data])
-        self.eval()
-        with torch.no_grad():
-            flow = self.diffuser.predict_step(batch)
-        return flow.squeeze().cpu()
-
-
-if __name__ == "__main__":
-    config = TrainingConfig()
-
-    wandb.init(
-        entity="r-pad",
-        # entity="leisure-thu-cv",
-        project="flowbothd",
-        group="diffusion-PN++",
-        job_type="train_closed_doors",
-        # job_type="overfit_trajectory",
-        # group="fullset_mixed_diffusion",
-        # job_type="train_diffuser_wta",
-    )
-
-    datamodule = FlowTrajectoryDataModule(
-        root="/home/yishu/datasets/partnet-mobility",
-        batch_size=16,
-        num_workers=30,
-        n_proc=2,
-        seed=42,
-        trajectory_len=config.traj_len,  # Only used when training trajectory model
-        # special_req="half-half",
-        special_req="fully-closed",
-        # toy_dataset = {
-        #     "id": "door-1",
-        #     "train-train": ["8994", "9035"],
-        #     "train-test": ["8994", "9035"],
-        #     "test": ["8867"],
-        #     # "train-train": ["8867"],
-        #     # "train-test": ["8867"],
-        #     # "test": ["8867"],
-        # }
-        toy_dataset={
-            "id": "door-full-new",
-            "train-train": [
-                "8877",
-                "8893",
-                "8897",
-                "8903",
-                "8919",
-                "8930",
-                "8961",
-                "8997",
-                "9016",
-                "9032",
-                "9035",
-                "9041",
-                "9065",
-                "9070",
-                "9107",
-                "9117",
-                "9127",
-                "9128",
-                "9148",
-                "9164",
-                "9168",
-                "9277",
-                "9280",
-                "9281",
-                "9288",
-                "9386",
-                "9388",
-                "9410",
-            ],
-            "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-            "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-        }
-        # toy_dataset = {
-        #     "id": "door-single-test",
-        #     "train-train": ["8877"],
-        #     "train-test": ["8877"],
-        #     "test": ["8877"],
-        # }
-    )
-
-    train_dataloader = datamodule.train_dataloader()
-    train_val_dataloader = datamodule.train_val_dataloader(bsz=1)
-    val_dataloader = datamodule.val_dataloader(bsz=1)
-    unseen_dataloader = datamodule.unseen_dataloader(bsz=1)
-
-    diffuser = TrajDiffuser(config, train_batch_num=len(train_dataloader))
-
-    # Train
-    # diffuser.load_model('/home/yishu/flowbothd/src/flowbothd/models/diffusion/door_diffusion_multimodal_ckpt.pth')
-    diffuser.train(
-        train_dataloader, train_val_dataloader, val_dataloader, unseen_dataloader
-    )
-
-    # # Overfit
-    # datamodule = FlowTrajectoryDataModule(
-    #     root="/home/yishu/datasets/partnet-mobility",
-    #     batch_size=1,
-    #     num_workers=30,
-    #     n_proc=2,
-    #     seed=42,
-    #     trajectory_len=config.traj_len,  # Only used when training trajectory model
-    # )
-
-    # train_dataloader = datamodule.train_dataloader()
-    # val_dataloader = datamodule.train_val_dataloader()
-
-    # # # Overfit
-    # samples = list(enumerate(train_dataloader))
-    # # breakpoint()
-    # sample = samples[0][1]
-    # diffuser.train([sample], [sample])
-
-    wandb.finish()
-
-    # diffuser.load_model('/home/yishu/flowbothd/logs/train_trajectory/2023-08-31/01-21-42/checkpoints/epoch=199-step=157200.ckpt')
-
-    # ##  Overfit sample prediction
-    # metric = diffuser.predict(sample, vis=True)
-    # print(f"dist:{metric['mean_dist']} cos:{metric['cos_sim']}")
-
-    # # Permutated sample prediction
-    # indices = torch.randperm(1200)
-    # sample.pos = sample.pos[indices]
-    # sample.delta = sample.delta[indices]
-    # sample.mask = sample.mask[indices]
-    # metric = diffuser.predict(sample, vis=True)
-    # print(f"dist:{metric['mean_dist']} cos:{metric['cos_sim']}")
diff --git a/src/flowbothd/models/diffusion/diffuser_synthetic.py b/src/flowbothd/models/diffusion/diffuser_synthetic.py
deleted file mode 100644
index 50fc95c..0000000
--- a/src/flowbothd/models/diffusion/diffuser_synthetic.py
+++ /dev/null
@@ -1,693 +0,0 @@
-## Diffusion model
-from dataclasses import dataclass
-
-import lightning as L
-import matplotlib.pyplot as plt
-import torch
-import torch.nn.functional as F
-import torch_geometric.data as tgd
-
-# import rpad.pyg.nets.pointnet2 as pnp
-import tqdm
-import wandb
-from diffusers import DDPMScheduler
-from diffusers.optimization import get_cosine_schedule_with_warmup
-from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation
-
-from flowbothd.metrics.trajectory import (
-    artflownet_loss,
-    flow_metrics,
-    normalize_trajectory,
-)
-from flowbothd.models.diffusion.model import PNDiffuser
-
-# import flowbothd.models.diffusion.module as pnp
-
-
-@dataclass
-class TrainingConfig:
-    device = "cuda"
-
-    image_size = 128  # the generated image resolution
-    batch_size = 32
-    # train_batch_size = 16
-    # eval_batch_size = 16  # how many images to sample during evaluation
-    num_epochs = 1000
-    # num_epochs = 10
-    gradient_accumulation_steps = 1
-    learning_rate = 1e-5
-    lr_warmup_steps = 10
-    save_image_epochs = 10
-    save_model_epochs = 30
-    mixed_precision = "no"  # `no` for float32, `fp16` for automatic mixed precision
-    # output_dir = "ddpm-butterflies-128"  # the model name locally and on the HF Hub
-    train_sample_number = 1
-
-    traj_len = 1
-    # Diffuser params
-    num_train_timesteps = 10
-    seed = 0
-    sample_size = [1, 1200]
-    in_channels = 3
-    out_channels = 3
-    cross_attention_dim = 3
-    block_out_channels = [128, 256, 512, 512]
-    attention_head_dim = 3
-
-    # ckpt params
-    read_ckpt_path = "./diffusion_synthetic_ckpt.pth"
-    save_ckpt_path = "./diffusion_synthetic_ckpt.pth"
-
-
-class TrajDiffuser:
-    def __init__(self, config, train_batch_num):
-        self.config = config
-        self.traj_len = config.traj_len
-        self.device = config.device
-
-        # self.model = UNet2DConditionModel(
-        #     sample_size=config.sample_size,
-        #     in_channels=config.in_channels,
-        #     out_channels=config.out_channels,
-        #     cross_attention_dim=config.cross_attention_dim,
-        #     block_out_channels=config.block_out_channels,
-        #     attention_head_dim=config.attention_head_dim,
-        # ).to(config.device)
-
-        # self.model = DGCNN(
-        #          in_channels=3 * config.traj_len,
-        #          sample_size=1200,
-        #          time_embed_dim=64,
-        #          emb_dims=3).to(config.device)
-
-        self.model = PNDiffuser(
-            in_channels=3 * config.traj_len,
-            # sample_size=1200,
-            traj_len=config.traj_len,
-            time_embed_dim=64,
-            # emb_dims=3
-        ).to(config.device)
-
-        # self.model = pnp.PN2Dense(
-        #     in_channels=3 * (self.traj_len + 1),   # noise concatenated with condition signal
-        #     out_channels=3 * self.traj_len,
-        #     p=pnp.PN2DenseParams(),
-        # )
-
-        self.noise_scheduler = DDPMScheduler(
-            num_train_timesteps=config.num_train_timesteps
-        )
-        self.optimizer = torch.optim.AdamW(
-            self.model.parameters(), lr=config.learning_rate
-        )
-        self.lr_scheduler = get_cosine_schedule_with_warmup(
-            optimizer=self.optimizer,
-            num_warmup_steps=config.lr_warmup_steps,
-            num_training_steps=(train_batch_num * config.num_epochs),
-            # num_training_steps=((config.train_sample_number // config.batch_size) * config.num_epochs),
-            # num_training_steps=(config.num_epochs),
-        )
-
-    def load_model(self, ckpt_path="./diffusion_best_ckpt.pth"):
-        self.model.load_state_dict(torch.load(ckpt_path))
-
-    def train(
-        self, train_dataloader, train_val_dataloader, val_dataloader, unseen_dataloader
-    ):  # TODO:currently only support overfit
-        ## Train loop
-        losses = []
-        min_loss = 100
-        global_step = 0
-        # clean_flow = torch.tensor(sample.delta.transpose(0, 2).unsqueeze(0)).to(
-        #     self.device
-        # )
-        # condition = torch.tensor(sample.pos.unsqueeze(0)).to(self.device)
-        for epoch in range(config.num_epochs):
-            print(f"Epoch: {epoch}")
-            self.model.train()
-            for step, batch in tqdm.tqdm(enumerate(train_dataloader)):
-                global_step += 1
-
-                clean_flow = batch.delta
-                clean_flow = clean_flow.reshape(
-                    -1, 1200, clean_flow.shape[1], clean_flow.shape[2]
-                ).to(self.device)
-                condition = batch.pos
-                condition = condition.reshape(-1, 1200, condition.shape[1]).to(
-                    self.device
-                )
-                # breakpoint()
-
-                # Random permutation
-                # perm = torch.randperm(1200).to(self.device)
-                # clean_flow = clean_flow[:, perm]
-                # condition = condition[:, perm]
-
-                clean_flow = clean_flow.transpose(1, 3)
-
-                # breakpoint()
-
-                # Sample noise to add to the images
-                noise = torch.randn(clean_flow.shape).to(clean_flow.device)
-                bs = clean_flow.shape[0]
-
-                # Sample a random timestep for each image
-                timesteps = torch.randint(
-                    0,
-                    self.noise_scheduler.config.num_train_timesteps,
-                    (bs,),
-                    device=clean_flow.device,
-                ).long()
-
-                # Add noise to the clean images according to the noise magnitude at each timestep
-                # (this is the forward diffusion process)
-                noisy_images = self.noise_scheduler.add_noise(
-                    clean_flow, noise, timesteps
-                )
-
-                # Predict the noise residual
-                # noise_pred = self.model(
-                #     noisy_images,
-                #     encoder_hidden_states=condition,
-                #     timestep=timesteps,
-                #     return_dict=False,
-                # )[0]
-                # model_input.pos = condition
-                noise_pred = self.model(
-                    noisy_images, timesteps, context=batch, return_dict=False
-                )[0]
-
-                # print("train: ", noisy_images[:, :, :, :2], noise_pred[:, :, :, :2])
-                # print("pred: ", noise_pred[:2])
-                # print("noisy_images: ", noisy_images[:2])
-                # print("noise: ", noise[:2])
-                # breakpoint()
-                loss = F.mse_loss(noise_pred, noise)
-
-                if global_step % 1 == 0:
-                    wandb.log({"train_loss/loss": loss}, step=global_step)
-                    wandb.log(
-                        {"train_loss/lr": self.lr_scheduler.get_lr()}, step=global_step
-                    )
-
-                loss.backward()
-                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
-                self.optimizer.step()
-                self.lr_scheduler.step()
-                self.optimizer.zero_grad()
-
-                loss = loss.detach().item()
-                # if epoch % 10 == 0 and loss < min_loss:
-                #     min_loss = loss
-                #     torch.save(self.model.state_dict(), self.config.save_ckpt_path)
-                losses.append(loss)
-                # print("loss", loss.detach().item(), "lr", lr_scheduler.get_last_lr()[0], "step", epoch)
-
-            # Wandb
-            if epoch % 10 == 0:
-                # TODO: multimodal eval
-                # Trainset val Metric
-                mrmse = 0
-                mcos = 0
-                mmag = 0
-                mflow = 0
-                repeat_time = 1
-                for i in range(repeat_time):
-                    # metric = self.predict(train_val_dataloader, vis=False)
-                    metric = self.predict_wta(train_val_dataloader, trial_times=20)
-                    mrmse += metric["rmse"]
-                    mcos += metric["cos_dist"]
-                    mmag += metric["mag_error"]
-                    mflow += metric["flow_loss"]
-                wandb.log({"train/rmse": mrmse / repeat_time}, step=global_step)
-                wandb.log({"train/cos": mcos / repeat_time}, step=global_step)
-                wandb.log({"train/mag": mmag / repeat_time}, step=global_step)
-                wandb.log({"train/flow": mflow / repeat_time}, step=global_step)
-                wandb.log({"train/multimodal": metric["multimodal"]}, step=global_step)
-
-                # Validation Metric
-                mrmse = 0
-                mcos = 0
-                mmag = 0
-                mflow = 0
-                repeat_time = 1
-                for i in range(repeat_time):
-                    # metric = self.predict(val_dataloader, vis=False)
-                    metric = self.predict_wta(val_dataloader, trial_times=20)
-                    mrmse += metric["rmse"]
-                    mcos += metric["cos_dist"]
-                    mmag += metric["mag_error"]
-                    mflow += metric["flow_loss"]
-                wandb.log({"val/rmse": mrmse / repeat_time}, step=global_step)
-                wandb.log({"val/cos": mcos / repeat_time}, step=global_step)
-                wandb.log({"val/mag": mmag / repeat_time}, step=global_step)
-                wandb.log({"val/flow": mflow / repeat_time}, step=global_step)
-                wandb.log({"val/multimodal": metric["multimodal"]}, step=global_step)
-
-                if epoch % 10 == 0 and mflow < min_loss:
-                    min_loss = mflow
-                    torch.save(self.model.state_dict(), self.config.save_ckpt_path)
-
-                # Test Metric
-                mrmse = 0
-                mcos = 0
-                mmag = 0
-                mflow = 0
-                repeat_time = 1
-                for i in range(repeat_time):
-                    # metric = self.predict(unseen_dataloader, vis=False)
-                    metric = self.predict_wta(unseen_dataloader, trial_times=20)
-                    mrmse += metric["rmse"]
-                    mcos += metric["cos_dist"]
-                    mmag += metric["mag_error"]
-                    mflow += metric["flow_loss"]
-                wandb.log({"unseen/rmse": mrmse / repeat_time}, step=global_step)
-                wandb.log({"unseen/cos": mcos / repeat_time}, step=global_step)
-                wandb.log({"unseen/mag": mmag / repeat_time}, step=global_step)
-                wandb.log({"unseen/flow": mflow / repeat_time}, step=global_step)
-
-        # Visualize loss
-        plt.figure()
-        plt.plot(losses[::50])
-
-    def predict_wta(self, val_dataloader, trial_times=20):
-        self.model.eval()
-
-        valid_sample_count = 0
-
-        all_rmse = 0
-        all_cos_dist = 0
-        all_mag_error = 0
-        all_flow_loss = 0
-        all_multimodal = 0
-
-        all_directions = []  # dataloader * trial_times
-
-        # Eval every dataloader
-        with torch.no_grad():
-            for id, orig_sample in tqdm.tqdm(enumerate(val_dataloader)):
-                # breakpoint()
-                batch_size = orig_sample.pos.shape[0] // 1200
-                assert batch_size == 1, f"batch size should be 1, now is {batch_size}"
-
-                # batch every sample into bsz of trial_times
-                data_list = orig_sample.to_data_list() * trial_times
-                sample = tgd.Batch.from_data_list(data_list)
-                batch_size = trial_times
-
-                # breakpoint()
-
-                noisy_input = (
-                    torch.randn((batch_size, 3, self.traj_len, 1200))
-                    .float()
-                    .to(self.device)
-                )
-
-                condition = sample.pos
-                # breakpoint()
-                condition = condition.reshape(-1, 1200, condition.shape[1]).to(
-                    self.device
-                )
-
-                mask = sample.mask == 1
-                if torch.sum(mask) == 0:  # Skip those unmovable samples
-                    continue
-                valid_sample_count += 1
-
-                mask = mask.reshape(-1, 1200).to(self.device)
-                # sample.mask = mask.repeat(batch_size, 1)
-
-                flow_gt = sample.delta.to(self.device)
-                flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-                # flow_gt = flow_gt.repeat(batch_size, 1, 1, 1)
-                # breakpoint()
-
-                # # Random permutation
-                # perm = torch.randperm(1200).to(self.device)
-                # mask = mask[:, perm]
-                # flow_gt = flow_gt[:, perm]
-                # condition = condition[:, perm]
-
-                flow_gt = normalize_trajectory(
-                    torch.flatten(flow_gt, start_dim=0, end_dim=1)
-                )
-                masked_flow_gt = flow_gt.reshape(
-                    -1, 1200, flow_gt.shape[1], flow_gt.shape[2]
-                )
-                masked_flow_gt = masked_flow_gt[mask == 1]
-
-                for t in self.noise_scheduler.timesteps:
-                    # model_output = self.model(
-                    #     noisy_input, encoder_hidden_states=condition, timestep=t
-                    # ).sample
-                    # breakpoint()
-                    model_output = self.model(noisy_input, t, context=sample).sample
-
-                    noisy_input = self.noise_scheduler.step(
-                        model_output, t, noisy_input
-                    ).prev_sample
-                #     print(t, model_output.transpose(1, 3)[0, :2, 0, :], noisy_input.transpose(1, 3)[0, :2, 0, :])
-
-                # print("-------------------")
-                # print("-------------------")
-                # print("-------------------")
-                # print("-------------------")
-
-                flow_prediction = noisy_input.transpose(1, 3)
-                flow_prediction = normalize_trajectory(
-                    torch.flatten(flow_prediction, start_dim=0, end_dim=1)
-                )
-
-                masked_flow_prediction = flow_prediction.reshape(
-                    -1, 1200, flow_prediction.shape[1], flow_prediction.shape[2]
-                )
-                masked_flow_prediction = masked_flow_prediction[mask == 1]
-                n_nodes = torch.as_tensor([d.num_nodes for d in sample.to_data_list()]).to(self.device)  # type: ignore
-
-                flow_loss = artflownet_loss(
-                    flow_prediction, flow_gt, n_nodes, reduce=False
-                )
-
-                # Compute some metrics on flow-only regions.
-                rmse, cos_dist, mag_error = flow_metrics(
-                    masked_flow_prediction, masked_flow_gt, reduce=False
-                )
-
-                # Aggregate the results
-                # Choose the one with smallest flow loss
-                flow_loss = flow_loss.reshape(batch_size, -1).mean(-1)
-                rmse = rmse.reshape(batch_size, -1).mean(-1)
-                cos_dist = cos_dist.reshape(batch_size, -1).mean(-1)
-                all_directions += list(cos_dist)
-
-                mag_error = mag_error.reshape(batch_size, -1).mean(-1)
-
-                # breakpoint()
-                chosen_id = torch.min(flow_loss, 0)[1]  # index
-                chosen_direction = cos_dist[chosen_id]
-                if chosen_direction > 0:
-                    multimodal = torch.sum((cos_dist + 0.3) < 0) != 0  # < 0.3
-                else:
-                    multimodal = torch.sum((cos_dist - 0.3) > 0) != 0  # > 0.3
-
-                # print(multimodal, rmse[chosen_id], cos_dist[chosen_id], mag_error[chosen_id], flow_loss[chosen_id])
-                all_multimodal += multimodal.item()
-                all_rmse += rmse[chosen_id].item()
-                all_cos_dist += cos_dist[chosen_id].item()
-                all_mag_error += mag_error[chosen_id].item()
-                all_flow_loss += flow_loss[chosen_id].item()
-
-                # print(all_rmse)
-
-        return {
-            "rmse": all_rmse / valid_sample_count,
-            "cos_dist": all_cos_dist / valid_sample_count,
-            "mag_error": all_mag_error / valid_sample_count,
-            "flow_loss": all_flow_loss / valid_sample_count,
-            "multimodal": all_multimodal / valid_sample_count,
-            "all_directions": all_directions,
-        }
-
-    def predict(self, val_dataloader, vis=False):
-        self.model.eval()
-
-        all_rmse = 0
-        all_cos_dist = 0
-        all_mag_error = 0
-        all_flow_loss = 0
-
-        with torch.no_grad():
-            for id, sample in enumerate(val_dataloader):
-                # breakpoint()
-                batch_size = sample.pos.shape[0] // 1200
-                noisy_input = (
-                    torch.randn((batch_size, 3, self.traj_len, 1200))
-                    .float()
-                    .to(self.device)
-                )
-                # condition = condition
-                # breakpoint()
-                condition = sample.pos
-                condition = condition.reshape(-1, 1200, condition.shape[1]).to(
-                    self.device
-                )
-
-                mask = sample.mask == 1
-                mask = mask.reshape(-1, 1200).to(self.device)
-
-                flow_gt = sample.delta.to(self.device)
-                flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-
-                # breakpoint()
-
-                # # Random permutation
-                # perm = torch.randperm(1200).to(self.device)
-                # mask = mask[:, perm]
-                # flow_gt = flow_gt[:, perm]
-                # condition = condition[:, perm]
-
-                flow_gt = normalize_trajectory(
-                    torch.flatten(flow_gt, start_dim=0, end_dim=1)
-                )
-                masked_flow_gt = flow_gt.reshape(
-                    -1, 1200, flow_gt.shape[1], flow_gt.shape[2]
-                )
-                masked_flow_gt = masked_flow_gt[mask == 1]
-
-                if vis:
-                    animation = FlowNetAnimation()
-                    pcd = sample.pos.numpy()
-
-                for t in self.noise_scheduler.timesteps:
-                    # model_output = self.model(
-                    #     noisy_input, encoder_hidden_states=condition, timestep=t
-                    # ).sample
-                    # breakpoint()
-                    model_output = self.model(noisy_input, t, context=sample).sample
-
-                    noisy_input = self.noise_scheduler.step(
-                        model_output, t, noisy_input
-                    ).prev_sample
-
-                    if vis:
-                        # print(noisy_input.shape)
-                        # print(torch.nn.functional.normalize(noisy_input, p=2, dim=1)
-                        #     .squeeze().permute(1, 0).shape)
-                        # print(torch.flatten(noisy_input.transpose(1, 3), start_dim=0, end_dim=1).shape)
-                        if t % 5 == 0 or t == 99:
-                            flow = (
-                                # torch.nn.functional.normalize(noisy_input, p=2, dim=1)
-                                # .squeeze()
-                                # .permute(1, 0)
-                                normalize_trajectory(
-                                    torch.flatten(
-                                        noisy_input.transpose(1, 3),
-                                        start_dim=0,
-                                        end_dim=1,
-                                    )
-                                )[:, 0, :]
-                            )
-                            animation.add_trace(
-                                torch.as_tensor(pcd),
-                                torch.as_tensor(
-                                    [pcd[mask.squeeze().detach().cpu().numpy()]]
-                                ),
-                                torch.as_tensor(
-                                    [
-                                        flow[mask.detach().cpu().numpy()]
-                                        .detach()
-                                        .cpu()
-                                        .numpy()
-                                    ]
-                                ),
-                                "red",
-                            )
-
-                flow_prediction = noisy_input.transpose(1, 3)
-                flow_prediction = normalize_trajectory(
-                    torch.flatten(flow_prediction, start_dim=0, end_dim=1)
-                )
-                masked_flow_prediction = flow_prediction.reshape(
-                    -1, 1200, flow_prediction.shape[1], flow_prediction.shape[2]
-                )
-                # flow_prediction = torch.nn.functional.normalize(
-                #     flow_prediction, p=2, dim=-1
-                # )
-                # largest_mag: float = torch.linalg.norm(
-                #     flow_prediction, ord=2, dim=-1
-                # ).max()
-                # flow_prediction = flow_prediction / (largest_mag + 1e-6)
-                # flow_prediction = flow_prediction.permute(1, 2, 0)
-                masked_flow_prediction = masked_flow_prediction[mask == 1]
-                # breakpoint()
-
-                # mean_dist = (masked_flow_prediction - flow_gt).norm(p=2, dim=-1).mean()
-                # cos_sim = torch.cosine_similarity(
-                #     masked_flow_prediction, flow_gt, dim=-1
-                # ).mean()
-                n_nodes = torch.as_tensor([d.num_nodes for d in sample.to_data_list()]).to(self.device)  # type: ignore
-                # breakpoint()
-                flow_loss = artflownet_loss(flow_prediction, flow_gt, n_nodes)
-
-                # Compute some metrics on flow-only regions.
-                rmse, cos_dist, mag_error = flow_metrics(
-                    masked_flow_prediction, masked_flow_gt
-                )
-
-                all_rmse += rmse.item()
-                all_cos_dist = cos_dist.item()
-                all_mag_error = mag_error.item()
-                all_flow_loss = flow_loss.item()
-
-        if vis:
-            # fig = animation.animate()
-            # fig.show()
-
-            return {
-                "animation": animation,
-                "rmse": all_rmse / len(val_dataloader),
-                "cos_dist": all_cos_dist / len(val_dataloader),
-                "mag_error": all_mag_error / len(val_dataloader),
-                "flow_loss": all_flow_loss / len(val_dataloader),
-            }
-        else:
-            return {
-                "rmse": all_rmse / len(val_dataloader),
-                "cos_dist": all_cos_dist / len(val_dataloader),
-                "mag_error": all_mag_error / len(val_dataloader),
-                "flow_loss": all_flow_loss / len(val_dataloader),
-            }
-
-    def predict_step(self, sample):  # For a single sample
-        batch_size = sample.pos.shape[0] // 1200
-        noisy_input = (
-            torch.randn((batch_size, 3, self.traj_len, 1200)).float().to(self.device)
-        )
-
-        condition = sample.pos
-        condition = condition.reshape(-1, 1200, condition.shape[1]).to(self.device)
-
-        for t in self.noise_scheduler.timesteps:
-            model_output = self.model(noisy_input, t, context=sample).sample
-
-            noisy_input = self.noise_scheduler.step(
-                model_output, t, noisy_input
-            ).prev_sample
-
-        flow_prediction = noisy_input.transpose(1, 3)
-
-        # # Metric
-
-        # new_flow_prediction = normalize_trajectory(
-        #         torch.flatten(flow_prediction, start_dim=0, end_dim=1)
-        #     )
-        # new_flow_prediction = new_flow_prediction.reshape(
-        #     -1, 1200, new_flow_prediction.shape[1], new_flow_prediction.shape[2]
-        # )
-
-        # flow_gt = sample.delta.to(self.device)
-        # flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-
-        # flow_gt = normalize_trajectory(
-        #     torch.flatten(flow_gt, start_dim=0, end_dim=1)
-        # )
-        # flow_gt = flow_gt.reshape(
-        #     -1, 1200, flow_gt.shape[1], flow_gt.shape[2]
-        # )
-
-        # n_nodes = torch.as_tensor([d.num_nodes for d in sample.to_data_list()]).to(self.device)  # type: ignore
-        # # breakpoint()
-        # flow_loss = artflownet_loss(flow_prediction, flow_gt, n_nodes)
-
-        # # Compute some metrics on flow-only regions.
-        # rmse, cos_dist, mag_error = flow_metrics(
-        #     new_flow_prediction, flow_gt
-        # )
-
-        # print("flow_loss: ", flow_loss)
-        # print("rmse, cos_dist, mag_error: ", rmse, cos_dist, mag_error)
-        return flow_prediction
-
-
-class TrajDiffuserSimWrapper(L.LightningModule):
-    def __init__(self, diffuser):
-        super().__init__()
-        self.diffuser = diffuser
-
-    def forward(self, data):
-        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
-        data = tgd.Data(
-            pos=torch.from_numpy(P_world).float().cuda(),
-            # mask=torch.ones(P_world.shape[0]).float(),
-        )
-        batch = tgd.Batch.from_data_list([data])
-        self.eval()
-        with torch.no_grad():
-            flow = self.diffuser.predict_step(batch)
-        return flow.squeeze().cpu()
-
-
-if __name__ == "__main__":
-    config = TrainingConfig()
-
-    wandb.init(
-        entity="r-pad",
-        # entity="leisure-thu-cv",
-        project="flowbothd",
-        # group="diffusion-PN++",
-        # job_type="overfit_trajectory",
-        group="synthetic",
-        job_type="train_diffuser_wta",
-    )
-
-    from flowbothd.datasets.synthetic_dataset import SyntheticDataModule
-
-    datamodule = SyntheticDataModule(batch_size=1, seed=42)
-
-    train_dataloader = datamodule.train_dataloader()
-    train_val_dataloader = datamodule.train_val_dataloader(bsz=1)
-    val_dataloader = datamodule.val_dataloader(bsz=1)
-    unseen_dataloader = datamodule.unseen_dataloader(bsz=1)
-
-    diffuser = TrajDiffuser(config, train_batch_num=len(train_dataloader))
-
-    # Train
-    # diffuser.load_model('/home/yishu/flowbothd/src/flowbothd/models/diffusion/door_diffusion_multimodal_ckpt.pth')
-    diffuser.train(
-        train_dataloader, train_val_dataloader, val_dataloader, unseen_dataloader
-    )
-
-    # # Overfit
-    # datamodule = FlowTrajectoryDataModule(
-    #     root="/home/yishu/datasets/partnet-mobility",
-    #     batch_size=1,
-    #     num_workers=30,
-    #     n_proc=2,
-    #     seed=42,
-    #     trajectory_len=config.traj_len,  # Only used when training trajectory model
-    # )
-
-    # train_dataloader = datamodule.train_dataloader()
-    # val_dataloader = datamodule.train_val_dataloader()
-
-    # # # Overfit
-    # samples = list(enumerate(train_dataloader))
-    # # breakpoint()
-    # sample = samples[0][1]
-    # diffuser.train([sample], [sample])
-
-    wandb.finish()
-
-    # diffuser.load_model('/home/yishu/flowbothd/logs/train_trajectory/2023-08-31/01-21-42/checkpoints/epoch=199-step=157200.ckpt')
-
-    # ##  Overfit sample prediction
-    # metric = diffuser.predict(sample, vis=True)
-    # print(f"dist:{metric['mean_dist']} cos:{metric['cos_sim']}")
-
-    # # Permutated sample prediction
-    # indices = torch.randperm(1200)
-    # sample.pos = sample.pos[indices]
-    # sample.delta = sample.delta[indices]
-    # sample.mask = sample.mask[indices]
-    # metric = diffuser.predict(sample, vis=True)
-    # print(f"dist:{metric['mean_dist']} cos:{metric['cos_sim']}")
diff --git a/src/flowbothd/models/diffusion/eval.py b/src/flowbothd/models/diffusion/eval.py
deleted file mode 100644
index 40890fe..0000000
--- a/src/flowbothd/models/diffusion/eval.py
+++ /dev/null
@@ -1,187 +0,0 @@
-from diffuser import TrainingConfig, TrajDiffuser
-
-config = TrainingConfig()
-diffuser = TrajDiffuser(config, train_batch_num=1)
-diffuser.load_model("./door_diffusion_ckpt_valbest.pth")
-
-# diffuser.load_model('/home/yishu/flowbothd/logs/train_trajectory/2023-08-31/16-13-10/checkpoints/epoch=399-step=314400.ckpt')
-
-from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule
-
-datamodule = FlowTrajectoryDataModule(
-    root="/home/yishu/datasets/partnet-mobility",
-    batch_size=1,
-    num_workers=30,
-    n_proc=2,
-    seed=42,
-    trajectory_len=config.traj_len,  # Only used when training trajectory model
-    special_req="fully-closed",
-    # toy_dataset = {
-    #     "id": "door-1",
-    #     "train-train": ["8994", "9035"],
-    #     "train-test": ["8994", "9035"],
-    #     "test": ["8867"],
-    #     # "train-train": ["8867"],
-    #     # "train-test": ["8867"],
-    #     # "test": ["8867"],
-    # }
-    toy_dataset={
-        "id": "door-full-new",
-        "train-train": [
-            "8877",
-            "8893",
-            "8897",
-            "8903",
-            "8919",
-            "8930",
-            "8961",
-            "8997",
-            "9016",
-            "9032",
-            "9035",
-            "9041",
-            "9065",
-            "9070",
-            "9107",
-            "9117",
-            "9127",
-            "9128",
-            "9148",
-            "9164",
-            "9168",
-            "9277",
-            "9280",
-            "9281",
-            "9288",
-            "9386",
-            "9388",
-            "9410",
-        ],
-        "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-        "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-    },
-)
-
-train_dataloader = datamodule.train_dataloader()
-train_val_dataloader = datamodule.train_val_dataloader()
-unseen_dataloader = datamodule.unseen_dataloader()
-
-trial_time = 50
-all_directions = []
-import tqdm
-
-for dataloader in [train_val_dataloader, unseen_dataloader]:
-    total_metrics = {
-        "best_rmse": 0,
-        "best_cosine": 0,
-        "best_mag": 0,
-        "best_flow_loss": 0,
-        "multimodal_ratio": 0,
-        "pos_rate": 0,
-        "neg_rate": 0,
-    }
-    multimodal_cnt = 0
-    total_sample_cnt = 0
-    for sample in tqdm.tqdm(dataloader):
-        total_sample_cnt += 1
-        # best_metric = {
-        #     'rmse': tensor(10.0, device='cuda:0'),
-        #     'cos_dist': tensor(-10.0, device='cuda:0'),
-        #     'mag_error': tensor(10.0, device='cuda:0'),
-        #     'flow_loss': tensor(10.0, device='cuda:0'),
-        #     'multimodal': 0,
-        # }
-        # multimodal = False
-        # for i in range(trial_time):
-        #     # metric = diffuser.predict([sample], vis=False)
-        #     metric = diffuser.predict_wta([sample], trial_times=trial_time)
-        #     all_directions += metric['all_directions']
-        #     # print(metric)
-        #     if i!=0 and metric['cos_dist'] * best_metric['cos_dist'] < 0:  # predicts different directions
-        #         multimodal = True
-        #     # if metric['cos_dist'] > best_metric['cos_dist']:
-        #     if metric['flow_loss'] < best_metric['flow_loss']:
-        #         for metric_type in best_metric.keys():
-        #             best_metric[metric_type] = metric[metric_type]
-
-        # multimodal = False
-        # for i in range(trial_time):
-        # metric = diffuser.predict([sample], vis=False)
-        best_metric = diffuser.predict_wta([sample], trial_times=trial_time)
-        all_directions += best_metric["all_directions"]
-        multimodal = best_metric["multimodal_ratio"]
-        # print(metric)
-        # if i!=0 and metric['cos_dist'] * best_metric['cos_dist'] < 0:  # predicts different directions
-        #     multimodal = True
-        # # if metric['cos_dist'] > best_metric['cos_dist']:
-        # if metric['flow_loss'] < best_metric['flow_loss']:
-        #     for metric_type in best_metric.keys():
-        #         best_metric[metric_type] = metric[metric_type]
-
-        # print(multimodal, best_metric["best_cosine"])
-        for metric_type in total_metrics.keys():
-            total_metrics[metric_type] += best_metric[metric_type]  # .item()
-
-        multimodal_cnt += multimodal
-
-    for metric_type in total_metrics.keys():
-        # total_metrics[metric_type] /= len(train_val_dataloader)
-        total_metrics[metric_type] /= total_sample_cnt
-
-    print(total_metrics)
-    print(multimodal_cnt)
-
-
-# Scatter plot
-ys = [d.item() for d in all_directions]
-xs = [
-    "8877",
-    "8893",
-    "8897",
-    "8903",
-    "8919",
-    "8930",
-    "8961",
-    "8997",
-    "9016",
-    "9032",
-    "9035",
-    "9041",
-    "9065",
-    "9070",
-    "9107",
-    "9117",
-    "9127",
-    "9128",
-    "9148",
-    "9164",
-    "9168",
-    "9277",
-    "9280",
-    "9281",
-    "9288",
-    "9386",
-    "9388",
-    "9410",
-    "8867",
-    "8983",
-    "8994",
-    "9003",
-    "9263",
-    "9393",
-]
-
-all_xs = []
-for x in xs:
-    all_xs += [x] * trial_time
-# breakpoint()
-colors = sorted(["red"]) * trial_time * (len(xs) - 6) + ["blue"] * trial_time * 6
-# colors = sorted(["red", "blue", "yellow"] * trial_time) * len(all_xs)
-import matplotlib.pyplot as plt
-
-fig = plt.figure()
-ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])
-ax.axhline(y=0)
-plt.scatter(all_xs, ys, s=5, c=colors[: len(ys)])
-plt.xticks(rotation=90)
-plt.savefig("./door_cos_stats.jpeg")
diff --git a/src/flowbothd/models/diffusion/history_diffuser.py b/src/flowbothd/models/diffusion/history_diffuser.py
deleted file mode 100644
index 5885568..0000000
--- a/src/flowbothd/models/diffusion/history_diffuser.py
+++ /dev/null
@@ -1,662 +0,0 @@
-## Diffusion model
-from dataclasses import dataclass
-
-import lightning as L
-import matplotlib.pyplot as plt
-import torch
-import torch.nn.functional as F
-import torch_geometric.data as tgd
-
-# import rpad.pyg.nets.pointnet2 as pnp
-import tqdm
-import wandb
-from diffusers import DDPMScheduler
-from diffusers.optimization import get_cosine_schedule_with_warmup
-from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation
-
-from flowbothd.datasets.flow_history_pdo import FlowHistoryDataModule
-from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule
-from flowbothd.metrics.trajectory import (
-    artflownet_loss,
-    flow_metrics,
-    normalize_trajectory,
-)
-from flowbothd.models.diffusion.history_model import PNHistoryDiffuser
-from flowbothd.models.modules.history_encoder import HistoryEncoder
-
-# import flowbothd.models.diffusion.module as pnp
-
-
-@dataclass
-class TrainingConfig:
-    device = "cuda"
-
-    # batch_size = 32
-    batch_size = 1
-    # train_batch_size = 16
-    # eval_batch_size = 16  # how many images to sample during evaluation
-    num_epochs = 400
-    # num_epochs = 10
-    gradient_accumulation_steps = 1
-    learning_rate = 1e-4
-    lr_warmup_steps = 1000
-    save_image_epochs = 10
-    save_model_epochs = 30
-    mixed_precision = "no"  # `no` for float32, `fp16` for automatic mixed precision
-    # output_dir = "ddpm-butterflies-128"  # the model name locally and on the HF Hub
-    train_sample_number = 1
-
-    traj_len = 1
-    # Diffuser params
-    num_train_timesteps = 100
-    seed = 0
-    sample_size = 1200
-    in_channels = 3
-    out_channels = 3
-    cross_attention_dim = 3
-    block_out_channels = [128, 256, 512, 512]
-    attention_head_dim = 3
-
-    # History encoder params
-    history_input_dim = 7
-    history_embed_dim = 128
-    # history_d_model = 128
-    # nhead = 4
-    # num_layers = 2
-    # dim_feedforward = 256
-
-    # ckpt params
-    read_ckpt_path = "./diffusion_condition_ckpt.pth"
-    save_ckpt_path = "./diffusion_condition_ckpt.pth"
-
-
-class HistoryTrajDiffuser:
-    def __init__(self, config, train_batch_num):
-        self.config = config
-        self.traj_len = config.traj_len
-        self.device = config.device
-
-        self.history_encoder = HistoryEncoder(
-            input_dim=config.history_input_dim, output_dim=config.history_embed_dim
-        ).to(config.device)
-
-        self.model = PNHistoryDiffuser(
-            in_channels=3 * config.traj_len,
-            # sample_size=1200,
-            traj_len=config.traj_len,
-            time_embed_dim=64,
-            history_embed_dim=config.history_embed_dim
-            # emb_dims=3
-        ).to(config.device)
-
-        self.noise_scheduler = DDPMScheduler(
-            num_train_timesteps=config.num_train_timesteps
-        )
-        self.optimizer_diffuser = torch.optim.AdamW(
-            self.model.parameters(), lr=config.learning_rate
-        )
-        self.optimizer_history = torch.optim.Adam(
-            self.history_encoder.parameters(),
-            lr=config.learning_rate,
-            betas=(0.9, 0.98),
-            eps=1e-9,
-        )
-        self.lr_scheduler_diffuser = get_cosine_schedule_with_warmup(
-            optimizer=self.optimizer_diffuser,
-            num_warmup_steps=config.lr_warmup_steps,
-            num_training_steps=(train_batch_num * config.num_epochs),
-            # num_training_steps=((config.train_sample_number // config.batch_size) * config.num_epochs),
-            # num_training_steps=(config.num_epochs),
-        )
-
-    def load_model(self, ckpt_path="./diffusion_best_ckpt.pth"):
-        self.model.load_state_dict(torch.load(ckpt_path))
-
-    def train(
-        self, train_dataloader, train_val_dataloader, val_dataloader, unseen_dataloader
-    ):
-        # Train loop
-        losses = []
-        min_loss = 10
-        global_step = 0
-        for epoch in range(config.num_epochs):
-            print(f"Epoch: {epoch}")
-            self.model.train()
-            self.history_encoder.train()
-
-            accu_loss = torch.tensor(0).float().to(self.device)
-            for id, batch in tqdm.tqdm(enumerate(train_dataloader)):
-                # The history embedding
-                history = torch.cat(
-                    [
-                        batch.trial_points,
-                        batch.trial_directions,
-                        batch.trial_results.unsqueeze(-1),
-                    ],
-                    dim=-1,
-                ).unsqueeze(0)
-                history_embed = self.history_encoder(history.to(self.device))
-                batch.history_embed = history_embed.repeat(self.config.sample_size, 1)
-
-                global_step += 1
-
-                clean_flow = batch.delta
-                clean_flow = clean_flow.reshape(
-                    -1, 1200, clean_flow.shape[1], clean_flow.shape[2]
-                ).to(self.device)
-                condition = batch.pos
-                condition = condition.reshape(-1, 1200, condition.shape[1]).to(
-                    self.device
-                )
-                clean_flow = clean_flow.transpose(1, 3)
-
-                # Sample noise to add
-                noise = torch.randn(clean_flow.shape).to(clean_flow.device)
-                bs = clean_flow.shape[0]
-
-                # Sample a random timestep for each image
-                timesteps = torch.randint(
-                    0,
-                    self.noise_scheduler.config.num_train_timesteps,
-                    (bs,),
-                    device=clean_flow.device,
-                ).long()
-
-                # Add noise to the clean images according to the noise magnitude at each timestep
-                # (this is the forward diffusion process)
-                noisy_images = self.noise_scheduler.add_noise(
-                    clean_flow, noise, timesteps
-                )
-
-                # Predict the noise residual
-                noise_pred = self.model(
-                    noisy_images, timesteps, context=batch, return_dict=False
-                )[0]
-                loss = F.mse_loss(noise_pred, noise)
-                if torch.isnan(loss):
-                    breakpoint()
-
-                accu_loss += loss
-
-                if (
-                    id % 32 == 31 or id == len(train_dataloader) - 1
-                ):  # BP every 32 sample
-                    loss.backward()
-                    self.optimizer_diffuser.step()
-                    self.optimizer_history.step()
-                    self.lr_scheduler_diffuser.step()
-                    self.optimizer_diffuser.zero_grad()
-                    self.optimizer_history.zero_grad()
-
-                    bsz_len = id % 32 + 1
-
-                    wandb.log(
-                        {"train_loss/loss": accu_loss.detach().item() / bsz_len},
-                        step=global_step,
-                    )
-                    wandb.log(
-                        {"train_loss/diffuser_lr": self.lr_scheduler_diffuser.get_lr()},
-                        step=global_step,
-                    )
-
-                    accu_loss = torch.tensor(0).float().to(self.device)
-
-                loss = loss.detach().item()
-                if epoch % 10 == 0 and loss < min_loss:
-                    min_loss = loss
-                    torch.save(self.model.state_dict(), self.config.save_ckpt_path)
-                losses.append(loss)
-
-            # Wandb
-            if epoch % 10 == 0:
-                # TODO: multimodal eval
-                # Trainset val Metric
-                mrmse = 0
-                mcos = 0
-                mmag = 0
-                mflow = 0
-                repeat_time = 1
-                for i in range(repeat_time):
-                    # metric = self.predict(train_val_dataloader, vis=False)
-                    metric = self.predict_pseudo_history(train_val_dataloader)
-                    mrmse += metric["rmse"]
-                    mcos += metric["cos_dist"]
-                    mmag += metric["mag_error"]
-                    mflow += metric["flow_loss"]
-                wandb.log({"train/rmse": mrmse / repeat_time}, step=global_step)
-                wandb.log({"train/cos": mcos / repeat_time}, step=global_step)
-                wandb.log({"train/mag": mmag / repeat_time}, step=global_step)
-                wandb.log({"train/flow": mflow / repeat_time}, step=global_step)
-
-                # Validation Metric - Always with history
-                mrmse = 0
-                mcos = 0
-                mmag = 0
-                mflow = 0
-                repeat_time = 1
-                for i in range(repeat_time):
-                    # metric = self.predict(val_dataloader, vis=False)
-                    metric = self.predict_pseudo_history(val_dataloader)
-                    mrmse += metric["rmse"]
-                    mcos += metric["cos_dist"]
-                    mmag += metric["mag_error"]
-                    mflow += metric["flow_loss"]
-                wandb.log(
-                    {"Pseudo-History/rmse": mrmse / repeat_time}, step=global_step
-                )
-                wandb.log({"Pseudo-History/cos": mcos / repeat_time}, step=global_step)
-                wandb.log({"Pseudo-History/mag": mmag / repeat_time}, step=global_step)
-                wandb.log(
-                    {"Pseudo-History/flow": mflow / repeat_time}, step=global_step
-                )
-
-                # Test Metric - Start from fully closed, obtain feedback, and predict until it gets correct
-                mrmse = 0
-                mcos = 0
-                mmag = 0
-                # mflow = 0
-                mstep = 0
-                repeat_time = 1
-                for i in range(repeat_time):
-                    # metric = self.predict(unseen_dataloader, vis=False)
-                    metric = self.predict_real_history(unseen_dataloader, max_step=20)
-                    mrmse += metric["rmse"]
-                    mcos += metric["cos_dist"]
-                    mmag += metric["mag_error"]
-                    mstep += metric["step"]
-                    # mflow += metric["flow_loss"]
-                wandb.log({"Real-History/rmse": mrmse / repeat_time}, step=global_step)
-                wandb.log({"Real-History/cos": mcos / repeat_time}, step=global_step)
-                wandb.log({"Real-History/mag": mmag / repeat_time}, step=global_step)
-                wandb.log({"Real-History/step": mstep / repeat_time}, step=global_step)
-
-        # Visualize loss
-        plt.figure()
-        plt.plot(losses[::50])
-
-    def predict_real_history(self, val_dataloader, max_step=20):
-        self.model.eval()
-        self.history_encoder.eval()
-
-        valid_sample_count = 0
-
-        all_rmse = 0
-        all_cos_dist = 0
-        all_mag_error = 0
-        # all_flow_loss = 0
-        all_steps = 0
-
-        all_cosines = []
-
-        # Eval every dataloader
-        with torch.no_grad():
-            for id, sample in tqdm.tqdm(enumerate(val_dataloader)):
-                valid_sample_count += 1
-                rmse, cos_dist, mag_error, step, cosines = self.predict_step(
-                    sample, max_step
-                )
-
-                all_steps += step
-                all_rmse += rmse.item()
-                all_cos_dist += cos_dist.item()
-                all_mag_error += mag_error.item()
-
-                all_cosines.append(cosines)
-                # print(all_rmse)
-
-        return {
-            "rmse": all_rmse / valid_sample_count,
-            "cos_dist": all_cos_dist / valid_sample_count,
-            "mag_error": all_mag_error / valid_sample_count,
-            # "flow_loss": all_flow_loss / valid_sample_count,
-            "step": all_steps / valid_sample_count,
-            "all_cosines": all_cosines,
-        }
-
-    def predict_pseudo_history(self, val_dataloader, vis=False):
-        self.model.eval()
-        self.history_encoder.eval()
-
-        all_rmse = 0
-        all_cos_dist = 0
-        all_mag_error = 0
-        all_flow_loss = 0
-
-        with torch.no_grad():
-            for id, sample in enumerate(val_dataloader):
-                # The history embedding
-                history = torch.cat(
-                    [
-                        sample.trial_points,
-                        sample.trial_directions,
-                        sample.trial_results.unsqueeze(-1),
-                    ],
-                    dim=-1,
-                ).unsqueeze(0)
-                history_embed = self.history_encoder(history.to(self.device))
-                sample.history_embed = history_embed.repeat(self.config.sample_size, 1)
-
-                batch_size = sample.pos.shape[0] // 1200
-                noisy_input = (
-                    torch.randn((batch_size, 3, self.traj_len, 1200))
-                    .float()
-                    .to(self.device)
-                )
-                condition = sample.pos
-                condition = condition.reshape(-1, 1200, condition.shape[1]).to(
-                    self.device
-                )
-
-                mask = sample.mask == 1
-                mask = mask.reshape(-1, 1200).to(self.device)
-
-                flow_gt = sample.delta.to(self.device)
-                flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-
-                flow_gt = normalize_trajectory(
-                    torch.flatten(flow_gt, start_dim=0, end_dim=1)
-                )
-                masked_flow_gt = flow_gt.reshape(
-                    -1, 1200, flow_gt.shape[1], flow_gt.shape[2]
-                )
-                masked_flow_gt = masked_flow_gt[mask == 1]
-
-                if vis:
-                    animation = FlowNetAnimation()
-                    pcd = sample.pos.numpy()
-
-                for t in self.noise_scheduler.timesteps:
-                    model_output = self.model(noisy_input, t, context=sample).sample
-
-                    noisy_input = self.noise_scheduler.step(
-                        model_output, t, noisy_input
-                    ).prev_sample
-
-                    if vis:
-                        if t % 5 == 0 or t == 99:
-                            flow = normalize_trajectory(
-                                torch.flatten(
-                                    noisy_input.transpose(1, 3),
-                                    start_dim=0,
-                                    end_dim=1,
-                                )
-                            )[:, 0, :]
-                            animation.add_trace(
-                                torch.as_tensor(pcd),
-                                torch.as_tensor(
-                                    [pcd[mask.squeeze().detach().cpu().numpy()]]
-                                ),
-                                torch.as_tensor(
-                                    [
-                                        flow[mask.detach().cpu().numpy()]
-                                        .detach()
-                                        .cpu()
-                                        .numpy()
-                                    ]
-                                ),
-                                "red",
-                            )
-
-                flow_prediction = noisy_input.transpose(1, 3)
-                flow_prediction = normalize_trajectory(
-                    torch.flatten(flow_prediction, start_dim=0, end_dim=1)
-                )
-                masked_flow_prediction = flow_prediction.reshape(
-                    -1, 1200, flow_prediction.shape[1], flow_prediction.shape[2]
-                )
-                masked_flow_prediction = masked_flow_prediction[mask == 1]
-                n_nodes = torch.as_tensor([d.num_nodes for d in sample.to_data_list()]).to(self.device)  # type: ignore
-                flow_loss = artflownet_loss(flow_prediction, flow_gt, n_nodes)
-
-                # Compute some metrics on flow-only regions.
-                rmse, cos_dist, mag_error = flow_metrics(
-                    masked_flow_prediction, masked_flow_gt
-                )
-
-                all_rmse += rmse.item()
-                all_cos_dist = cos_dist.item()
-                all_mag_error = mag_error.item()
-                all_flow_loss = flow_loss.item()
-
-        if vis:
-            # fig = animation.animate()
-            # fig.show()
-
-            return {
-                "animation": animation,
-                "rmse": all_rmse / len(val_dataloader),
-                "cos_dist": all_cos_dist / len(val_dataloader),
-                "mag_error": all_mag_error / len(val_dataloader),
-                "flow_loss": all_flow_loss / len(val_dataloader),
-            }
-        else:
-            return {
-                "rmse": all_rmse / len(val_dataloader),
-                "cos_dist": all_cos_dist / len(val_dataloader),
-                "mag_error": all_mag_error / len(val_dataloader),
-                "flow_loss": all_flow_loss / len(val_dataloader),
-            }
-
-    # TODO: change to adapt to history
-    @torch.no_grad()
-    def predict_step(self, sample, max_step=20):  # For a single sample
-        self.model.eval()
-        self.history_encoder.eval()
-
-        batch_size = sample.pos.shape[0] // 1200
-        assert batch_size == 1, f"batch size should be 1, now is {batch_size}"
-
-        cosines = []
-
-        # The history embedding - start with no history
-        history_stack = []
-        history = torch.zeros(batch_size, 1, 7).to(self.device)
-        history_embed = self.history_encoder(history)
-        sample.history_embed = history_embed.repeat(self.config.sample_size, 1)
-
-        for step in range(max_step):
-            noisy_input = (
-                torch.randn((batch_size, 3, self.traj_len, 1200))
-                .float()
-                .to(self.device)
-            )
-
-            condition = sample.pos
-            condition = condition.reshape(-1, 1200, condition.shape[1]).to(self.device)
-
-            for t in self.noise_scheduler.timesteps:
-                model_output = self.model(noisy_input, t, context=sample).sample
-
-                noisy_input = self.noise_scheduler.step(
-                    model_output, t, noisy_input
-                ).prev_sample
-
-            flow_prediction = noisy_input.transpose(
-                1, 3
-            ).squeeze()  ## TODO: only support batch size = 1, traj length = 1
-
-            # Pseudo trial and feedback
-            grasp_point_id = flow_prediction.norm(dim=-1).argmax()
-            grasp_direction = flow_prediction[grasp_point_id]
-            gt_direction = sample.delta[grasp_point_id, 0].to(self.device)
-            pseudo_feedback = torch.sum(grasp_direction * gt_direction)
-
-            # If success
-            pseudo_result = torch.cosine_similarity(
-                flow_prediction, sample.delta.squeeze(), dim=-1
-            ).mean()
-            cosines.append(pseudo_result)
-            if pseudo_result > 0.7:  # Success
-                break
-            history_stack.append(
-                torch.cat(
-                    [grasp_direction, gt_direction, pseudo_feedback.unsqueeze(0)],
-                    dim=-1,
-                )
-            )
-
-            # Update history
-            history = torch.stack(history_stack, dim=0).unsqueeze(0).to(self.device)
-            history_embed = self.history_encoder(history)
-            sample.history_embed = history_embed.repeat(self.config.sample_size, 1)
-
-        # Metric
-        new_flow_prediction = normalize_trajectory(flow_prediction.unsqueeze(1))
-        new_flow_prediction = new_flow_prediction.reshape(
-            -1, 1200, new_flow_prediction.shape[1], new_flow_prediction.shape[2]
-        )
-
-        flow_gt = sample.delta.to(self.device)
-        flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-
-        flow_gt = normalize_trajectory(torch.flatten(flow_gt, start_dim=0, end_dim=1))
-        flow_gt = flow_gt.reshape(-1, 1200, flow_gt.shape[1], flow_gt.shape[2])
-
-        n_nodes = torch.as_tensor([d.num_nodes for d in sample.to_data_list()]).to(self.device)  # type: ignore
-        # flow_loss = artflownet_loss(flow_prediction, flow_gt, n_nodes)
-
-        # Compute some metrics on flow-only regions.
-        rmse, cos_dist, mag_error = flow_metrics(new_flow_prediction, flow_gt)
-
-        return rmse, cos_dist, mag_error, step, cosines
-
-
-# TODO: change to adapt to history
-class TrajHistoryDiffuserSimWrapper(L.LightningModule):
-    def __init__(self, diffuser):
-        super().__init__()
-        self.diffuser = diffuser
-
-    def forward(self, data):
-        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = data
-        data = tgd.Data(
-            pos=torch.from_numpy(P_world).float().cuda(),
-            # mask=torch.ones(P_world.shape[0]).float(),
-        )
-        batch = tgd.Batch.from_data_list([data])
-        self.eval()
-        with torch.no_grad():
-            flow = self.diffuser.predict_step(batch)
-        return flow.squeeze().cpu()
-
-
-if __name__ == "__main__":
-    config = TrainingConfig()
-
-    wandb.init(
-        entity="r-pad",
-        # entity="leisure-thu-cv",
-        project="flowbothd",
-        # group="diffusion-PN++",
-        # job_type="overfit_trajectory",
-        group="door_condition_diffusion",
-        job_type="train_condition_diffuser(pseudo feedback)",
-    )
-
-    # Create dataset
-    toy_dataset = {
-        "id": "door-full-new",
-        "train-train": [
-            "8877",
-            "8893",
-            "8897",
-            "8903",
-            "8919",
-            "8930",
-            "8961",
-            "8997",
-            "9016",
-            "9032",
-            "9035",
-            "9041",
-            "9065",
-            "9070",
-            "9107",
-            "9117",
-            "9127",
-            "9128",
-            "9148",
-            "9164",
-            "9168",
-            "9277",
-            "9280",
-            "9281",
-            "9288",
-            "9386",
-            "9388",
-            "9410",
-        ],
-        "train-test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-        "test": ["8867", "8983", "8994", "9003", "9263", "9393"],
-    }
-
-    # Trajectory module
-    trajectory_module = FlowTrajectoryDataModule(
-        root="/home/yishu/datasets/partnet-mobility",
-        batch_size=1,
-        num_workers=30,
-        n_proc=2,
-        seed=42,
-        trajectory_len=1,  # Only used when training trajectory model
-        toy_dataset=toy_dataset,
-    )
-
-    # History Trajectory module
-    datamodule = FlowHistoryDataModule(
-        root="/home/yishu/datasets/partnet-mobility",
-        batch_size=1,
-        num_workers=30,
-        n_proc=2,
-        trajectory_datasets=trajectory_module,
-        seed=42,
-        trajectory_len=1,  # Only used when inference trajectory model
-        toy_dataset=toy_dataset,
-    )
-
-    train_dataloader = datamodule.train_dataloader()
-    train_val_dataloader = datamodule.train_val_dataloader(bsz=1)
-    val_dataloader = datamodule.val_dataloader(bsz=1)
-    unseen_dataloader = datamodule.unseen_dataloader(bsz=1)
-
-    diffuser = HistoryTrajDiffuser(config, train_batch_num=len(train_dataloader))
-
-    # Train
-    # diffuser.load_model('/home/yishu/flowbothd/src/flowbothd/models/diffusion/door_diffusion_multimodal_ckpt.pth')
-    diffuser.train(
-        train_dataloader, train_val_dataloader, val_dataloader, unseen_dataloader
-    )
-
-    # # Overfit
-    # datamodule = FlowTrajectoryDataModule(
-    #     root="/home/yishu/datasets/partnet-mobility",
-    #     batch_size=1,
-    #     num_workers=30,
-    #     n_proc=2,
-    #     seed=42,
-    #     trajectory_len=config.traj_len,  # Only used when training trajectory model
-    # )
-
-    # train_dataloader = datamodule.train_dataloader()
-    # val_dataloader = datamodule.train_val_dataloader()
-
-    # # # Overfit
-    # samples = list(enumerate(train_dataloader))
-    # # breakpoint()
-    # sample = samples[0][1]
-    # diffuser.train([sample], [sample])
-
-    wandb.finish()
-
-    # diffuser.load_model('/home/yishu/flowbothd/logs/train_trajectory/2023-08-31/01-21-42/checkpoints/epoch=199-step=157200.ckpt')
-
-    # ##  Overfit sample prediction
-    # metric = diffuser.predict(sample, vis=True)
-    # print(f"dist:{metric['mean_dist']} cos:{metric['cos_sim']}")
-
-    # # Permutated sample prediction
-    # indices = torch.randperm(1200)
-    # sample.pos = sample.pos[indices]
-    # sample.delta = sample.delta[indices]
-    # sample.mask = sample.mask[indices]
-    # metric = diffuser.predict(sample, vis=True)
-    # print(f"dist:{metric['mean_dist']} cos:{metric['cos_sim']}")
diff --git a/src/flowbothd/models/diffusion/history_model.py b/src/flowbothd/models/diffusion/history_model.py
deleted file mode 100644
index ef254c1..0000000
--- a/src/flowbothd/models/diffusion/history_model.py
+++ /dev/null
@@ -1,136 +0,0 @@
-from dataclasses import dataclass
-
-import rpad.pyg.nets.pointnet2 as pnp
-import torch
-import torch_geometric.data as tgd
-from diffusers.configuration_utils import ConfigMixin, register_to_config
-from diffusers.models.embeddings import TimestepEmbedding, Timesteps
-from diffusers.models.modeling_utils import ModelMixin
-from diffusers.utils import BaseOutput
-
-# import flowbothd.models.diffusion.module as pnp
-
-# from .util import quat2mat
-
-
-@dataclass
-class PointNetOutput(BaseOutput):
-    sample: torch.FloatTensor
-
-
-class PNHistoryDiffuser(ModelMixin, ConfigMixin):
-    @register_to_config
-    def __init__(
-        self, in_channels, time_embed_dim, history_embed_dim, traj_len, cond_emb_dims=64
-    ):
-        super(PNHistoryDiffuser, self).__init__()
-        self.in_channels = in_channels
-        self.sample_size = 1200
-        self.time_embed_dim = time_embed_dim
-        self.history_embed_dim = history_embed_dim
-        self.traj_len = traj_len
-
-        # positional time embeddings
-        flip_sin_to_cos = True
-        freq_shift = 0
-        self.time_proj = Timesteps(64, flip_sin_to_cos, freq_shift)
-        timestep_input_dim = 64
-        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)
-
-        self.condition_encoder = pnp.PN2Dense(
-            in_channels=1,
-            out_channels=cond_emb_dims,
-            p=pnp.PN2DenseParams(),
-        )
-
-        self.predictor = pnp.PN2Dense(
-            # in_channels = self.in_channels + timestep_input_dim,
-            in_channels=self.in_channels + time_embed_dim + history_embed_dim,
-            out_channels=3 * self.traj_len,
-            p=pnp.PN2DenseParams(),
-        )
-        # self.act = torch.nn.SiLU()
-
-    def forward(
-        self,
-        noisy_input,
-        timestep,
-        context,
-        return_dict: bool = True,
-    ):
-        """
-        Args:
-            x:  Flow at some timestep t, (B, 3, 1, N).
-            timestep:     Time. (B, ).
-            context: Batched data with .pos (context pointcloud observation), .x (should be replaced to be the noisy input)
-        """
-
-        # time embedding
-        timesteps = timestep
-        if not torch.is_tensor(timesteps):
-            timesteps = torch.tensor(
-                [timesteps], dtype=torch.long, device=noisy_input.device
-            )
-        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:
-            timesteps = timesteps[None].to(noisy_input.device)
-
-        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML
-        timesteps = timesteps * torch.ones(
-            noisy_input.shape[0], dtype=timesteps.dtype, device=timesteps.device
-        )
-
-        t_emb = self.time_proj(timesteps)
-
-        # timesteps does not contain any weights and will always return f32 tensors
-        # but time_embedding might actually be running in fp16. so we need to cast here.
-        # there might be better ways to encapsulate this.
-        t_emb = t_emb.to(dtype=self.dtype)
-        t_emb = self.time_embedding(t_emb)
-
-        t_emb = t_emb.unsqueeze(-1).repeat(1, 1, self.sample_size)
-
-        context = context.to(noisy_input.device)
-        history_info = context.history_embed.permute(1, 0).unsqueeze(0)
-        context.x = torch.cat(
-            (torch.flatten(noisy_input, start_dim=1, end_dim=2), history_info, t_emb),
-            dim=1,
-        )  # (B, 3 + 32 + 64 , N)
-        # context.x = torch.cat(
-        #     (torch.flatten(noisy_input, start_dim=1, end_dim=2), t_emb), dim=1
-        # )  # (B, 3 + 64 , N)
-        context.x = torch.flatten(context.x.permute(0, 2, 1), start_dim=0, end_dim=1)
-
-        # x = self.act(self.predictor(context))
-        x = self.predictor(context)
-        x = x.reshape(-1, self.sample_size, 3, self.traj_len).permute(0, 2, 3, 1)
-        if not return_dict:
-            return (x,)
-
-        return PointNetOutput(sample=x)
-        # return x
-
-
-if __name__ == "__main__":
-    model = PNHistoryDiffuser(3, 64, 1, 64)
-
-    from typing import Protocol, cast
-
-    class Flowbot3DTGData(Protocol):
-        id: str  # Object ID.
-
-        pos: torch.Tensor  # Points in the point cloud.
-        flow: torch.Tensor  # instantaneous positive 3D flow.
-        mask: torch.Tensor  # Mask of the part of interest.
-
-    condition = cast(
-        Flowbot3DTGData,
-        tgd.Data(
-            id="1",
-            pos=torch.randn((1200, 3)),
-            flow=torch.randn((1200, 3)),
-            mask=torch.randn(1200),
-            x=torch.randn(1200, 1),
-        ),
-    )
-    input = torch.randn((10, 3, 1200))
-    print(model(input, 1, condition))
diff --git a/src/flowbothd/models/diffusion/inference.ipynb b/src/flowbothd/models/diffusion/inference.ipynb
deleted file mode 100644
index d252d19..0000000
--- a/src/flowbothd/models/diffusion/inference.ipynb
+++ /dev/null
@@ -1,277 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "from diffuser import TrainingConfig, TrajDiffuser"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "config = TrainingConfig()\n",
-    "diffuser = TrajDiffuser(config, train_batch_num=1)\n",
-    "diffuser.load_model('./ckpts/closed_diffusion_fullset_ckpt.pth')"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule\n",
-    "datamodule = FlowTrajectoryDataModule(\n",
-    "    root=\"/home/yishu/datasets/partnet-mobility\",\n",
-    "    batch_size=1,\n",
-    "    num_workers=30,\n",
-    "    n_proc=2,\n",
-    "    seed=42,\n",
-    "    trajectory_len=config.traj_len,  # Only used when training trajectory model\n",
-    "    special_req=\"fully-closed\",\n",
-    "    # toy_dataset = {\n",
-    "    #     \"id\": \"door-1\",\n",
-    "    #     \"train-train\": [\"8994\", \"9035\"],\n",
-    "    #     \"train-test\": [\"8994\", \"9035\"],\n",
-    "    #     \"test\": [\"8867\"],\n",
-    "    #     # \"train-train\": [\"8867\"],\n",
-    "    #     # \"train-test\": [\"8867\"],\n",
-    "    #     # \"test\": [\"8867\"],\n",
-    "    # }\n",
-    "    # toy_dataset = {\n",
-    "    #     \"id\": \"door-full-new\",\n",
-    "    #     \"train-train\": [\"8877\", \"8893\", \"8897\", \"8903\", \"8919\", \"8930\", \"8936\", \"8961\", \"8997\", \"9016\", \"9032\", \"9035\", \"9041\", \"9065\", \"9070\", \"9107\", \"9117\", \"9127\", \"9128\", \"9148\", \"9164\", \"9168\", \"9277\", \"9280\", \"9281\", \"9288\", \"9386\", \"9388\", \"9410\"],\n",
-    "    #     \"train-test\": [\"8867\", \"8983\", \"8994\", \"9003\", \"9263\", \"9393\"],\n",
-    "    #     \"test\": [\"8867\", \"8983\", \"8994\", \"9003\", \"9263\", \"9393\"],\n",
-    "    # }\n",
-    "\n",
-    "    # toy_dataset = {\n",
-    "    #     \"id\": \"door-single-test-8867\",\n",
-    "    #     \"train-train\": [\"8867\"],\n",
-    "    #     \"train-test\": [\"8867\"],\n",
-    "    #     \"test\": [\"8867\"],\n",
-    "    # }\n",
-    "    toy_dataset = {\n",
-    "        \"id\": \"door-full-new\",\n",
-    "        \"train-train\": [\"8877\", \"8893\", \"8897\", \"8903\", \"8919\", \"8930\", \"8961\", \"8997\", \"9016\", \"9032\", \"9035\", \"9041\", \"9065\", \"9070\", \"9107\", \"9117\", \"9127\", \"9128\", \"9148\", \"9164\", \"9168\", \"9277\", \"9280\", \"9281\", \"9288\", \"9386\", \"9388\", \"9410\"],\n",
-    "        \"train-test\": [\"8867\", \"8983\", \"8994\", \"9003\", \"9263\", \"9393\"],\n",
-    "        \"test\": [\"8867\", \"8983\", \"8994\", \"9003\", \"9263\", \"9393\"],\n",
-    "    }\n",
-    ")\n",
-    "\n",
-    "train_val_dataloader = datamodule.train_val_dataloader()\n",
-    "val_dataloader = datamodule.val_dataloader()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "samples = list(enumerate(train_val_dataloader))\n",
-    "sample = samples[23][1]"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import numpy as np\n",
-    "import matplotlib.pyplot as plt\n",
-    "from mpl_toolkits.mplot3d import Axes3D  # Importing 3D plotting tools\n",
-    "\n",
-    "# Create a 3D scatter plot\n",
-    "fig = plt.figure()\n",
-    "ax = fig.add_subplot(111, projection='3d')\n",
-    "ax.scatter(sample.pos.cpu().numpy()[:, 0], sample.pos.cpu().numpy()[:, 1], sample.pos.cpu().numpy()[:, 2])\n",
-    "plt.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# for i in range(1200):\n",
-    "#     sample.pos[i][0] = -1 * sample.pos[i][0]\n",
-    "#     sample.pos[i][1] = -1 * sample.pos[i][1]\n",
-    "\n",
-    "#     sample.delta[i][0][0] = -1 * sample.delta[i][0][0]\n",
-    "#     sample.delta[i][0][1] = -1 * sample.delta[i][0][1]"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metrics = []\n",
-    "\n",
-    "import tqdm\n",
-    "for i in tqdm.tqdm(range(20)):\n",
-    "    samples = list(enumerate(val_dataloader))\n",
-    "    sample = samples[0][1]\n",
-    "    metric = diffuser.predict([sample], vis=True)\n",
-    "    metrics.append(metric)\n",
-    "    if metric['cos_dist'] > 0.7:\n",
-    "        print(\"hi\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metrics"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "minus = []\n",
-    "for i in range(20):\n",
-    "    if metrics[i]['cos_dist'] > 0.8:\n",
-    "        minus.append(i)\n",
-    "print(minus)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metrics[0]['animation'].animate().show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metrics[2]"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metrics"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metric_two = diffuser.predict([samples[1][1]], vis=True)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metric_two"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Unseen"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "test_dataloader = datamodule.unseen_dataloader()\n",
-    "samples = list(enumerate(test_dataloader))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metric_unseen = diffuser.predict([samples[0][1]], vis=True)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "metric_unseen"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "for i in range(100):\n",
-    "    metric_unseen = diffuser.predict([samples[3][1]], vis=False)\n",
-    "    if metric_unseen['cos_dist'] > 0:\n",
-    "        print(\"other direction!\")\n",
-    "        break"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "i"
-   ]
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "openany",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.9.17"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
-}
diff --git a/src/flowbothd/models/diffusion/model.py b/src/flowbothd/models/diffusion/model.py
deleted file mode 100644
index 14f0b25..0000000
--- a/src/flowbothd/models/diffusion/model.py
+++ /dev/null
@@ -1,135 +0,0 @@
-from dataclasses import dataclass
-
-import rpad.pyg.nets.pointnet2 as pnp
-import torch
-import torch_geometric.data as tgd
-from diffusers.configuration_utils import ConfigMixin, register_to_config
-from diffusers.models.embeddings import TimestepEmbedding, Timesteps
-from diffusers.models.modeling_utils import ModelMixin
-from diffusers.utils import BaseOutput
-
-# import flowbothd.models.diffusion.module as pnp
-
-# from .util import quat2mat
-
-
-@dataclass
-class PointNetOutput(BaseOutput):
-    sample: torch.FloatTensor
-
-
-class PNDiffuser(ModelMixin, ConfigMixin):
-    @register_to_config
-    def __init__(self, in_channels, time_embed_dim, traj_len, cond_emb_dims=64):
-        super(PNDiffuser, self).__init__()
-        self.in_channels = in_channels
-        self.sample_size = 1200
-        self.time_embed_dim = time_embed_dim
-        self.traj_len = traj_len
-
-        # positional time embeddings
-        flip_sin_to_cos = True
-        freq_shift = 0
-        self.time_proj = Timesteps(64, flip_sin_to_cos, freq_shift)
-        timestep_input_dim = 64
-        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)
-
-        # self.condition_encoder = pnp.PN2Dense(
-        #     in_channels=1,
-        #     out_channels=cond_emb_dims,
-        #     p=pnp.PN2DenseParams(),
-        # )
-
-        self.predictor = pnp.PN2Dense(
-            # in_channels = self.in_channels + timestep_input_dim,
-            in_channels=self.in_channels + time_embed_dim,
-            out_channels=3 * self.traj_len,
-            p=pnp.PN2DenseParams(),
-        )
-
-    def forward(
-        self,
-        noisy_input,
-        timestep,
-        context,
-        return_dict: bool = True,
-    ):
-        """
-        Args:
-            x:  Flow at some timestep t, (B, 3, 1, N).
-            timestep:     Time. (B, ).
-            context: Batched data with .pos (context pointcloud observation), .x (should be replaced to be the noisy input)
-        """
-
-        # time embedding
-        timesteps = timestep
-        if not torch.is_tensor(timesteps):
-            timesteps = torch.tensor(
-                [timesteps], dtype=torch.long, device=noisy_input.device
-            )
-        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:
-            timesteps = timesteps[None].to(noisy_input.device)
-
-        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML
-        timesteps = timesteps * torch.ones(
-            noisy_input.shape[0], dtype=timesteps.dtype, device=timesteps.device
-        )
-
-        t_emb = self.time_proj(timesteps)
-
-        # timesteps does not contain any weights and will always return f32 tensors
-        # but time_embedding might actually be running in fp16. so we need to cast here.
-        # there might be better ways to encapsulate this.
-        t_emb = t_emb.to(dtype=self.dtype)
-        t_emb = self.time_embedding(t_emb)
-
-        t_emb = t_emb.unsqueeze(-1).repeat(1, 1, self.sample_size)
-
-        # #  condition embedding
-        # cond_emb = self.condition_encoder(condition)
-
-        # concatenate embeddings
-        # breakpoint()
-        context = context.to(noisy_input.device)
-        context.x = torch.cat(
-            (torch.flatten(noisy_input, start_dim=1, end_dim=2), t_emb), dim=1
-        )  # (B, 3 + 64 , N)
-        # breakpoint()
-        context.x = torch.flatten(context.x.permute(0, 2, 1), start_dim=0, end_dim=1)
-        x = self.predictor(context)
-        x = x.reshape(-1, self.sample_size, 3, self.traj_len).permute(0, 2, 3, 1)
-        # breakpoint()
-        if not return_dict:
-            return (x,)
-
-        # if torch.sum(torch.abs(x[0, :, 0, :2])) > 1e4:
-        #     breakpoint()
-        # print(x[0, :, 0, :2])
-        return PointNetOutput(sample=x)
-        # return x
-
-
-if __name__ == "__main__":
-    model = PNDiffuser(3, 64, 1, 64)
-
-    from typing import Protocol, cast
-
-    class Flowbot3DTGData(Protocol):
-        id: str  # Object ID.
-
-        pos: torch.Tensor  # Points in the point cloud.
-        flow: torch.Tensor  # instantaneous positive 3D flow.
-        mask: torch.Tensor  # Mask of the part of interest.
-
-    condition = cast(
-        Flowbot3DTGData,
-        tgd.Data(
-            id="1",
-            pos=torch.randn((1200, 3)),
-            flow=torch.randn((1200, 3)),
-            mask=torch.randn(1200),
-            x=torch.randn(1200, 1),
-        ),
-    )
-    input = torch.randn((10, 3, 1200))
-    print(model(input, 1, condition))
diff --git a/src/flowbothd/models/diffusion/simulation.ipynb b/src/flowbothd/models/diffusion/simulation.ipynb
deleted file mode 100644
index d785fc9..0000000
--- a/src/flowbothd/models/diffusion/simulation.ipynb
+++ /dev/null
@@ -1,125 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import pandas as pd \n",
-    "import numpy as np\n",
-    "import json\n",
-    "from flowbothd.datasets.flow_trajectory import FlowTrajectoryDataModule\n",
-    "from diffuser import TrainingConfig, TrajDiffuser, TrajDiffuserSimWrapper\n",
-    "from flowbothd.simulations.simulation import trial_with_diffuser, trial_gt_trajectory\n",
-    "\n",
-    "def load_obj_and_link():\n",
-    "    with open(\"../../../../scripts/umpnet_object_list.json\", \"r\") as f:\n",
-    "        object_link_json = json.load(f)\n",
-    "    return object_link_json\n",
-    "\n",
-    "object_to_link = load_obj_and_link()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "config = TrainingConfig()\n",
-    "diffuser = TrajDiffuser(config, train_batch_num=1)\n",
-    "diffuser.load_model(\"./half_diffusion_fullset_ckpt_backup.pth\")\n",
-    "\n",
-    "diffuser_simulator = TrajDiffuserSimWrapper(diffuser)\n",
-    "\n",
-    "success_rate = 0\n",
-    "norm_dist = 0\n",
-    "count = 0\n",
-    "import tqdm"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# object_ids = [\"8877\", \"8893\", \"8897\", \"8903\", \"8919\", \"8930\", \"8961\", \"8997\", \"9016\", \"9032\", \"9035\", \"9041\", \"9065\", \"9070\", \"9107\", \"9117\", \"9127\", \"9128\", \"9148\", \"9164\", \"9168\", \"9277\", \"9280\", \"9281\", \"9288\", \"9386\", \"9388\", \"9410\", \"8867\", \"8983\", \"8994\", \"9003\", \"9263\", \"9393\"]\n",
-    "object_ids = [\"9410\"]\n",
-    "all_results = []\n",
-    "all_figs = []\n",
-    "for obj_id in tqdm.tqdm(object_ids):\n",
-    "    if obj_id not in object_to_link.keys():\n",
-    "        continue\n",
-    "    count += 1\n",
-    "    available_links = object_to_link[obj_id]\n",
-    "    trial_figs, trial_results = trial_with_diffuser(\n",
-    "        obj_id=obj_id,\n",
-    "        model=diffuser_simulator,\n",
-    "        n_step=30,\n",
-    "        gui=False,\n",
-    "        website=True,\n",
-    "        all_joint=True,\n",
-    "        # available_joints=available_links,\n",
-    "    )\n",
-    "\n",
-    "    for (fig, result) in zip(trial_figs, trial_results):\n",
-    "        if result.success == True:\n",
-    "            print(\"YES\")\n",
-    "            all_results.append(result)\n",
-    "            all_figs.append(trial_figs[fig])\n",
-    "    # all_results.append(trial_results)\n",
-    "    # all_figs.append(trial_figs)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "len(all_results)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "trial_results"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "trial_figs['link_3']"
-   ]
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "openany",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.9.17"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
-}
diff --git a/src/flowbothd/models/diffusion/simulation.py b/src/flowbothd/models/diffusion/simulation.py
deleted file mode 100644
index d29413e..0000000
--- a/src/flowbothd/models/diffusion/simulation.py
+++ /dev/null
@@ -1,91 +0,0 @@
-# Simulation results for diffusion models
-
-import json
-
-from diffuser import TrainingConfig, TrajDiffuser, TrajDiffuserSimWrapper
-
-from flowbothd.simulations.simulation import trial_with_diffuser
-
-
-def load_obj_and_link():
-    with open("../../../../scripts/umpnet_object_list.json", "r") as f:
-        object_link_json = json.load(f)
-    return object_link_json
-
-
-object_to_link = load_obj_and_link()
-
-config = TrainingConfig()
-diffuser = TrajDiffuser(config, train_batch_num=1)
-diffuser.load_model("./half_diffusion_fullset_ckpt_backup.pth")
-
-diffuser_simulator = TrajDiffuserSimWrapper(diffuser)
-
-success_rate = 0
-norm_dist = 0
-count = 0
-import tqdm
-
-object_ids = [
-    "8867",
-    "8877",
-    "8893",
-    "8897",
-    "8903",
-    "8919",
-    "8930",
-    "8961",
-    "8997",
-    "9016",
-    "9032",
-    "9035",
-    "9041",
-    "9065",
-    "9070",
-    "9107",
-    "9117",
-    "9127",
-    "9128",
-    "9148",
-    "9164",
-    "9168",
-    "9277",
-    "9280",
-    "9281",
-    "9288",
-    "9386",
-    "9388",
-    "9410",
-    "8867",
-    "8983",
-    "8994",
-    "9003",
-    "9263",
-    "9393",
-]
-for obj_id in tqdm.tqdm(object_ids):
-    if obj_id not in object_to_link.keys():
-        continue
-    available_links = object_to_link[obj_id]
-    trial_figs, trial_results = trial_with_diffuser(
-        obj_id=obj_id,
-        model=diffuser_simulator,
-        n_step=30,
-        gui=False,
-        website=True,
-        all_joint=True,
-        # available_joints=available_links,
-    )
-
-    for result in trial_results:
-        if not result.contact:
-            continue
-        success_rate += result.success
-        norm_dist += result.metric
-        count += 1
-    # all_trial_results.append(trial_results)
-    # all_trial_figs.append(trial_figs)
-
-print(success_rate / count, norm_dist / count)
-
-breakpoint()
diff --git a/src/flowbothd/nets/__init__.py b/src/flowbothd/nets/__init__.py
deleted file mode 100644
index e69de29..0000000
